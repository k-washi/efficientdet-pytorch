{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/rwightman/efficientdet-pytorch.git\n",
    "#!wget https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d1-4c7ebaf2.pth\n",
    "#!mv tf_efficientdet_d1-4c7ebaf2.pth tf_efficientdet_d1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm\n",
    "#!pip install omegaconf\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [apex](https://github.com/NVIDIA/apex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/NVIDIA/apex\n",
    "#!cd apex\n",
    "#!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using apex: False\n"
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'effdet'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18df63530a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using apex:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_apex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meffdet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_efficientdet_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEfficientDet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDetBenchTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOCOEvaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrap_bench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meffdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientdet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeadNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mefdetTF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'effdet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision.utils\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "sys.path.append(os.path.join('./efficientdet-pytorch'))\n",
    "sys.path.append(os.path.join('./apex'))\n",
    "\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "    from apex.optimizers import FusedSGD #,FusedNovoGrad, FusedAdam, FusedLAMB\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    has_apex = False\n",
    "\n",
    "\n",
    "#has_apex = False\n",
    "print(\"Using apex:\", has_apex)\n",
    "\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, COCOEvaluator, unwrap_bench\n",
    "from effdet.efficientdet import HeadNet\n",
    "from data import transforms as efdetTF\n",
    "from data import create_loader, CocoDetection\n",
    "\n",
    "from timm.models import resume_checkpoint, load_checkpoint\n",
    "from timm.utils import *\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler, CosineLRScheduler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"./output/t1/logs\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定\n",
    "img_size = 640\n",
    "BatchSize = 20\n",
    "setup_default_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ローダーの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.52s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#dataset_train = CocoDetection('../../ballod_dataset/', \"./annotations/train_ball_coco.json\")\n",
    "dataset_train = CocoDetection(\"./\", \"./train_cocoAno.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = create_loader(dataset_train, batch_size=BatchSize, input_size=img_size, is_training=True, use_prefetcher=True,\n",
    "                            interpolation=\"random\", num_workers=1, distributed=False, pin_mem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n",
      "{'img_id': 999, 'bbox': array([[  0.      ,   0.      , 142.68845 ,  93.64639 ],\n",
      "       [ 47.564392, 120.665695, 141.20605 , 292.77936 ]], dtype=float32), 'cls': array([1, 1]), 'img_size': (640, 347), 'img_scale': 2.111453458856479}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "img, target = dataset_train[100]\n",
    "img = img.transpose(1,2,0)\n",
    "print(img.shape)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "print(target)\n",
    "boxes = target['bbox']\n",
    "for box in boxes:\n",
    "    cv2.rectangle(img, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = CocoDetection(\"./\", \"./eval_cocoAno.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_eval = create_loader(dataset_eval, batch_size=BatchSize, input_size=img_size, is_training=False, use_prefetcher=True,\n",
    "                            interpolation=\"\", num_workers=1, distributed=False, pin_mem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = COCOEvaluator(dataset_eval.coco, distributed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf_efficientdet_d1\"\n",
    "pretrained = True\n",
    "prefetcher = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations\t\t      eval_cocoAno.json     tf_efficientdet_d1.pth\r\n",
      "annotations_trainval2017.zip  eval_create.ipynb     train2017\r\n",
      "apex\t\t\t      eval_issia\t    train_cocoAno.json\r\n",
      "cocoAno.json\t\t      issia\t\t    train_issia\r\n",
      "dataset_create.ipynb\t      learning_efdet.ipynb  val2017\r\n",
      "ed_pytorch.ipynb\t      output\r\n",
      "efficientdet-pytorch\t      temp.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "def get_net():\n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load('./tf_efficientdet_d1.pth')\n",
    "    net.load_state_dict(checkpoint)\n",
    "    config.num_classes = 2\n",
    "    config.image_size = img_size\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(net, config)\n",
    "model = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda use\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(\"Cuda use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_learning_rate = 1e-5\n",
    "learning_rate = 1e-5 * 4\n",
    "learning_rate_min = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "parameters = model.parameters()\n",
    "use_amp = False\n",
    "if has_apex:\n",
    "    optimizer = FusedSGD(parameters, lr=learning_rate, momentum=0.9, weight_decay=4e-5, nesterov=True)\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    use_amp = True\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(parameters, lr=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ema = ModelEma(model, decay=0.9998)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = \"loss\"\n",
    "\n",
    "exp_name = '-'.join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_name])\n",
    "output_dir = get_outdir(\"./output/t1\", 'train', exp_name)\n",
    "decreasing = True if eval_metric == 'loss' else False\n",
    "saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スケジューラ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "warmup_epochs = 5\n",
    "cooldown_epochs = 10\n",
    "num_epochs = epochs\n",
    "noise_range = None\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "            optimizer,\n",
    "            t_initial=num_epochs,\n",
    "            t_mul=1.0,\n",
    "            lr_min=learning_rate_min,\n",
    "            decay_rate=0.1,\n",
    "            warmup_lr_init=warmup_learning_rate,\n",
    "            warmup_t=warmup_epochs,\n",
    "            cycle_limit=1,\n",
    "            t_in_epochs=True,\n",
    "            noise_range_t=noise_range,\n",
    "            noise_pct=0.67,\n",
    "            noise_std=1.0,\n",
    "            noise_seed=42,\n",
    "        )\n",
    "num_epochs = lr_scheduler.get_cycle_length() + cooldown_epochs\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, loader, optimizer,  lr_scheduler=None, saver=None, output_dir='', use_amp=False, model_ema=None):\n",
    "    \"\"\"\n",
    "    if args.prefetcher and args.mixup > 0 and loader.mixup_enabled:\n",
    "        if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n",
    "            loader.mixup_enabled = False\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        #print(input, target)\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "\n",
    "        output = model(input, target)\n",
    "        loss = output['loss']\n",
    "        \n",
    "        losses_m.update(loss.item(), input.size(0))\n",
    "        #print(loss)\n",
    "        #print(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        clip_grad = 10\n",
    "        if use_amp:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), clip_grad)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "        num_updates += 1\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % 50 == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "\n",
    "            if True:\n",
    "                logging.info(\n",
    "                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'LR: {lr:.3e}  '\n",
    "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                        epoch,\n",
    "                        batch_idx, len(loader),\n",
    "                        100. * batch_idx / last_idx,\n",
    "                        loss=losses_m,\n",
    "                        batch_time=batch_time_m,\n",
    "                        rate=input.size(0) * 1 / batch_time_m.val,\n",
    "                        rate_avg=input.size(0) * 1 / batch_time_m.avg,\n",
    "                        lr=lr,\n",
    "                        data_time=data_time_m))\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('loss', 1)])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = OrderedDict([('loss', 1)])\n",
    "print(a)\n",
    "print(a['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, evaluator=None, log_suffix=''):\n",
    "    batch_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            last_batch = batch_idx == last_idx\n",
    "\n",
    "            output = model(input, target)\n",
    "            loss = output['loss']\n",
    "          \n",
    "\n",
    "            if evaluator is not None:\n",
    "                #detect = output['detections'].to('cpu').type(torch.int32)\n",
    "                #target['bbox'].to('cpu').type(torch.int32)\n",
    "                evaluator.add_predictions(output['detections'], target)\n",
    "                \n",
    "      \n",
    "            reduced_loss = loss.data\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            losses_m.update(reduced_loss.item(), input.size(0))\n",
    "\n",
    "            batch_time_m.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if True and (last_batch or batch_idx % 50 == 0):\n",
    "                log_name = 'Test' + log_suffix\n",
    "                logging.info(\n",
    "                    '{0}: [{1:>4d}/{2}]  '\n",
    "                    'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '.format(\n",
    "                        log_name, batch_idx, last_idx, batch_time=batch_time_m, loss=losses_m))\n",
    "            \n",
    "    metrics = OrderedDict([('loss', losses_m.avg)])\n",
    "    if evaluator is not None:\n",
    "        metrics['map'] = evaluator.evaluate()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 0 [   0/3456 (  0%)]  Loss: 537.004883 (537.0049)  Time: 8.715s,    2.29/s  (8.715s,    2.29/s)  LR: 1.000e-04  Data: 0.798 (0.798)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 0 [  50/3456 (  1%)]  Loss: 373.103271 (468.3328)  Time: 0.545s,   36.67/s  (0.705s,   28.36/s)  LR: 1.000e-04  Data: 0.011 (0.026)\n",
      "Train: 0 [ 100/3456 (  3%)]  Loss: 303.381226 (424.4337)  Time: 0.547s,   36.55/s  (0.626s,   31.93/s)  LR: 1.000e-04  Data: 0.011 (0.018)\n",
      "Train: 0 [ 150/3456 (  4%)]  Loss: 179.209183 (376.1252)  Time: 0.548s,   36.50/s  (0.600s,   33.36/s)  LR: 1.000e-04  Data: 0.011 (0.016)\n",
      "Train: 0 [ 200/3456 (  6%)]  Loss: 143.946930 (335.0638)  Time: 0.545s,   36.68/s  (0.587s,   34.07/s)  LR: 1.000e-04  Data: 0.010 (0.014)\n",
      "Train: 0 [ 250/3456 (  7%)]  Loss: 82.591270 (295.6014)  Time: 0.545s,   36.73/s  (0.579s,   34.57/s)  LR: 1.000e-04  Data: 0.010 (0.014)\n",
      "Train: 0 [ 300/3456 (  9%)]  Loss: 46.317493 (260.8847)  Time: 0.548s,   36.47/s  (0.573s,   34.90/s)  LR: 1.000e-04  Data: 0.011 (0.013)\n",
      "Train: 0 [ 350/3456 ( 10%)]  Loss: 27.238207 (230.6961)  Time: 0.548s,   36.52/s  (0.569s,   35.15/s)  LR: 1.000e-04  Data: 0.011 (0.013)\n",
      "Train: 0 [ 400/3456 ( 12%)]  Loss: 13.086912 (204.8871)  Time: 0.542s,   36.90/s  (0.566s,   35.34/s)  LR: 1.000e-04  Data: 0.010 (0.012)\n",
      "Train: 0 [ 450/3456 ( 13%)]  Loss:  9.244216 (183.4345)  Time: 0.565s,   35.37/s  (0.565s,   35.39/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 500/3456 ( 14%)]  Loss:  3.248085 (165.7008)  Time: 0.528s,   37.90/s  (0.563s,   35.54/s)  LR: 1.000e-04  Data: 0.010 (0.012)\n",
      "Train: 0 [ 550/3456 ( 16%)]  Loss:  3.309918 (151.0319)  Time: 0.528s,   37.84/s  (0.560s,   35.74/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 600/3456 ( 17%)]  Loss:  2.105419 (138.7267)  Time: 0.527s,   37.93/s  (0.557s,   35.91/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 650/3456 ( 19%)]  Loss:  2.489633 (128.2851)  Time: 0.525s,   38.11/s  (0.555s,   36.06/s)  LR: 1.000e-04  Data: 0.010 (0.012)\n",
      "Train: 0 [ 700/3456 ( 20%)]  Loss:  2.854556 (119.3218)  Time: 0.540s,   37.02/s  (0.554s,   36.12/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 750/3456 ( 22%)]  Loss:  2.236787 (111.5445)  Time: 0.544s,   36.80/s  (0.553s,   36.17/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 800/3456 ( 23%)]  Loss:  2.357997 (104.7334)  Time: 0.543s,   36.81/s  (0.552s,   36.21/s)  LR: 1.000e-04  Data: 0.011 (0.012)\n",
      "Train: 0 [ 850/3456 ( 25%)]  Loss:  2.050818 (98.7173)  Time: 0.542s,   36.91/s  (0.552s,   36.25/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [ 900/3456 ( 26%)]  Loss:  2.579299 (93.3657)  Time: 0.541s,   36.97/s  (0.551s,   36.27/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [ 950/3456 ( 27%)]  Loss:  2.330697 (88.5689)  Time: 0.541s,   36.95/s  (0.551s,   36.29/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1000/3456 ( 29%)]  Loss:  2.499003 (84.2517)  Time: 0.541s,   37.00/s  (0.551s,   36.32/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1050/3456 ( 30%)]  Loss:  1.703448 (80.3444)  Time: 0.542s,   36.93/s  (0.550s,   36.34/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1100/3456 ( 32%)]  Loss:  2.256632 (76.7878)  Time: 0.543s,   36.86/s  (0.550s,   36.36/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1150/3456 ( 33%)]  Loss:  2.148593 (73.5423)  Time: 0.544s,   36.74/s  (0.550s,   36.38/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1200/3456 ( 35%)]  Loss:  1.725296 (70.5641)  Time: 0.547s,   36.57/s  (0.550s,   36.40/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1250/3456 ( 36%)]  Loss:  2.267004 (67.8240)  Time: 0.547s,   36.54/s  (0.549s,   36.41/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1300/3456 ( 38%)]  Loss:  1.837821 (65.2929)  Time: 0.540s,   37.05/s  (0.549s,   36.42/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1350/3456 ( 39%)]  Loss:  2.315615 (62.9484)  Time: 0.543s,   36.81/s  (0.549s,   36.43/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1400/3456 ( 41%)]  Loss:  1.850832 (60.7726)  Time: 0.546s,   36.61/s  (0.549s,   36.45/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1450/3456 ( 42%)]  Loss:  2.217752 (58.7433)  Time: 0.548s,   36.50/s  (0.549s,   36.46/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1500/3456 ( 43%)]  Loss:  1.881778 (56.8504)  Time: 0.527s,   37.93/s  (0.548s,   36.50/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1550/3456 ( 45%)]  Loss:  2.442414 (55.0800)  Time: 0.528s,   37.89/s  (0.547s,   36.55/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1600/3456 ( 46%)]  Loss:  1.699821 (53.4205)  Time: 0.527s,   37.94/s  (0.547s,   36.59/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1650/3456 ( 48%)]  Loss:  1.898081 (51.8608)  Time: 0.526s,   38.01/s  (0.546s,   36.63/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1700/3456 ( 49%)]  Loss:  2.150199 (50.3894)  Time: 0.526s,   38.02/s  (0.545s,   36.66/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1750/3456 ( 51%)]  Loss:  2.257931 (49.0039)  Time: 0.528s,   37.85/s  (0.545s,   36.69/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1800/3456 ( 52%)]  Loss:  1.683587 (47.6926)  Time: 0.526s,   38.02/s  (0.544s,   36.73/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [1850/3456 ( 54%)]  Loss:  1.774635 (46.4525)  Time: 0.542s,   36.93/s  (0.544s,   36.75/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1900/3456 ( 55%)]  Loss:  1.719703 (45.2780)  Time: 0.708s,   28.23/s  (0.544s,   36.74/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [1950/3456 ( 56%)]  Loss:  1.720384 (44.1648)  Time: 0.544s,   36.78/s  (0.544s,   36.74/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2000/3456 ( 58%)]  Loss:  2.090752 (43.1066)  Time: 0.524s,   38.15/s  (0.544s,   36.77/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2050/3456 ( 59%)]  Loss:  1.500724 (42.1018)  Time: 0.529s,   37.83/s  (0.544s,   36.79/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2100/3456 ( 61%)]  Loss:  1.379121 (41.1410)  Time: 0.528s,   37.89/s  (0.543s,   36.82/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2150/3456 ( 62%)]  Loss:  1.944658 (40.2258)  Time: 0.531s,   37.68/s  (0.543s,   36.85/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2200/3456 ( 64%)]  Loss:  2.157597 (39.3532)  Time: 0.530s,   37.77/s  (0.543s,   36.86/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2250/3456 ( 65%)]  Loss:  1.650540 (38.5188)  Time: 0.530s,   37.73/s  (0.542s,   36.88/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2300/3456 ( 67%)]  Loss:  1.570957 (37.7198)  Time: 0.525s,   38.06/s  (0.542s,   36.91/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2350/3456 ( 68%)]  Loss:  1.601312 (36.9548)  Time: 0.523s,   38.21/s  (0.542s,   36.93/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2400/3456 ( 69%)]  Loss:  1.515766 (36.2220)  Time: 0.530s,   37.74/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2450/3456 ( 71%)]  Loss:  1.794540 (35.5193)  Time: 0.541s,   37.00/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2500/3456 ( 72%)]  Loss:  1.542692 (34.8420)  Time: 0.541s,   36.95/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2550/3456 ( 74%)]  Loss:  1.515921 (34.1920)  Time: 0.542s,   36.93/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2600/3456 ( 75%)]  Loss:  1.625787 (33.5674)  Time: 0.543s,   36.80/s  (0.541s,   36.94/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2650/3456 ( 77%)]  Loss:  1.442858 (32.9656)  Time: 0.530s,   37.75/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [2700/3456 ( 78%)]  Loss:  1.599943 (32.3871)  Time: 0.541s,   36.94/s  (0.541s,   36.97/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2750/3456 ( 80%)]  Loss:  1.431970 (31.8287)  Time: 0.541s,   36.98/s  (0.541s,   36.97/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2800/3456 ( 81%)]  Loss:  1.735286 (31.2894)  Time: 0.541s,   36.94/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2850/3456 ( 82%)]  Loss:  1.515309 (30.7689)  Time: 0.542s,   36.87/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2900/3456 ( 84%)]  Loss:  1.469774 (30.2664)  Time: 0.540s,   37.02/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [2950/3456 ( 85%)]  Loss:  1.464272 (29.7818)  Time: 0.541s,   36.96/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [3000/3456 ( 87%)]  Loss:  1.608761 (29.3115)  Time: 0.545s,   36.67/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [3050/3456 ( 88%)]  Loss:  1.742461 (28.8581)  Time: 0.548s,   36.50/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [3100/3456 ( 90%)]  Loss:  1.235657 (28.4189)  Time: 0.525s,   38.06/s  (0.541s,   36.95/s)  LR: 1.000e-04  Data: 0.010 (0.011)\n",
      "Train: 0 [3150/3456 ( 91%)]  Loss:  1.832589 (27.9936)  Time: 0.543s,   36.85/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [3200/3456 ( 93%)]  Loss:  1.371948 (27.5811)  Time: 0.548s,   36.52/s  (0.541s,   36.96/s)  LR: 1.000e-04  Data: 0.011 (0.011)\n",
      "Train: 0 [3250/3456 ( 94%)]  Loss: 28.668535 (27.4774)  Time: 0.974s,   20.54/s  (0.546s,   36.63/s)  LR: 1.000e-04  Data: 0.424 (0.015)\n",
      "Train: 0 [3300/3456 ( 96%)]  Loss: 17.211542 (27.3822)  Time: 0.927s,   21.57/s  (0.552s,   36.23/s)  LR: 1.000e-04  Data: 0.373 (0.021)\n",
      "Train: 0 [3350/3456 ( 97%)]  Loss:  6.683560 (27.1503)  Time: 0.909s,   22.00/s  (0.558s,   35.86/s)  LR: 1.000e-04  Data: 0.354 (0.026)\n",
      "Train: 0 [3400/3456 ( 98%)]  Loss:  2.558588 (26.8021)  Time: 0.968s,   20.66/s  (0.563s,   35.51/s)  LR: 1.000e-04  Data: 0.418 (0.032)\n",
      "Train: 0 [3450/3456 (100%)]  Loss:  2.110681 (26.4469)  Time: 0.953s,   20.98/s  (0.569s,   35.18/s)  LR: 1.000e-04  Data: 0.422 (0.037)\n",
      "Train: 0 [3455/3456 (100%)]  Loss:  3.464862 (26.4153)  Time: 6.243s,    1.76/s  (0.571s,   19.28/s)  LR: 1.000e-04  Data: 0.035 (0.037)\n",
      "Test: [   0/147]  Time: 1.018 (1.018)  Loss:  1.0794 (1.0794)  \n",
      "Test: [  50/147]  Time: 0.299 (0.319)  Loss:  1.2983 (1.2403)  \n",
      "Test: [ 100/147]  Time: 0.318 (0.310)  Loss:  1.3244 (1.2280)  \n",
      "Test: [ 147/147]  Time: 0.275 (0.357)  Loss:  3.1078 (1.3770)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      "\n",
      "Train: 1 [   0/3456 (  0%)]  Loss:  1.142575 (1.1426)  Time: 1.419s,   14.09/s  (1.419s,   14.09/s)  LR: 8.080e-03  Data: 0.830 (0.830)\n",
      "Train: 1 [  50/3456 (  1%)]  Loss:  0.514484 (0.6956)  Time: 0.528s,   37.88/s  (0.551s,   36.33/s)  LR: 8.080e-03  Data: 0.010 (0.031)\n",
      "Train: 1 [ 100/3456 (  3%)]  Loss:  0.649518 (0.6489)  Time: 0.530s,   37.73/s  (0.538s,   37.16/s)  LR: 8.080e-03  Data: 0.010 (0.021)\n",
      "Train: 1 [ 150/3456 (  4%)]  Loss:  0.520607 (0.6191)  Time: 0.525s,   38.11/s  (0.534s,   37.44/s)  LR: 8.080e-03  Data: 0.010 (0.017)\n",
      "Train: 1 [ 200/3456 (  6%)]  Loss:  0.571760 (0.6033)  Time: 0.522s,   38.31/s  (0.532s,   37.59/s)  LR: 8.080e-03  Data: 0.010 (0.016)\n",
      "Train: 1 [ 250/3456 (  7%)]  Loss:  0.589087 (0.5940)  Time: 0.527s,   37.98/s  (0.531s,   37.68/s)  LR: 8.080e-03  Data: 0.010 (0.015)\n",
      "Train: 1 [ 300/3456 (  9%)]  Loss:  0.636564 (0.5867)  Time: 0.526s,   38.01/s  (0.531s,   37.68/s)  LR: 8.080e-03  Data: 0.011 (0.014)\n",
      "Train: 1 [ 350/3456 ( 10%)]  Loss:  0.550187 (0.5797)  Time: 0.523s,   38.24/s  (0.530s,   37.73/s)  LR: 8.080e-03  Data: 0.010 (0.013)\n",
      "Train: 1 [ 400/3456 ( 12%)]  Loss:  0.504531 (0.5726)  Time: 0.529s,   37.78/s  (0.530s,   37.77/s)  LR: 8.080e-03  Data: 0.010 (0.013)\n",
      "Train: 1 [ 450/3456 ( 13%)]  Loss:  0.467604 (0.5698)  Time: 0.549s,   36.44/s  (0.530s,   37.74/s)  LR: 8.080e-03  Data: 0.011 (0.013)\n",
      "Train: 1 [ 500/3456 ( 14%)]  Loss:  0.567109 (0.5654)  Time: 0.547s,   36.54/s  (0.531s,   37.64/s)  LR: 8.080e-03  Data: 0.011 (0.013)\n",
      "Train: 1 [ 550/3456 ( 16%)]  Loss:  0.546926 (0.5603)  Time: 0.548s,   36.52/s  (0.533s,   37.53/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 600/3456 ( 17%)]  Loss:  0.537072 (0.5573)  Time: 0.542s,   36.88/s  (0.534s,   37.45/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 650/3456 ( 19%)]  Loss:  0.581770 (0.5543)  Time: 0.542s,   36.87/s  (0.535s,   37.39/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 700/3456 ( 20%)]  Loss:  0.552322 (0.5515)  Time: 0.545s,   36.69/s  (0.535s,   37.38/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 750/3456 ( 22%)]  Loss:  0.590854 (0.5500)  Time: 0.551s,   36.32/s  (0.536s,   37.34/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 800/3456 ( 23%)]  Loss:  0.619432 (0.5478)  Time: 0.542s,   36.89/s  (0.536s,   37.29/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 850/3456 ( 25%)]  Loss:  0.545063 (0.5456)  Time: 0.545s,   36.69/s  (0.537s,   37.26/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 900/3456 ( 26%)]  Loss:  0.469316 (0.5438)  Time: 0.547s,   36.55/s  (0.537s,   37.22/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [ 950/3456 ( 27%)]  Loss:  0.505843 (0.5412)  Time: 0.542s,   36.87/s  (0.538s,   37.19/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1000/3456 ( 29%)]  Loss:  0.450732 (0.5394)  Time: 0.548s,   36.46/s  (0.538s,   37.15/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1050/3456 ( 30%)]  Loss:  0.623155 (0.5374)  Time: 0.550s,   36.36/s  (0.539s,   37.13/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1100/3456 ( 32%)]  Loss:  0.559609 (0.5361)  Time: 0.547s,   36.58/s  (0.539s,   37.11/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1150/3456 ( 33%)]  Loss:  0.464433 (0.5346)  Time: 0.547s,   36.55/s  (0.539s,   37.09/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1200/3456 ( 35%)]  Loss:  0.559843 (0.5331)  Time: 0.544s,   36.76/s  (0.540s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1250/3456 ( 36%)]  Loss:  0.563062 (0.5314)  Time: 0.542s,   36.88/s  (0.540s,   37.05/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1300/3456 ( 38%)]  Loss:  0.516507 (0.5296)  Time: 0.548s,   36.52/s  (0.540s,   37.04/s)  LR: 8.080e-03  Data: 0.011 (0.012)\n",
      "Train: 1 [1350/3456 ( 39%)]  Loss:  0.593825 (0.5281)  Time: 0.542s,   36.89/s  (0.540s,   37.02/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1400/3456 ( 41%)]  Loss:  0.453395 (0.5265)  Time: 0.539s,   37.07/s  (0.540s,   37.01/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1450/3456 ( 42%)]  Loss:  0.540919 (0.5255)  Time: 0.542s,   36.92/s  (0.541s,   37.00/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1500/3456 ( 43%)]  Loss:  0.568230 (0.5244)  Time: 0.544s,   36.78/s  (0.541s,   36.99/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1550/3456 ( 45%)]  Loss:  0.364876 (0.5238)  Time: 0.541s,   36.97/s  (0.541s,   36.98/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1600/3456 ( 46%)]  Loss:  0.495818 (0.5223)  Time: 0.543s,   36.82/s  (0.541s,   36.98/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1650/3456 ( 48%)]  Loss:  0.477613 (0.5213)  Time: 0.548s,   36.53/s  (0.541s,   36.97/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1700/3456 ( 49%)]  Loss:  0.430305 (0.5207)  Time: 0.545s,   36.69/s  (0.541s,   36.96/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1750/3456 ( 51%)]  Loss:  0.476850 (0.5198)  Time: 0.541s,   36.97/s  (0.541s,   36.95/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1800/3456 ( 52%)]  Loss:  0.543066 (0.5190)  Time: 0.548s,   36.52/s  (0.541s,   36.94/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1850/3456 ( 54%)]  Loss:  0.545929 (0.5177)  Time: 0.550s,   36.39/s  (0.541s,   36.94/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1900/3456 ( 55%)]  Loss:  0.575156 (0.5171)  Time: 0.545s,   36.71/s  (0.542s,   36.93/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [1950/3456 ( 56%)]  Loss:  0.512845 (0.5162)  Time: 0.549s,   36.41/s  (0.542s,   36.93/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2000/3456 ( 58%)]  Loss:  0.509525 (0.5156)  Time: 0.547s,   36.56/s  (0.542s,   36.92/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2050/3456 ( 59%)]  Loss:  0.501287 (0.5151)  Time: 0.542s,   36.90/s  (0.542s,   36.91/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2100/3456 ( 61%)]  Loss:  0.530662 (0.5148)  Time: 0.546s,   36.60/s  (0.542s,   36.91/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2150/3456 ( 62%)]  Loss:  0.508224 (0.5142)  Time: 0.542s,   36.88/s  (0.542s,   36.91/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2200/3456 ( 64%)]  Loss:  0.452802 (0.5133)  Time: 0.544s,   36.77/s  (0.542s,   36.91/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2250/3456 ( 65%)]  Loss:  0.480005 (0.5125)  Time: 0.543s,   36.80/s  (0.542s,   36.90/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2300/3456 ( 67%)]  Loss:  0.489273 (0.5118)  Time: 0.542s,   36.91/s  (0.542s,   36.90/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2350/3456 ( 68%)]  Loss:  0.501927 (0.5111)  Time: 0.524s,   38.17/s  (0.542s,   36.91/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2400/3456 ( 69%)]  Loss:  0.497529 (0.5107)  Time: 0.526s,   38.01/s  (0.542s,   36.93/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2450/3456 ( 71%)]  Loss:  0.432689 (0.5103)  Time: 0.527s,   37.97/s  (0.541s,   36.96/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2500/3456 ( 72%)]  Loss:  0.396106 (0.5097)  Time: 0.526s,   38.02/s  (0.541s,   36.97/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2550/3456 ( 74%)]  Loss:  0.462879 (0.5090)  Time: 0.526s,   38.03/s  (0.541s,   36.99/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2600/3456 ( 75%)]  Loss:  0.491831 (0.5082)  Time: 0.531s,   37.67/s  (0.540s,   37.01/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2650/3456 ( 77%)]  Loss:  0.557530 (0.5080)  Time: 0.524s,   38.16/s  (0.540s,   37.03/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2700/3456 ( 78%)]  Loss:  0.436785 (0.5074)  Time: 0.523s,   38.23/s  (0.540s,   37.04/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2750/3456 ( 80%)]  Loss:  0.468587 (0.5069)  Time: 0.527s,   37.94/s  (0.540s,   37.06/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2800/3456 ( 81%)]  Loss:  0.366846 (0.5063)  Time: 0.542s,   36.88/s  (0.539s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2850/3456 ( 82%)]  Loss:  0.629273 (0.5057)  Time: 0.544s,   36.78/s  (0.540s,   37.06/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2900/3456 ( 84%)]  Loss:  0.543172 (0.5053)  Time: 0.527s,   37.94/s  (0.539s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [2950/3456 ( 85%)]  Loss:  0.507746 (0.5047)  Time: 0.546s,   36.66/s  (0.539s,   37.09/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3000/3456 ( 87%)]  Loss:  0.501241 (0.5045)  Time: 0.545s,   36.72/s  (0.539s,   37.08/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3050/3456 ( 88%)]  Loss:  0.431292 (0.5040)  Time: 0.547s,   36.54/s  (0.539s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3100/3456 ( 90%)]  Loss:  0.505159 (0.5036)  Time: 0.548s,   36.51/s  (0.540s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3150/3456 ( 91%)]  Loss:  0.557607 (0.5031)  Time: 0.547s,   36.59/s  (0.540s,   37.07/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3200/3456 ( 93%)]  Loss:  0.500345 (0.5027)  Time: 0.547s,   36.56/s  (0.540s,   37.06/s)  LR: 8.080e-03  Data: 0.011 (0.011)\n",
      "Train: 1 [3250/3456 ( 94%)]  Loss:  3.308752 (0.5334)  Time: 0.939s,   21.31/s  (0.544s,   36.78/s)  LR: 8.080e-03  Data: 0.407 (0.015)\n",
      "Train: 1 [3300/3456 ( 96%)]  Loss:  1.501442 (0.5650)  Time: 0.902s,   22.17/s  (0.549s,   36.41/s)  LR: 8.080e-03  Data: 0.369 (0.020)\n",
      "Train: 1 [3350/3456 ( 97%)]  Loss:  1.569904 (0.5926)  Time: 0.890s,   22.46/s  (0.554s,   36.07/s)  LR: 8.080e-03  Data: 0.342 (0.025)\n",
      "Train: 1 [3400/3456 ( 98%)]  Loss:  2.782637 (0.6178)  Time: 0.877s,   22.80/s  (0.559s,   35.75/s)  LR: 8.080e-03  Data: 0.326 (0.030)\n",
      "Train: 1 [3450/3456 (100%)]  Loss:  1.749690 (0.6407)  Time: 0.904s,   22.11/s  (0.564s,   35.44/s)  LR: 8.080e-03  Data: 0.353 (0.034)\n",
      "Train: 1 [3455/3456 (100%)]  Loss:  4.388725 (0.6427)  Time: 0.483s,   22.78/s  (0.565s,   19.48/s)  LR: 8.080e-03  Data: 0.036 (0.035)\n",
      "Test: [   0/147]  Time: 0.874 (0.874)  Loss:  1.9309 (1.9309)  \n",
      "Test: [  50/147]  Time: 0.277 (0.289)  Loss:  2.2619 (2.1434)  \n",
      "Test: [ 100/147]  Time: 0.276 (0.282)  Loss:  2.2964 (2.1354)  \n",
      "Test: [ 147/147]  Time: 0.299 (0.328)  Loss:  7.3518 (2.5605)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      "\n",
      "Train: 2 [   0/3456 (  0%)]  Loss:  1.720518 (1.7205)  Time: 1.472s,   13.59/s  (1.472s,   13.59/s)  LR: 1.606e-02  Data: 0.850 (0.850)\n",
      "Train: 2 [  50/3456 (  1%)]  Loss:  0.457143 (0.6399)  Time: 0.551s,   36.32/s  (0.568s,   35.24/s)  LR: 1.606e-02  Data: 0.011 (0.027)\n",
      "Train: 2 [ 100/3456 (  3%)]  Loss:  0.626778 (0.5906)  Time: 0.551s,   36.29/s  (0.557s,   35.90/s)  LR: 1.606e-02  Data: 0.011 (0.019)\n",
      "Train: 2 [ 150/3456 (  4%)]  Loss:  0.469176 (0.5608)  Time: 0.528s,   37.91/s  (0.551s,   36.30/s)  LR: 1.606e-02  Data: 0.010 (0.016)\n",
      "Train: 2 [ 200/3456 (  6%)]  Loss:  0.567097 (0.5500)  Time: 0.547s,   36.55/s  (0.545s,   36.69/s)  LR: 1.606e-02  Data: 0.011 (0.015)\n",
      "Train: 2 [ 250/3456 (  7%)]  Loss:  0.564051 (0.5449)  Time: 0.531s,   37.67/s  (0.542s,   36.89/s)  LR: 1.606e-02  Data: 0.011 (0.014)\n",
      "Train: 2 [ 300/3456 (  9%)]  Loss:  0.706845 (0.5408)  Time: 0.525s,   38.09/s  (0.540s,   37.03/s)  LR: 1.606e-02  Data: 0.011 (0.013)\n",
      "Train: 2 [ 350/3456 ( 10%)]  Loss:  0.596012 (0.5385)  Time: 0.529s,   37.82/s  (0.538s,   37.15/s)  LR: 1.606e-02  Data: 0.010 (0.013)\n",
      "Train: 2 [ 400/3456 ( 12%)]  Loss:  0.427522 (0.5340)  Time: 0.531s,   37.64/s  (0.537s,   37.24/s)  LR: 1.606e-02  Data: 0.011 (0.013)\n",
      "Train: 2 [ 450/3456 ( 13%)]  Loss:  0.492321 (0.5327)  Time: 0.545s,   36.72/s  (0.538s,   37.19/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 500/3456 ( 14%)]  Loss:  0.524748 (0.5310)  Time: 0.543s,   36.81/s  (0.539s,   37.14/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 550/3456 ( 16%)]  Loss:  0.503855 (0.5281)  Time: 0.544s,   36.77/s  (0.539s,   37.11/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 600/3456 ( 17%)]  Loss:  0.497290 (0.5252)  Time: 0.544s,   36.78/s  (0.539s,   37.08/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 650/3456 ( 19%)]  Loss:  0.552483 (0.5239)  Time: 0.545s,   36.70/s  (0.540s,   37.05/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 700/3456 ( 20%)]  Loss:  0.458903 (0.5231)  Time: 0.525s,   38.08/s  (0.540s,   37.07/s)  LR: 1.606e-02  Data: 0.010 (0.012)\n",
      "Train: 2 [ 750/3456 ( 22%)]  Loss:  0.596153 (0.5232)  Time: 0.532s,   37.61/s  (0.539s,   37.13/s)  LR: 1.606e-02  Data: 0.012 (0.012)\n",
      "Train: 2 [ 800/3456 ( 23%)]  Loss:  0.568154 (0.5221)  Time: 0.524s,   38.16/s  (0.538s,   37.18/s)  LR: 1.606e-02  Data: 0.010 (0.012)\n",
      "Train: 2 [ 850/3456 ( 25%)]  Loss:  0.468991 (0.5211)  Time: 0.526s,   38.00/s  (0.537s,   37.21/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 900/3456 ( 26%)]  Loss:  0.525498 (0.5206)  Time: 0.546s,   36.66/s  (0.538s,   37.19/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [ 950/3456 ( 27%)]  Loss:  0.527498 (0.5192)  Time: 0.553s,   36.19/s  (0.538s,   37.15/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [1000/3456 ( 29%)]  Loss:  0.475748 (0.5186)  Time: 0.551s,   36.27/s  (0.539s,   37.12/s)  LR: 1.606e-02  Data: 0.011 (0.012)\n",
      "Train: 2 [1050/3456 ( 30%)]  Loss:  0.479067 (0.5178)  Time: 0.551s,   36.31/s  (0.539s,   37.10/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1100/3456 ( 32%)]  Loss:  0.574634 (0.5177)  Time: 0.552s,   36.25/s  (0.540s,   37.06/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1150/3456 ( 33%)]  Loss:  0.409033 (0.5174)  Time: 0.549s,   36.43/s  (0.540s,   37.04/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1200/3456 ( 35%)]  Loss:  0.486421 (0.5166)  Time: 0.543s,   36.84/s  (0.540s,   37.02/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1250/3456 ( 36%)]  Loss:  0.507277 (0.5165)  Time: 0.555s,   36.06/s  (0.541s,   37.00/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1300/3456 ( 38%)]  Loss:  0.518714 (0.5160)  Time: 0.549s,   36.45/s  (0.541s,   36.98/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1350/3456 ( 39%)]  Loss:  0.553380 (0.5156)  Time: 0.550s,   36.38/s  (0.541s,   36.96/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1400/3456 ( 41%)]  Loss:  0.496386 (0.5149)  Time: 0.550s,   36.37/s  (0.541s,   36.95/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1450/3456 ( 42%)]  Loss:  0.615756 (0.5148)  Time: 0.549s,   36.40/s  (0.541s,   36.94/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1500/3456 ( 43%)]  Loss:  0.494500 (0.5148)  Time: 0.545s,   36.71/s  (0.542s,   36.93/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1550/3456 ( 45%)]  Loss:  0.420657 (0.5148)  Time: 0.545s,   36.69/s  (0.542s,   36.91/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1600/3456 ( 46%)]  Loss:  0.541943 (0.5142)  Time: 0.546s,   36.63/s  (0.542s,   36.90/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1650/3456 ( 48%)]  Loss:  0.400728 (0.5137)  Time: 0.549s,   36.41/s  (0.542s,   36.89/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1700/3456 ( 49%)]  Loss:  0.409891 (0.5133)  Time: 0.524s,   38.13/s  (0.542s,   36.89/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1750/3456 ( 51%)]  Loss:  0.494653 (0.5130)  Time: 0.527s,   37.98/s  (0.542s,   36.92/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1800/3456 ( 52%)]  Loss:  0.580326 (0.5129)  Time: 0.524s,   38.19/s  (0.541s,   36.95/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1850/3456 ( 54%)]  Loss:  0.589285 (0.5122)  Time: 0.526s,   38.00/s  (0.541s,   36.98/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1900/3456 ( 55%)]  Loss:  0.590416 (0.5121)  Time: 0.525s,   38.09/s  (0.540s,   37.01/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [1950/3456 ( 56%)]  Loss:  0.540841 (0.5117)  Time: 0.525s,   38.06/s  (0.540s,   37.03/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2000/3456 ( 58%)]  Loss:  0.439738 (0.5113)  Time: 0.525s,   38.07/s  (0.540s,   37.05/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2050/3456 ( 59%)]  Loss:  0.489035 (0.5111)  Time: 0.546s,   36.61/s  (0.540s,   37.06/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2100/3456 ( 61%)]  Loss:  0.476103 (0.5109)  Time: 0.545s,   36.67/s  (0.540s,   37.06/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2150/3456 ( 62%)]  Loss:  0.483704 (0.5107)  Time: 0.543s,   36.82/s  (0.540s,   37.05/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2200/3456 ( 64%)]  Loss:  0.437204 (0.5101)  Time: 0.547s,   36.55/s  (0.540s,   37.04/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2250/3456 ( 65%)]  Loss:  0.462350 (0.5098)  Time: 0.540s,   37.01/s  (0.540s,   37.03/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2300/3456 ( 67%)]  Loss:  0.489993 (0.5095)  Time: 0.543s,   36.84/s  (0.540s,   37.03/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2350/3456 ( 68%)]  Loss:  0.505340 (0.5090)  Time: 0.544s,   36.74/s  (0.540s,   37.02/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2400/3456 ( 69%)]  Loss:  0.574072 (0.5091)  Time: 0.548s,   36.52/s  (0.540s,   37.01/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2450/3456 ( 71%)]  Loss:  0.468408 (0.5092)  Time: 0.543s,   36.86/s  (0.540s,   37.00/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2500/3456 ( 72%)]  Loss:  0.454707 (0.5091)  Time: 0.543s,   36.85/s  (0.541s,   37.00/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2550/3456 ( 74%)]  Loss:  0.496578 (0.5089)  Time: 0.548s,   36.49/s  (0.541s,   36.99/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2600/3456 ( 75%)]  Loss:  0.501112 (0.5088)  Time: 0.544s,   36.80/s  (0.541s,   36.99/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2650/3456 ( 77%)]  Loss:  0.544721 (0.5087)  Time: 0.535s,   37.39/s  (0.541s,   36.99/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2700/3456 ( 78%)]  Loss:  0.495755 (0.5083)  Time: 0.525s,   38.11/s  (0.540s,   37.00/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2750/3456 ( 80%)]  Loss:  0.502849 (0.5082)  Time: 0.526s,   37.99/s  (0.540s,   37.02/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2800/3456 ( 81%)]  Loss:  0.484917 (0.5079)  Time: 0.525s,   38.11/s  (0.540s,   37.04/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2850/3456 ( 82%)]  Loss:  0.593455 (0.5076)  Time: 0.525s,   38.07/s  (0.540s,   37.06/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2900/3456 ( 84%)]  Loss:  0.543432 (0.5074)  Time: 0.532s,   37.61/s  (0.539s,   37.07/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [2950/3456 ( 85%)]  Loss:  0.457426 (0.5071)  Time: 0.527s,   37.98/s  (0.539s,   37.09/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [3000/3456 ( 87%)]  Loss:  0.471897 (0.5071)  Time: 0.691s,   28.93/s  (0.539s,   37.10/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [3050/3456 ( 88%)]  Loss:  0.451430 (0.5069)  Time: 0.543s,   36.86/s  (0.539s,   37.09/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [3100/3456 ( 90%)]  Loss:  0.473444 (0.5067)  Time: 0.546s,   36.61/s  (0.539s,   37.09/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [3150/3456 ( 91%)]  Loss:  0.580077 (0.5064)  Time: 0.547s,   36.56/s  (0.539s,   37.08/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n",
      "Train: 2 [3200/3456 ( 93%)]  Loss:  0.460292 (0.5062)  Time: 0.543s,   36.84/s  (0.539s,   37.07/s)  LR: 1.606e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 2 [3250/3456 ( 94%)]  Loss:  2.499962 (0.5468)  Time: 0.969s,   20.64/s  (0.544s,   36.79/s)  LR: 1.606e-02  Data: 0.431 (0.015)\n",
      "Train: 2 [3300/3456 ( 96%)]  Loss:  3.065710 (0.5924)  Time: 0.938s,   21.33/s  (0.549s,   36.42/s)  LR: 1.606e-02  Data: 0.405 (0.021)\n",
      "Train: 2 [3350/3456 ( 97%)]  Loss:  6.289629 (0.6369)  Time: 0.902s,   22.17/s  (0.554s,   36.08/s)  LR: 1.606e-02  Data: 0.368 (0.026)\n",
      "Train: 2 [3400/3456 ( 98%)]  Loss:  4.189095 (0.6788)  Time: 0.902s,   22.17/s  (0.559s,   35.76/s)  LR: 1.606e-02  Data: 0.367 (0.031)\n",
      "Train: 2 [3450/3456 (100%)]  Loss:  0.757441 (0.7145)  Time: 0.849s,   23.55/s  (0.564s,   35.46/s)  LR: 1.606e-02  Data: 0.319 (0.035)\n",
      "Train: 2 [3455/3456 (100%)]  Loss:  5.435573 (0.7191)  Time: 0.466s,   23.63/s  (0.564s,   19.50/s)  LR: 1.606e-02  Data: 0.038 (0.035)\n",
      "Test: [   0/147]  Time: 0.862 (0.862)  Loss:  2.4070 (2.4070)  \n",
      "Test: [  50/147]  Time: 0.262 (0.277)  Loss:  2.6684 (2.5473)  \n",
      "Test: [ 100/147]  Time: 0.263 (0.271)  Loss:  2.5817 (2.5336)  \n",
      "Test: [ 147/147]  Time: 0.283 (0.319)  Loss:  5.6269 (2.7803)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      "\n",
      "Train: 3 [   0/3456 (  0%)]  Loss:  2.235052 (2.2351)  Time: 1.443s,   13.86/s  (1.443s,   13.86/s)  LR: 2.404e-02  Data: 0.844 (0.844)\n",
      "Train: 3 [  50/3456 (  1%)]  Loss:  0.445856 (0.7550)  Time: 0.531s,   37.66/s  (0.544s,   36.78/s)  LR: 2.404e-02  Data: 0.011 (0.027)\n",
      "Train: 3 [ 100/3456 (  3%)]  Loss:  0.610883 (0.6562)  Time: 0.527s,   37.93/s  (0.535s,   37.40/s)  LR: 2.404e-02  Data: 0.010 (0.019)\n",
      "Train: 3 [ 150/3456 (  4%)]  Loss:  0.499102 (0.6160)  Time: 0.539s,   37.09/s  (0.531s,   37.63/s)  LR: 2.404e-02  Data: 0.010 (0.016)\n",
      "Train: 3 [ 200/3456 (  6%)]  Loss:  0.570771 (0.5956)  Time: 0.523s,   38.23/s  (0.530s,   37.74/s)  LR: 2.404e-02  Data: 0.010 (0.015)\n",
      "Train: 3 [ 250/3456 (  7%)]  Loss:  0.565892 (0.5852)  Time: 0.524s,   38.13/s  (0.529s,   37.82/s)  LR: 2.404e-02  Data: 0.010 (0.014)\n",
      "Train: 3 [ 300/3456 (  9%)]  Loss:  0.517062 (0.5756)  Time: 0.524s,   38.14/s  (0.529s,   37.77/s)  LR: 2.404e-02  Data: 0.010 (0.013)\n",
      "Train: 3 [ 350/3456 ( 10%)]  Loss:  0.596684 (0.5709)  Time: 0.524s,   38.16/s  (0.530s,   37.76/s)  LR: 2.404e-02  Data: 0.010 (0.013)\n",
      "Train: 3 [ 400/3456 ( 12%)]  Loss:  0.516478 (0.5656)  Time: 0.526s,   38.00/s  (0.529s,   37.79/s)  LR: 2.404e-02  Data: 0.011 (0.013)\n",
      "Train: 3 [ 450/3456 ( 13%)]  Loss:  0.517667 (0.5636)  Time: 0.525s,   38.09/s  (0.529s,   37.81/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 500/3456 ( 14%)]  Loss:  0.538342 (0.5607)  Time: 0.530s,   37.73/s  (0.529s,   37.83/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 550/3456 ( 16%)]  Loss:  0.510437 (0.5559)  Time: 0.526s,   37.99/s  (0.529s,   37.83/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 600/3456 ( 17%)]  Loss:  0.591061 (0.5538)  Time: 0.525s,   38.11/s  (0.529s,   37.83/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 650/3456 ( 19%)]  Loss:  0.625609 (0.5527)  Time: 0.528s,   37.90/s  (0.529s,   37.84/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 700/3456 ( 20%)]  Loss:  0.461922 (0.5507)  Time: 0.528s,   37.85/s  (0.528s,   37.85/s)  LR: 2.404e-02  Data: 0.010 (0.012)\n",
      "Train: 3 [ 750/3456 ( 22%)]  Loss:  0.557353 (0.5496)  Time: 0.524s,   38.17/s  (0.529s,   37.84/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 800/3456 ( 23%)]  Loss:  0.703360 (0.5479)  Time: 0.523s,   38.21/s  (0.528s,   37.85/s)  LR: 2.404e-02  Data: 0.011 (0.012)\n",
      "Train: 3 [ 850/3456 ( 25%)]  Loss:  0.507775 (0.5462)  Time: 0.525s,   38.10/s  (0.528s,   37.87/s)  LR: 2.404e-02  Data: 0.010 (0.012)\n",
      "Train: 3 [ 900/3456 ( 26%)]  Loss:  0.450755 (0.5456)  Time: 0.528s,   37.91/s  (0.528s,   37.88/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [ 950/3456 ( 27%)]  Loss:  0.593999 (0.5442)  Time: 0.525s,   38.10/s  (0.528s,   37.88/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1000/3456 ( 29%)]  Loss:  0.511688 (0.5435)  Time: 0.525s,   38.09/s  (0.528s,   37.89/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1050/3456 ( 30%)]  Loss:  0.437054 (0.5420)  Time: 0.523s,   38.21/s  (0.528s,   37.90/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1100/3456 ( 32%)]  Loss:  0.560489 (0.5419)  Time: 0.524s,   38.15/s  (0.528s,   37.91/s)  LR: 2.404e-02  Data: 0.010 (0.011)\n",
      "Train: 3 [1150/3456 ( 33%)]  Loss:  0.480129 (0.5406)  Time: 0.524s,   38.17/s  (0.528s,   37.91/s)  LR: 2.404e-02  Data: 0.010 (0.011)\n",
      "Train: 3 [1200/3456 ( 35%)]  Loss:  0.518724 (0.5399)  Time: 0.525s,   38.06/s  (0.527s,   37.92/s)  LR: 2.404e-02  Data: 0.010 (0.011)\n",
      "Train: 3 [1250/3456 ( 36%)]  Loss:  0.558613 (0.5396)  Time: 0.530s,   37.77/s  (0.527s,   37.92/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1300/3456 ( 38%)]  Loss:  0.564705 (0.5385)  Time: 0.545s,   36.71/s  (0.528s,   37.89/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1350/3456 ( 39%)]  Loss:  0.577791 (0.5375)  Time: 0.553s,   36.16/s  (0.528s,   37.84/s)  LR: 2.404e-02  Data: 0.012 (0.011)\n",
      "Train: 3 [1400/3456 ( 41%)]  Loss:  0.497707 (0.5366)  Time: 0.543s,   36.82/s  (0.529s,   37.81/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1450/3456 ( 42%)]  Loss:  0.667951 (0.5362)  Time: 0.543s,   36.84/s  (0.530s,   37.77/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1500/3456 ( 43%)]  Loss:  0.543148 (0.5357)  Time: 0.544s,   36.73/s  (0.530s,   37.73/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1550/3456 ( 45%)]  Loss:  0.426621 (0.5356)  Time: 0.546s,   36.65/s  (0.531s,   37.69/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1600/3456 ( 46%)]  Loss:  0.486001 (0.5351)  Time: 0.525s,   38.08/s  (0.531s,   37.69/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1650/3456 ( 48%)]  Loss:  0.406821 (0.5346)  Time: 0.549s,   36.44/s  (0.531s,   37.67/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1700/3456 ( 49%)]  Loss:  0.457454 (0.5343)  Time: 0.546s,   36.65/s  (0.531s,   37.64/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1750/3456 ( 51%)]  Loss:  0.486197 (0.5341)  Time: 0.547s,   36.57/s  (0.532s,   37.61/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1800/3456 ( 52%)]  Loss:  0.540946 (0.5338)  Time: 0.543s,   36.82/s  (0.532s,   37.58/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1850/3456 ( 54%)]  Loss:  0.513153 (0.5331)  Time: 0.544s,   36.73/s  (0.533s,   37.56/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1900/3456 ( 55%)]  Loss:  0.586095 (0.5327)  Time: 0.543s,   36.82/s  (0.533s,   37.54/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [1950/3456 ( 56%)]  Loss:  0.503730 (0.5324)  Time: 0.547s,   36.53/s  (0.533s,   37.51/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2000/3456 ( 58%)]  Loss:  0.482956 (0.5324)  Time: 0.544s,   36.80/s  (0.533s,   37.50/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2050/3456 ( 59%)]  Loss:  0.502605 (0.5321)  Time: 0.543s,   36.82/s  (0.534s,   37.48/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2100/3456 ( 61%)]  Loss:  0.517492 (0.5322)  Time: 0.533s,   37.56/s  (0.534s,   37.46/s)  LR: 2.404e-02  Data: 0.014 (0.011)\n",
      "Train: 3 [2150/3456 ( 62%)]  Loss:  0.550405 (0.5318)  Time: 0.542s,   36.87/s  (0.534s,   37.45/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2200/3456 ( 64%)]  Loss:  0.460520 (0.5309)  Time: 0.548s,   36.47/s  (0.534s,   37.43/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2250/3456 ( 65%)]  Loss:  0.605767 (0.5304)  Time: 0.540s,   37.01/s  (0.535s,   37.42/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2300/3456 ( 67%)]  Loss:  0.485235 (0.5297)  Time: 0.546s,   36.63/s  (0.535s,   37.40/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2350/3456 ( 68%)]  Loss:  0.498295 (0.5294)  Time: 0.539s,   37.09/s  (0.535s,   37.39/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2400/3456 ( 69%)]  Loss:  0.573005 (0.5292)  Time: 0.544s,   36.74/s  (0.535s,   37.38/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2450/3456 ( 71%)]  Loss:  0.608653 (0.5289)  Time: 0.542s,   36.87/s  (0.535s,   37.36/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2500/3456 ( 72%)]  Loss:  0.418172 (0.5286)  Time: 0.525s,   38.08/s  (0.535s,   37.36/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2550/3456 ( 74%)]  Loss:  0.581768 (0.5283)  Time: 0.532s,   37.61/s  (0.535s,   37.37/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2600/3456 ( 75%)]  Loss:  0.387644 (0.5279)  Time: 0.544s,   36.75/s  (0.535s,   37.36/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2650/3456 ( 77%)]  Loss:  0.582526 (0.5277)  Time: 0.526s,   37.99/s  (0.535s,   37.37/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2700/3456 ( 78%)]  Loss:  0.564850 (0.5272)  Time: 0.524s,   38.14/s  (0.535s,   37.38/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2750/3456 ( 80%)]  Loss:  0.564538 (0.5271)  Time: 0.528s,   37.91/s  (0.535s,   37.39/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2800/3456 ( 81%)]  Loss:  0.471093 (0.5269)  Time: 0.525s,   38.07/s  (0.535s,   37.40/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2850/3456 ( 82%)]  Loss:  0.582134 (0.5266)  Time: 0.525s,   38.09/s  (0.535s,   37.41/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2900/3456 ( 84%)]  Loss:  0.533351 (0.5266)  Time: 0.522s,   38.30/s  (0.534s,   37.42/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [2950/3456 ( 85%)]  Loss:  0.526687 (0.5262)  Time: 0.545s,   36.69/s  (0.535s,   37.42/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [3000/3456 ( 87%)]  Loss:  0.512721 (0.5260)  Time: 0.523s,   38.24/s  (0.535s,   37.41/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [3050/3456 ( 88%)]  Loss:  0.500566 (0.5256)  Time: 0.525s,   38.12/s  (0.534s,   37.43/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [3100/3456 ( 90%)]  Loss:  0.485406 (0.5251)  Time: 0.529s,   37.83/s  (0.534s,   37.43/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [3150/3456 ( 91%)]  Loss:  0.421132 (0.5246)  Time: 0.529s,   37.77/s  (0.534s,   37.44/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n",
      "Train: 3 [3200/3456 ( 93%)]  Loss:  0.481338 (0.5245)  Time: 0.525s,   38.13/s  (0.534s,   37.45/s)  LR: 2.404e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 3 [3250/3456 ( 94%)]  Loss:  5.995880 (0.5770)  Time: 0.929s,   21.54/s  (0.538s,   37.15/s)  LR: 2.404e-02  Data: 0.395 (0.015)\n",
      "Train: 3 [3300/3456 ( 96%)]  Loss:  5.503247 (0.6356)  Time: 0.900s,   22.21/s  (0.544s,   36.76/s)  LR: 2.404e-02  Data: 0.371 (0.021)\n",
      "Train: 3 [3350/3456 ( 97%)]  Loss:  7.707211 (0.6909)  Time: 0.870s,   23.00/s  (0.549s,   36.41/s)  LR: 2.404e-02  Data: 0.316 (0.026)\n",
      "Train: 3 [3400/3456 ( 98%)]  Loss:  4.499018 (0.7407)  Time: 0.878s,   22.79/s  (0.554s,   36.08/s)  LR: 2.404e-02  Data: 0.328 (0.030)\n",
      "Train: 3 [3450/3456 (100%)]  Loss:  1.068467 (0.7786)  Time: 0.893s,   22.40/s  (0.559s,   35.77/s)  LR: 2.404e-02  Data: 0.356 (0.035)\n",
      "Train: 3 [3455/3456 (100%)]  Loss:  7.320656 (0.7824)  Time: 0.484s,   22.72/s  (0.559s,   19.67/s)  LR: 2.404e-02  Data: 0.038 (0.035)\n",
      "Test: [   0/147]  Time: 0.884 (0.884)  Loss:  2.9523 (2.9523)  \n",
      "Test: [  50/147]  Time: 0.281 (0.293)  Loss:  3.4284 (3.1829)  \n",
      "Test: [ 100/147]  Time: 0.278 (0.287)  Loss:  3.0430 (3.1771)  \n",
      "Test: [ 147/147]  Time: 0.298 (0.331)  Loss: 12.0379 (3.9161)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 4 [   0/3456 (  0%)]  Loss:  2.882893 (2.8829)  Time: 1.432s,   13.96/s  (1.432s,   13.96/s)  LR: 3.202e-02  Data: 0.816 (0.816)\n",
      "Train: 4 [  50/3456 (  1%)]  Loss:  0.674570 (0.8524)  Time: 0.545s,   36.73/s  (0.563s,   35.53/s)  LR: 3.202e-02  Data: 0.011 (0.027)\n",
      "Train: 4 [ 100/3456 (  3%)]  Loss:  0.604170 (0.7272)  Time: 0.544s,   36.76/s  (0.554s,   36.10/s)  LR: 3.202e-02  Data: 0.011 (0.019)\n",
      "Train: 4 [ 150/3456 (  4%)]  Loss:  0.519141 (0.6702)  Time: 0.545s,   36.71/s  (0.551s,   36.29/s)  LR: 3.202e-02  Data: 0.011 (0.016)\n",
      "Train: 4 [ 200/3456 (  6%)]  Loss:  0.605307 (0.6431)  Time: 0.547s,   36.57/s  (0.550s,   36.35/s)  LR: 3.202e-02  Data: 0.011 (0.015)\n",
      "Train: 4 [ 250/3456 (  7%)]  Loss:  0.490797 (0.6276)  Time: 0.546s,   36.60/s  (0.549s,   36.41/s)  LR: 3.202e-02  Data: 0.011 (0.014)\n",
      "Train: 4 [ 300/3456 (  9%)]  Loss:  0.534981 (0.6172)  Time: 0.529s,   37.82/s  (0.546s,   36.64/s)  LR: 3.202e-02  Data: 0.010 (0.013)\n",
      "Train: 4 [ 350/3456 ( 10%)]  Loss:  0.620116 (0.6077)  Time: 0.531s,   37.66/s  (0.543s,   36.82/s)  LR: 3.202e-02  Data: 0.011 (0.013)\n",
      "Train: 4 [ 400/3456 ( 12%)]  Loss:  0.554740 (0.6003)  Time: 0.526s,   38.01/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.011 (0.013)\n",
      "Train: 4 [ 450/3456 ( 13%)]  Loss:  0.540117 (0.5958)  Time: 0.525s,   38.07/s  (0.540s,   37.06/s)  LR: 3.202e-02  Data: 0.010 (0.012)\n",
      "Train: 4 [ 500/3456 ( 14%)]  Loss:  0.549751 (0.5905)  Time: 0.547s,   36.59/s  (0.539s,   37.09/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 550/3456 ( 16%)]  Loss:  0.478007 (0.5849)  Time: 0.543s,   36.84/s  (0.540s,   37.06/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 600/3456 ( 17%)]  Loss:  0.616946 (0.5817)  Time: 0.546s,   36.66/s  (0.541s,   37.00/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 650/3456 ( 19%)]  Loss:  0.658973 (0.5806)  Time: 0.543s,   36.86/s  (0.541s,   36.98/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 700/3456 ( 20%)]  Loss:  0.540342 (0.5776)  Time: 0.544s,   36.77/s  (0.541s,   36.96/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 750/3456 ( 22%)]  Loss:  0.592129 (0.5763)  Time: 0.546s,   36.65/s  (0.541s,   36.94/s)  LR: 3.202e-02  Data: 0.012 (0.012)\n",
      "Train: 4 [ 800/3456 ( 23%)]  Loss:  0.625816 (0.5741)  Time: 0.541s,   36.95/s  (0.542s,   36.92/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 850/3456 ( 25%)]  Loss:  0.617073 (0.5719)  Time: 0.542s,   36.90/s  (0.542s,   36.89/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 900/3456 ( 26%)]  Loss:  0.385393 (0.5706)  Time: 0.546s,   36.62/s  (0.542s,   36.87/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [ 950/3456 ( 27%)]  Loss:  0.651579 (0.5691)  Time: 0.541s,   36.97/s  (0.543s,   36.86/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [1000/3456 ( 29%)]  Loss:  0.515091 (0.5675)  Time: 0.541s,   37.00/s  (0.543s,   36.85/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [1050/3456 ( 30%)]  Loss:  0.581998 (0.5658)  Time: 0.547s,   36.54/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [1100/3456 ( 32%)]  Loss:  0.514385 (0.5648)  Time: 0.542s,   36.90/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [1150/3456 ( 33%)]  Loss:  0.523589 (0.5634)  Time: 0.541s,   36.99/s  (0.543s,   36.83/s)  LR: 3.202e-02  Data: 0.011 (0.012)\n",
      "Train: 4 [1200/3456 ( 35%)]  Loss:  0.555911 (0.5625)  Time: 0.530s,   37.72/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1250/3456 ( 36%)]  Loss:  0.559189 (0.5613)  Time: 0.544s,   36.79/s  (0.543s,   36.86/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1300/3456 ( 38%)]  Loss:  0.543334 (0.5598)  Time: 0.547s,   36.58/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1350/3456 ( 39%)]  Loss:  0.562804 (0.5585)  Time: 0.543s,   36.84/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1400/3456 ( 41%)]  Loss:  0.493685 (0.5576)  Time: 0.545s,   36.68/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1450/3456 ( 42%)]  Loss:  0.584350 (0.5573)  Time: 0.545s,   36.69/s  (0.543s,   36.83/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1500/3456 ( 43%)]  Loss:  0.572418 (0.5565)  Time: 0.545s,   36.69/s  (0.543s,   36.82/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1550/3456 ( 45%)]  Loss:  0.483032 (0.5556)  Time: 0.546s,   36.63/s  (0.543s,   36.82/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1600/3456 ( 46%)]  Loss:  0.623829 (0.5546)  Time: 0.527s,   37.97/s  (0.543s,   36.82/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1650/3456 ( 48%)]  Loss:  0.484769 (0.5538)  Time: 0.525s,   38.11/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1700/3456 ( 49%)]  Loss:  0.491557 (0.5532)  Time: 0.546s,   36.63/s  (0.543s,   36.84/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1750/3456 ( 51%)]  Loss:  0.603232 (0.5525)  Time: 0.528s,   37.91/s  (0.543s,   36.86/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1800/3456 ( 52%)]  Loss:  0.597318 (0.5521)  Time: 0.526s,   38.01/s  (0.542s,   36.89/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1850/3456 ( 54%)]  Loss:  0.471188 (0.5510)  Time: 0.525s,   38.10/s  (0.542s,   36.92/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1900/3456 ( 55%)]  Loss:  0.591960 (0.5505)  Time: 0.527s,   37.94/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [1950/3456 ( 56%)]  Loss:  0.483645 (0.5500)  Time: 0.545s,   36.73/s  (0.541s,   36.97/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2000/3456 ( 58%)]  Loss:  0.475453 (0.5494)  Time: 0.545s,   36.70/s  (0.541s,   36.97/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2050/3456 ( 59%)]  Loss:  0.500180 (0.5488)  Time: 0.711s,   28.13/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2100/3456 ( 61%)]  Loss:  0.515436 (0.5485)  Time: 0.541s,   36.95/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2150/3456 ( 62%)]  Loss:  0.637472 (0.5484)  Time: 0.546s,   36.64/s  (0.541s,   36.94/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2200/3456 ( 64%)]  Loss:  0.501675 (0.5473)  Time: 0.550s,   36.37/s  (0.542s,   36.93/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2250/3456 ( 65%)]  Loss:  0.487064 (0.5466)  Time: 0.547s,   36.59/s  (0.542s,   36.93/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2300/3456 ( 67%)]  Loss:  0.504385 (0.5459)  Time: 0.548s,   36.48/s  (0.542s,   36.92/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2350/3456 ( 68%)]  Loss:  0.491244 (0.5453)  Time: 0.548s,   36.53/s  (0.542s,   36.91/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2400/3456 ( 69%)]  Loss:  0.589461 (0.5450)  Time: 0.535s,   37.41/s  (0.542s,   36.91/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2450/3456 ( 71%)]  Loss:  0.561618 (0.5445)  Time: 0.530s,   37.77/s  (0.542s,   36.92/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2500/3456 ( 72%)]  Loss:  0.423358 (0.5442)  Time: 0.527s,   37.96/s  (0.541s,   36.94/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2550/3456 ( 74%)]  Loss:  0.550382 (0.5437)  Time: 0.526s,   38.02/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.010 (0.011)\n",
      "Train: 4 [2600/3456 ( 75%)]  Loss:  0.524461 (0.5434)  Time: 0.529s,   37.81/s  (0.541s,   36.95/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2650/3456 ( 77%)]  Loss:  0.572954 (0.5434)  Time: 0.529s,   37.80/s  (0.541s,   36.96/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2700/3456 ( 78%)]  Loss:  0.537922 (0.5430)  Time: 0.530s,   37.73/s  (0.541s,   36.98/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2750/3456 ( 80%)]  Loss:  0.531095 (0.5427)  Time: 0.526s,   38.02/s  (0.541s,   36.99/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2800/3456 ( 81%)]  Loss:  0.456370 (0.5424)  Time: 0.526s,   38.04/s  (0.540s,   37.01/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2850/3456 ( 82%)]  Loss:  0.621879 (0.5419)  Time: 0.550s,   36.37/s  (0.540s,   37.01/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2900/3456 ( 84%)]  Loss:  0.610769 (0.5417)  Time: 0.541s,   36.95/s  (0.540s,   37.01/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [2950/3456 ( 85%)]  Loss:  0.587851 (0.5412)  Time: 0.547s,   36.58/s  (0.540s,   37.00/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [3000/3456 ( 87%)]  Loss:  0.470964 (0.5408)  Time: 0.524s,   38.16/s  (0.540s,   37.00/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [3050/3456 ( 88%)]  Loss:  0.545343 (0.5403)  Time: 0.525s,   38.09/s  (0.540s,   37.02/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [3100/3456 ( 90%)]  Loss:  0.489779 (0.5399)  Time: 0.526s,   38.04/s  (0.540s,   37.04/s)  LR: 3.202e-02  Data: 0.010 (0.011)\n",
      "Train: 4 [3150/3456 ( 91%)]  Loss:  0.594782 (0.5396)  Time: 0.530s,   37.71/s  (0.540s,   37.05/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [3200/3456 ( 93%)]  Loss:  0.517417 (0.5394)  Time: 0.544s,   36.78/s  (0.540s,   37.04/s)  LR: 3.202e-02  Data: 0.011 (0.011)\n",
      "Train: 4 [3250/3456 ( 94%)]  Loss:  4.028433 (0.5904)  Time: 0.930s,   21.51/s  (0.544s,   36.76/s)  LR: 3.202e-02  Data: 0.380 (0.015)\n",
      "Train: 4 [3300/3456 ( 96%)]  Loss:  3.824130 (0.6416)  Time: 0.942s,   21.23/s  (0.550s,   36.39/s)  LR: 3.202e-02  Data: 0.388 (0.020)\n",
      "Train: 4 [3350/3456 ( 97%)]  Loss:  1.824919 (0.6624)  Time: 0.924s,   21.65/s  (0.555s,   36.05/s)  LR: 3.202e-02  Data: 0.389 (0.025)\n",
      "Train: 4 [3400/3456 ( 98%)]  Loss:  2.409122 (0.6769)  Time: 0.898s,   22.26/s  (0.560s,   35.73/s)  LR: 3.202e-02  Data: 0.365 (0.030)\n",
      "Train: 4 [3450/3456 (100%)]  Loss:  2.142821 (0.6896)  Time: 0.879s,   22.75/s  (0.565s,   35.43/s)  LR: 3.202e-02  Data: 0.348 (0.035)\n",
      "Train: 4 [3455/3456 (100%)]  Loss:  1.018299 (0.6907)  Time: 0.463s,   23.78/s  (0.565s,   19.48/s)  LR: 3.202e-02  Data: 0.037 (0.035)\n",
      "Test: [   0/147]  Time: 0.875 (0.875)  Loss:  3.0306 (3.0306)  \n",
      "Test: [  50/147]  Time: 0.283 (0.295)  Loss:  3.6577 (3.2493)  \n",
      "Test: [ 100/147]  Time: 0.279 (0.288)  Loss:  3.2103 (3.2488)  \n",
      "Test: [ 147/147]  Time: 0.299 (0.332)  Loss:  3.0367 (3.2365)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 5 [   0/3456 (  0%)]  Loss:  3.148353 (3.1484)  Time: 1.446s,   13.83/s  (1.446s,   13.83/s)  LR: 3.997e-02  Data: 0.816 (0.816)\n",
      "Train: 5 [  50/3456 (  1%)]  Loss:  0.689411 (1.2671)  Time: 0.543s,   36.86/s  (0.561s,   35.67/s)  LR: 3.997e-02  Data: 0.011 (0.027)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 5 [ 100/3456 (  3%)]  Loss:  0.615164 (0.9861)  Time: 0.523s,   38.21/s  (0.547s,   36.55/s)  LR: 3.997e-02  Data: 0.010 (0.019)\n",
      "Train: 5 [ 150/3456 (  4%)]  Loss:  0.583408 (0.8614)  Time: 0.544s,   36.76/s  (0.542s,   36.90/s)  LR: 3.997e-02  Data: 0.011 (0.016)\n",
      "Train: 5 [ 200/3456 (  6%)]  Loss:  0.672137 (0.7984)  Time: 0.545s,   36.71/s  (0.543s,   36.82/s)  LR: 3.997e-02  Data: 0.011 (0.015)\n",
      "Train: 5 [ 250/3456 (  7%)]  Loss:  0.595863 (0.7611)  Time: 0.544s,   36.79/s  (0.544s,   36.80/s)  LR: 3.997e-02  Data: 0.011 (0.014)\n",
      "Train: 5 [ 300/3456 (  9%)]  Loss:  0.640045 (0.7295)  Time: 0.545s,   36.71/s  (0.544s,   36.78/s)  LR: 3.997e-02  Data: 0.011 (0.013)\n",
      "Train: 5 [ 350/3456 ( 10%)]  Loss:  0.578159 (0.7071)  Time: 0.540s,   37.05/s  (0.544s,   36.77/s)  LR: 3.997e-02  Data: 0.011 (0.013)\n",
      "Train: 5 [ 400/3456 ( 12%)]  Loss:  0.520771 (0.6891)  Time: 0.548s,   36.50/s  (0.544s,   36.76/s)  LR: 3.997e-02  Data: 0.011 (0.013)\n",
      "Train: 5 [ 450/3456 ( 13%)]  Loss:  0.522382 (0.6765)  Time: 0.542s,   36.91/s  (0.545s,   36.73/s)  LR: 3.997e-02  Data: 0.011 (0.013)\n",
      "Train: 5 [ 500/3456 ( 14%)]  Loss:  0.506516 (0.6661)  Time: 0.542s,   36.91/s  (0.545s,   36.72/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 550/3456 ( 16%)]  Loss:  0.555880 (0.6554)  Time: 0.541s,   36.94/s  (0.545s,   36.72/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 600/3456 ( 17%)]  Loss:  0.608700 (0.6464)  Time: 0.543s,   36.81/s  (0.544s,   36.74/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 650/3456 ( 19%)]  Loss:  0.686626 (0.6406)  Time: 0.544s,   36.75/s  (0.544s,   36.74/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 700/3456 ( 20%)]  Loss:  0.531860 (0.6341)  Time: 0.548s,   36.53/s  (0.544s,   36.74/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 750/3456 ( 22%)]  Loss:  0.628207 (0.6282)  Time: 0.547s,   36.53/s  (0.544s,   36.75/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 800/3456 ( 23%)]  Loss:  0.667236 (0.6242)  Time: 0.548s,   36.52/s  (0.544s,   36.75/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 850/3456 ( 25%)]  Loss:  0.578437 (0.6198)  Time: 0.527s,   37.98/s  (0.544s,   36.77/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [ 900/3456 ( 26%)]  Loss:  0.516341 (0.6168)  Time: 0.523s,   38.23/s  (0.543s,   36.84/s)  LR: 3.997e-02  Data: 0.010 (0.012)\n",
      "Train: 5 [ 950/3456 ( 27%)]  Loss:  0.556297 (0.6131)  Time: 0.541s,   36.98/s  (0.542s,   36.88/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1000/3456 ( 29%)]  Loss:  0.496033 (0.6095)  Time: 0.527s,   37.98/s  (0.542s,   36.90/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1050/3456 ( 30%)]  Loss:  0.530346 (0.6061)  Time: 0.529s,   37.78/s  (0.541s,   36.96/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1100/3456 ( 32%)]  Loss:  0.586433 (0.6037)  Time: 0.526s,   38.02/s  (0.541s,   37.00/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1150/3456 ( 33%)]  Loss:  0.475063 (0.6003)  Time: 0.527s,   37.96/s  (0.540s,   37.03/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1200/3456 ( 35%)]  Loss:  0.605463 (0.5979)  Time: 0.526s,   38.03/s  (0.540s,   37.07/s)  LR: 3.997e-02  Data: 0.012 (0.012)\n",
      "Train: 5 [1250/3456 ( 36%)]  Loss:  0.662505 (0.5962)  Time: 0.523s,   38.22/s  (0.539s,   37.09/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1300/3456 ( 38%)]  Loss:  0.591344 (0.5941)  Time: 0.531s,   37.64/s  (0.539s,   37.12/s)  LR: 3.997e-02  Data: 0.011 (0.012)\n",
      "Train: 5 [1350/3456 ( 39%)]  Loss:  0.698973 (0.5920)  Time: 0.527s,   37.96/s  (0.538s,   37.15/s)  LR: 3.997e-02  Data: 0.012 (0.011)\n",
      "Train: 5 [1400/3456 ( 41%)]  Loss:  0.535133 (0.5895)  Time: 0.525s,   38.09/s  (0.538s,   37.17/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1450/3456 ( 42%)]  Loss:  0.631913 (0.5878)  Time: 0.528s,   37.91/s  (0.538s,   37.19/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1500/3456 ( 43%)]  Loss:  0.603201 (0.5864)  Time: 0.524s,   38.16/s  (0.538s,   37.21/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1550/3456 ( 45%)]  Loss:  0.448495 (0.5853)  Time: 0.533s,   37.54/s  (0.537s,   37.23/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1600/3456 ( 46%)]  Loss:  0.550475 (0.5832)  Time: 0.527s,   37.98/s  (0.537s,   37.25/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1650/3456 ( 48%)]  Loss:  0.468786 (0.5818)  Time: 0.529s,   37.77/s  (0.537s,   37.28/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1700/3456 ( 49%)]  Loss:  0.409905 (0.5804)  Time: 0.524s,   38.16/s  (0.536s,   37.30/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1750/3456 ( 51%)]  Loss:  0.513075 (0.5790)  Time: 0.529s,   37.84/s  (0.536s,   37.32/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1800/3456 ( 52%)]  Loss:  0.529545 (0.5781)  Time: 0.544s,   36.76/s  (0.536s,   37.32/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1850/3456 ( 54%)]  Loss:  0.535449 (0.5763)  Time: 0.544s,   36.79/s  (0.536s,   37.31/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1900/3456 ( 55%)]  Loss:  0.508605 (0.5750)  Time: 0.541s,   36.98/s  (0.536s,   37.30/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [1950/3456 ( 56%)]  Loss:  0.510916 (0.5740)  Time: 0.545s,   36.71/s  (0.537s,   37.28/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2000/3456 ( 58%)]  Loss:  0.470421 (0.5731)  Time: 0.545s,   36.66/s  (0.537s,   37.26/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2050/3456 ( 59%)]  Loss:  0.518934 (0.5720)  Time: 0.546s,   36.63/s  (0.537s,   37.25/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2100/3456 ( 61%)]  Loss:  0.495719 (0.5711)  Time: 0.545s,   36.67/s  (0.537s,   37.23/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2150/3456 ( 62%)]  Loss:  0.545418 (0.5703)  Time: 0.545s,   36.67/s  (0.537s,   37.22/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2200/3456 ( 64%)]  Loss:  0.415953 (0.5692)  Time: 0.550s,   36.38/s  (0.537s,   37.21/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2250/3456 ( 65%)]  Loss:  0.521449 (0.5681)  Time: 0.525s,   38.10/s  (0.537s,   37.22/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2300/3456 ( 67%)]  Loss:  0.578632 (0.5668)  Time: 0.546s,   36.63/s  (0.537s,   37.21/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2350/3456 ( 68%)]  Loss:  0.519177 (0.5659)  Time: 0.541s,   36.95/s  (0.538s,   37.20/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2400/3456 ( 69%)]  Loss:  0.558915 (0.5654)  Time: 0.547s,   36.54/s  (0.538s,   37.19/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2450/3456 ( 71%)]  Loss:  0.478713 (0.5647)  Time: 0.533s,   37.49/s  (0.538s,   37.18/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2500/3456 ( 72%)]  Loss:  0.385957 (0.5640)  Time: 0.545s,   36.72/s  (0.538s,   37.17/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2550/3456 ( 74%)]  Loss:  0.505770 (0.5633)  Time: 0.542s,   36.92/s  (0.538s,   37.16/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2600/3456 ( 75%)]  Loss:  0.492487 (0.5625)  Time: 0.541s,   36.97/s  (0.538s,   37.16/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2650/3456 ( 77%)]  Loss:  0.553518 (0.5620)  Time: 0.547s,   36.58/s  (0.538s,   37.15/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2700/3456 ( 78%)]  Loss:  0.502177 (0.5614)  Time: 0.540s,   37.02/s  (0.538s,   37.14/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2750/3456 ( 80%)]  Loss:  0.493367 (0.5608)  Time: 0.527s,   37.95/s  (0.538s,   37.15/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2800/3456 ( 81%)]  Loss:  0.416967 (0.5601)  Time: 0.525s,   38.10/s  (0.538s,   37.16/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2850/3456 ( 82%)]  Loss:  0.543238 (0.5595)  Time: 0.526s,   37.99/s  (0.538s,   37.18/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2900/3456 ( 84%)]  Loss:  0.648577 (0.5591)  Time: 0.545s,   36.72/s  (0.538s,   37.17/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [2950/3456 ( 85%)]  Loss:  0.635146 (0.5585)  Time: 0.545s,   36.70/s  (0.538s,   37.17/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3000/3456 ( 87%)]  Loss:  0.484040 (0.5580)  Time: 0.547s,   36.56/s  (0.538s,   37.16/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3050/3456 ( 88%)]  Loss:  0.488310 (0.5573)  Time: 0.550s,   36.39/s  (0.538s,   37.15/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3100/3456 ( 90%)]  Loss:  0.486687 (0.5566)  Time: 0.525s,   38.09/s  (0.538s,   37.15/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3150/3456 ( 91%)]  Loss:  0.475329 (0.5560)  Time: 0.530s,   37.76/s  (0.538s,   37.17/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3200/3456 ( 93%)]  Loss:  0.544599 (0.5555)  Time: 0.525s,   38.12/s  (0.538s,   37.18/s)  LR: 3.997e-02  Data: 0.011 (0.011)\n",
      "Train: 5 [3250/3456 ( 94%)]  Loss:  1.830243 (0.5915)  Time: 0.932s,   21.47/s  (0.542s,   36.89/s)  LR: 3.997e-02  Data: 0.401 (0.015)\n",
      "Train: 5 [3300/3456 ( 96%)]  Loss:  1.253015 (0.6128)  Time: 0.884s,   22.61/s  (0.548s,   36.52/s)  LR: 3.997e-02  Data: 0.370 (0.021)\n",
      "Train: 5 [3350/3456 ( 97%)]  Loss:  1.432396 (0.6274)  Time: 0.924s,   21.64/s  (0.553s,   36.18/s)  LR: 3.997e-02  Data: 0.408 (0.026)\n",
      "Train: 5 [3400/3456 ( 98%)]  Loss:  1.268731 (0.6404)  Time: 0.881s,   22.69/s  (0.558s,   35.86/s)  LR: 3.997e-02  Data: 0.367 (0.031)\n",
      "Train: 5 [3450/3456 (100%)]  Loss:  1.137817 (0.6499)  Time: 0.870s,   22.98/s  (0.563s,   35.55/s)  LR: 3.997e-02  Data: 0.357 (0.036)\n",
      "Train: 5 [3455/3456 (100%)]  Loss:  2.125437 (0.6510)  Time: 0.442s,   24.89/s  (0.563s,   19.55/s)  LR: 3.997e-02  Data: 0.036 (0.037)\n",
      "Test: [   0/147]  Time: 0.867 (0.867)  Loss:  3.0570 (3.0570)  \n",
      "Test: [  50/147]  Time: 0.270 (0.281)  Loss:  3.6042 (3.3145)  \n",
      "Test: [ 100/147]  Time: 0.271 (0.275)  Loss:  3.2386 (3.3162)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.321)  Loss:  1.0144 (3.1229)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 6 [   0/3456 (  0%)]  Loss:  3.320444 (3.3204)  Time: 1.416s,   14.12/s  (1.416s,   14.12/s)  LR: 3.996e-02  Data: 0.804 (0.804)\n",
      "Train: 6 [  50/3456 (  1%)]  Loss:  0.581713 (1.3288)  Time: 0.526s,   38.02/s  (0.542s,   36.90/s)  LR: 3.996e-02  Data: 0.011 (0.026)\n",
      "Train: 6 [ 100/3456 (  3%)]  Loss:  0.615294 (1.0159)  Time: 0.525s,   38.13/s  (0.534s,   37.47/s)  LR: 3.996e-02  Data: 0.010 (0.018)\n",
      "Train: 6 [ 150/3456 (  4%)]  Loss:  0.570305 (0.8727)  Time: 0.525s,   38.12/s  (0.531s,   37.69/s)  LR: 3.996e-02  Data: 0.010 (0.016)\n",
      "Train: 6 [ 200/3456 (  6%)]  Loss:  0.544823 (0.8021)  Time: 0.525s,   38.10/s  (0.530s,   37.70/s)  LR: 3.996e-02  Data: 0.011 (0.014)\n",
      "Train: 6 [ 250/3456 (  7%)]  Loss:  0.576306 (0.7568)  Time: 0.523s,   38.21/s  (0.529s,   37.78/s)  LR: 3.996e-02  Data: 0.010 (0.014)\n",
      "Train: 6 [ 300/3456 (  9%)]  Loss:  0.554073 (0.7260)  Time: 0.523s,   38.21/s  (0.529s,   37.84/s)  LR: 3.996e-02  Data: 0.011 (0.013)\n",
      "Train: 6 [ 350/3456 ( 10%)]  Loss:  0.524733 (0.7025)  Time: 0.522s,   38.32/s  (0.528s,   37.87/s)  LR: 3.996e-02  Data: 0.010 (0.013)\n",
      "Train: 6 [ 400/3456 ( 12%)]  Loss:  0.548222 (0.6839)  Time: 0.527s,   37.98/s  (0.528s,   37.90/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 450/3456 ( 13%)]  Loss:  0.522603 (0.6705)  Time: 0.527s,   37.96/s  (0.528s,   37.91/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 500/3456 ( 14%)]  Loss:  0.547153 (0.6581)  Time: 0.529s,   37.79/s  (0.527s,   37.92/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 550/3456 ( 16%)]  Loss:  0.550186 (0.6463)  Time: 0.543s,   36.85/s  (0.528s,   37.87/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 600/3456 ( 17%)]  Loss:  0.622875 (0.6377)  Time: 0.524s,   38.17/s  (0.529s,   37.80/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 650/3456 ( 19%)]  Loss:  0.599318 (0.6308)  Time: 0.534s,   37.46/s  (0.529s,   37.80/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 700/3456 ( 20%)]  Loss:  0.449771 (0.6243)  Time: 0.546s,   36.60/s  (0.530s,   37.74/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 750/3456 ( 22%)]  Loss:  0.643439 (0.6200)  Time: 0.525s,   38.08/s  (0.531s,   37.69/s)  LR: 3.996e-02  Data: 0.010 (0.012)\n",
      "Train: 6 [ 800/3456 ( 23%)]  Loss:  0.630571 (0.6150)  Time: 0.543s,   36.85/s  (0.531s,   37.68/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 850/3456 ( 25%)]  Loss:  0.525056 (0.6107)  Time: 0.541s,   36.95/s  (0.532s,   37.63/s)  LR: 3.996e-02  Data: 0.011 (0.012)\n",
      "Train: 6 [ 900/3456 ( 26%)]  Loss:  0.555071 (0.6065)  Time: 0.539s,   37.09/s  (0.532s,   37.58/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [ 950/3456 ( 27%)]  Loss:  0.556687 (0.6018)  Time: 0.543s,   36.82/s  (0.533s,   37.54/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1000/3456 ( 29%)]  Loss:  0.475239 (0.5981)  Time: 0.547s,   36.59/s  (0.533s,   37.50/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1050/3456 ( 30%)]  Loss:  0.562912 (0.5954)  Time: 0.541s,   36.99/s  (0.534s,   37.45/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1100/3456 ( 32%)]  Loss:  0.608468 (0.5931)  Time: 0.541s,   36.96/s  (0.534s,   37.42/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1150/3456 ( 33%)]  Loss:  0.488535 (0.5905)  Time: 0.542s,   36.88/s  (0.535s,   37.39/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1200/3456 ( 35%)]  Loss:  0.592205 (0.5879)  Time: 0.543s,   36.86/s  (0.535s,   37.36/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1250/3456 ( 36%)]  Loss:  0.549121 (0.5852)  Time: 0.526s,   38.03/s  (0.535s,   37.35/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1300/3456 ( 38%)]  Loss:  0.525275 (0.5821)  Time: 0.526s,   38.04/s  (0.535s,   37.38/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1350/3456 ( 39%)]  Loss:  0.505994 (0.5801)  Time: 0.545s,   36.72/s  (0.535s,   37.35/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1400/3456 ( 41%)]  Loss:  0.477987 (0.5777)  Time: 0.546s,   36.61/s  (0.536s,   37.33/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1450/3456 ( 42%)]  Loss:  0.624371 (0.5763)  Time: 0.542s,   36.87/s  (0.536s,   37.32/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1500/3456 ( 43%)]  Loss:  0.537431 (0.5749)  Time: 0.541s,   36.96/s  (0.536s,   37.30/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1550/3456 ( 45%)]  Loss:  0.447128 (0.5737)  Time: 0.547s,   36.57/s  (0.536s,   37.28/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1600/3456 ( 46%)]  Loss:  0.489516 (0.5715)  Time: 0.543s,   36.84/s  (0.537s,   37.26/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1650/3456 ( 48%)]  Loss:  0.409569 (0.5701)  Time: 0.541s,   37.00/s  (0.537s,   37.24/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1700/3456 ( 49%)]  Loss:  0.417652 (0.5685)  Time: 0.545s,   36.66/s  (0.537s,   37.23/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1750/3456 ( 51%)]  Loss:  0.533949 (0.5675)  Time: 0.541s,   36.95/s  (0.537s,   37.21/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1800/3456 ( 52%)]  Loss:  0.520353 (0.5666)  Time: 0.540s,   37.01/s  (0.538s,   37.20/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1850/3456 ( 54%)]  Loss:  0.465706 (0.5649)  Time: 0.544s,   36.74/s  (0.538s,   37.18/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1900/3456 ( 55%)]  Loss:  0.553077 (0.5641)  Time: 0.540s,   37.06/s  (0.538s,   37.17/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [1950/3456 ( 56%)]  Loss:  0.494023 (0.5626)  Time: 0.547s,   36.56/s  (0.538s,   37.16/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2000/3456 ( 58%)]  Loss:  0.511560 (0.5615)  Time: 0.547s,   36.59/s  (0.538s,   37.15/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2050/3456 ( 59%)]  Loss:  0.508540 (0.5604)  Time: 0.547s,   36.56/s  (0.538s,   37.14/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2100/3456 ( 61%)]  Loss:  0.457941 (0.5598)  Time: 0.547s,   36.57/s  (0.539s,   37.13/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2150/3456 ( 62%)]  Loss:  0.531640 (0.5588)  Time: 0.546s,   36.65/s  (0.539s,   37.12/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2200/3456 ( 64%)]  Loss:  0.436284 (0.5575)  Time: 0.546s,   36.64/s  (0.539s,   37.11/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2250/3456 ( 65%)]  Loss:  0.568357 (0.5564)  Time: 0.548s,   36.49/s  (0.539s,   37.10/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2300/3456 ( 67%)]  Loss:  0.505248 (0.5551)  Time: 0.541s,   36.95/s  (0.539s,   37.10/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2350/3456 ( 68%)]  Loss:  0.507367 (0.5542)  Time: 0.546s,   36.62/s  (0.539s,   37.09/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2400/3456 ( 69%)]  Loss:  0.596501 (0.5542)  Time: 0.548s,   36.48/s  (0.539s,   37.08/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2450/3456 ( 71%)]  Loss:  0.464726 (0.5534)  Time: 0.541s,   36.95/s  (0.540s,   37.07/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2500/3456 ( 72%)]  Loss:  0.438746 (0.5528)  Time: 0.541s,   36.95/s  (0.540s,   37.06/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2550/3456 ( 74%)]  Loss:  0.523405 (0.5519)  Time: 0.542s,   36.92/s  (0.540s,   37.05/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2600/3456 ( 75%)]  Loss:  0.454266 (0.5513)  Time: 0.541s,   36.96/s  (0.540s,   37.04/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2650/3456 ( 77%)]  Loss:  0.651900 (0.5508)  Time: 0.542s,   36.91/s  (0.540s,   37.04/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2700/3456 ( 78%)]  Loss:  0.543222 (0.5500)  Time: 0.551s,   36.27/s  (0.540s,   37.03/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2750/3456 ( 80%)]  Loss:  0.557198 (0.5493)  Time: 0.544s,   36.80/s  (0.540s,   37.02/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2800/3456 ( 81%)]  Loss:  0.564978 (0.5486)  Time: 0.544s,   36.80/s  (0.540s,   37.02/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2850/3456 ( 82%)]  Loss:  0.570753 (0.5477)  Time: 0.547s,   36.55/s  (0.540s,   37.01/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2900/3456 ( 84%)]  Loss:  0.580409 (0.5472)  Time: 0.544s,   36.75/s  (0.540s,   37.01/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [2950/3456 ( 85%)]  Loss:  0.457852 (0.5467)  Time: 0.548s,   36.51/s  (0.541s,   37.00/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [3000/3456 ( 87%)]  Loss:  0.499219 (0.5462)  Time: 0.540s,   37.01/s  (0.541s,   37.00/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [3050/3456 ( 88%)]  Loss:  0.426251 (0.5455)  Time: 0.541s,   36.98/s  (0.541s,   36.99/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [3100/3456 ( 90%)]  Loss:  0.527989 (0.5450)  Time: 0.546s,   36.64/s  (0.541s,   36.99/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [3150/3456 ( 91%)]  Loss:  0.531107 (0.5445)  Time: 0.549s,   36.45/s  (0.541s,   36.98/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n",
      "Train: 6 [3200/3456 ( 93%)]  Loss:  0.557661 (0.5438)  Time: 0.548s,   36.52/s  (0.541s,   36.98/s)  LR: 3.996e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 6 [3250/3456 ( 94%)]  Loss:  2.024201 (0.5707)  Time: 0.984s,   20.31/s  (0.545s,   36.69/s)  LR: 3.996e-02  Data: 0.447 (0.015)\n",
      "Train: 6 [3300/3456 ( 96%)]  Loss:  1.588229 (0.5862)  Time: 0.900s,   22.23/s  (0.551s,   36.33/s)  LR: 3.996e-02  Data: 0.370 (0.021)\n",
      "Train: 6 [3350/3456 ( 97%)]  Loss:  0.733339 (0.5978)  Time: 0.904s,   22.12/s  (0.556s,   35.98/s)  LR: 3.996e-02  Data: 0.390 (0.026)\n",
      "Train: 6 [3400/3456 ( 98%)]  Loss:  2.399501 (0.6097)  Time: 0.878s,   22.79/s  (0.561s,   35.65/s)  LR: 3.996e-02  Data: 0.360 (0.031)\n",
      "Train: 6 [3450/3456 (100%)]  Loss:  0.746257 (0.6187)  Time: 0.878s,   22.79/s  (0.566s,   35.34/s)  LR: 3.996e-02  Data: 0.345 (0.036)\n",
      "Train: 6 [3455/3456 (100%)]  Loss:  1.508338 (0.6196)  Time: 0.470s,   23.43/s  (0.566s,   19.43/s)  LR: 3.996e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.888 (0.888)  Loss:  2.6813 (2.6813)  \n",
      "Test: [  50/147]  Time: 0.283 (0.297)  Loss:  3.1101 (2.9182)  \n",
      "Test: [ 100/147]  Time: 0.283 (0.290)  Loss:  2.9387 (2.9166)  \n",
      "Test: [ 147/147]  Time: 0.287 (0.335)  Loss:  0.7306 (2.7313)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 7 [   0/3456 (  0%)]  Loss:  3.020098 (3.0201)  Time: 1.436s,   13.93/s  (1.436s,   13.93/s)  LR: 3.995e-02  Data: 0.813 (0.813)\n",
      "Train: 7 [  50/3456 (  1%)]  Loss:  0.580729 (1.1094)  Time: 0.532s,   37.58/s  (0.545s,   36.70/s)  LR: 3.995e-02  Data: 0.011 (0.026)\n",
      "Train: 7 [ 100/3456 (  3%)]  Loss:  0.586893 (0.8751)  Time: 0.529s,   37.78/s  (0.536s,   37.29/s)  LR: 3.995e-02  Data: 0.010 (0.018)\n",
      "Train: 7 [ 150/3456 (  4%)]  Loss:  0.506765 (0.7707)  Time: 0.526s,   38.04/s  (0.534s,   37.44/s)  LR: 3.995e-02  Data: 0.010 (0.016)\n",
      "Train: 7 [ 200/3456 (  6%)]  Loss:  0.565932 (0.7159)  Time: 0.522s,   38.28/s  (0.532s,   37.59/s)  LR: 3.995e-02  Data: 0.010 (0.015)\n",
      "Train: 7 [ 250/3456 (  7%)]  Loss:  0.486110 (0.6861)  Time: 0.529s,   37.78/s  (0.531s,   37.69/s)  LR: 3.995e-02  Data: 0.010 (0.014)\n",
      "Train: 7 [ 300/3456 (  9%)]  Loss:  0.606868 (0.6637)  Time: 0.524s,   38.20/s  (0.530s,   37.75/s)  LR: 3.995e-02  Data: 0.010 (0.013)\n",
      "Train: 7 [ 350/3456 ( 10%)]  Loss:  0.507483 (0.6449)  Time: 0.546s,   36.60/s  (0.531s,   37.64/s)  LR: 3.995e-02  Data: 0.011 (0.013)\n",
      "Train: 7 [ 400/3456 ( 12%)]  Loss:  0.492315 (0.6297)  Time: 0.544s,   36.79/s  (0.533s,   37.54/s)  LR: 3.995e-02  Data: 0.011 (0.013)\n",
      "Train: 7 [ 450/3456 ( 13%)]  Loss:  0.464819 (0.6214)  Time: 0.541s,   36.95/s  (0.534s,   37.45/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 500/3456 ( 14%)]  Loss:  0.484102 (0.6120)  Time: 0.547s,   36.58/s  (0.535s,   37.38/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 550/3456 ( 16%)]  Loss:  0.469053 (0.6040)  Time: 0.544s,   36.73/s  (0.536s,   37.31/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 600/3456 ( 17%)]  Loss:  0.535565 (0.5977)  Time: 0.543s,   36.80/s  (0.537s,   37.26/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 650/3456 ( 19%)]  Loss:  0.632309 (0.5929)  Time: 0.545s,   36.70/s  (0.538s,   37.21/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 700/3456 ( 20%)]  Loss:  0.453050 (0.5879)  Time: 0.550s,   36.33/s  (0.538s,   37.17/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 750/3456 ( 22%)]  Loss:  0.617018 (0.5837)  Time: 0.542s,   36.89/s  (0.538s,   37.14/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 800/3456 ( 23%)]  Loss:  0.585993 (0.5798)  Time: 0.550s,   36.37/s  (0.539s,   37.10/s)  LR: 3.995e-02  Data: 0.012 (0.012)\n",
      "Train: 7 [ 850/3456 ( 25%)]  Loss:  0.538811 (0.5777)  Time: 0.531s,   37.66/s  (0.539s,   37.10/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 900/3456 ( 26%)]  Loss:  0.484509 (0.5748)  Time: 0.526s,   38.02/s  (0.538s,   37.14/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [ 950/3456 ( 27%)]  Loss:  0.571357 (0.5715)  Time: 0.525s,   38.11/s  (0.538s,   37.19/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [1000/3456 ( 29%)]  Loss:  0.493999 (0.5687)  Time: 0.527s,   37.95/s  (0.537s,   37.23/s)  LR: 3.995e-02  Data: 0.011 (0.012)\n",
      "Train: 7 [1050/3456 ( 30%)]  Loss:  0.481904 (0.5661)  Time: 0.525s,   38.12/s  (0.537s,   37.27/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1100/3456 ( 32%)]  Loss:  0.556617 (0.5644)  Time: 0.527s,   37.95/s  (0.536s,   37.30/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1150/3456 ( 33%)]  Loss:  0.490909 (0.5626)  Time: 0.526s,   37.99/s  (0.536s,   37.33/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1200/3456 ( 35%)]  Loss:  0.572677 (0.5613)  Time: 0.527s,   37.92/s  (0.535s,   37.35/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1250/3456 ( 36%)]  Loss:  0.581021 (0.5596)  Time: 0.527s,   37.94/s  (0.535s,   37.36/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1300/3456 ( 38%)]  Loss:  0.497709 (0.5572)  Time: 0.526s,   38.04/s  (0.535s,   37.39/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1350/3456 ( 39%)]  Loss:  0.598039 (0.5555)  Time: 0.524s,   38.16/s  (0.535s,   37.41/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1400/3456 ( 41%)]  Loss:  0.519176 (0.5543)  Time: 0.526s,   38.04/s  (0.534s,   37.43/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1450/3456 ( 42%)]  Loss:  0.639634 (0.5532)  Time: 0.525s,   38.07/s  (0.534s,   37.45/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1500/3456 ( 43%)]  Loss:  0.550291 (0.5522)  Time: 0.527s,   37.96/s  (0.534s,   37.48/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1550/3456 ( 45%)]  Loss:  0.459888 (0.5512)  Time: 0.525s,   38.09/s  (0.533s,   37.50/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1600/3456 ( 46%)]  Loss:  0.474281 (0.5498)  Time: 0.525s,   38.12/s  (0.533s,   37.51/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1650/3456 ( 48%)]  Loss:  0.373903 (0.5487)  Time: 0.526s,   38.00/s  (0.533s,   37.52/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1700/3456 ( 49%)]  Loss:  0.402415 (0.5479)  Time: 0.527s,   37.96/s  (0.533s,   37.54/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1750/3456 ( 51%)]  Loss:  0.433153 (0.5471)  Time: 0.526s,   38.02/s  (0.533s,   37.56/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1800/3456 ( 52%)]  Loss:  0.511349 (0.5464)  Time: 0.525s,   38.11/s  (0.532s,   37.57/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1850/3456 ( 54%)]  Loss:  0.550390 (0.5451)  Time: 0.527s,   37.98/s  (0.532s,   37.59/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [1900/3456 ( 55%)]  Loss:  0.551597 (0.5443)  Time: 0.523s,   38.26/s  (0.532s,   37.60/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [1950/3456 ( 56%)]  Loss:  0.425120 (0.5435)  Time: 0.523s,   38.23/s  (0.532s,   37.61/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [2000/3456 ( 58%)]  Loss:  0.538855 (0.5430)  Time: 0.523s,   38.27/s  (0.532s,   37.61/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [2050/3456 ( 59%)]  Loss:  0.548047 (0.5422)  Time: 0.530s,   37.73/s  (0.532s,   37.62/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2100/3456 ( 61%)]  Loss:  0.529305 (0.5416)  Time: 0.525s,   38.08/s  (0.531s,   37.63/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2150/3456 ( 62%)]  Loss:  0.536572 (0.5407)  Time: 0.532s,   37.61/s  (0.531s,   37.64/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2200/3456 ( 64%)]  Loss:  0.404488 (0.5396)  Time: 0.523s,   38.23/s  (0.531s,   37.64/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2250/3456 ( 65%)]  Loss:  0.510705 (0.5386)  Time: 0.528s,   37.91/s  (0.531s,   37.65/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2300/3456 ( 67%)]  Loss:  0.483160 (0.5379)  Time: 0.529s,   37.80/s  (0.531s,   37.65/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2350/3456 ( 68%)]  Loss:  0.581369 (0.5370)  Time: 0.530s,   37.72/s  (0.531s,   37.66/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2400/3456 ( 69%)]  Loss:  0.523503 (0.5366)  Time: 0.527s,   37.99/s  (0.531s,   37.67/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2450/3456 ( 71%)]  Loss:  0.512736 (0.5363)  Time: 0.525s,   38.12/s  (0.531s,   37.67/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2500/3456 ( 72%)]  Loss:  0.413375 (0.5357)  Time: 0.524s,   38.19/s  (0.531s,   37.67/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2550/3456 ( 74%)]  Loss:  0.500520 (0.5350)  Time: 0.524s,   38.19/s  (0.531s,   37.68/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2600/3456 ( 75%)]  Loss:  0.449596 (0.5345)  Time: 0.524s,   38.20/s  (0.531s,   37.69/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [2650/3456 ( 77%)]  Loss:  0.576372 (0.5345)  Time: 0.526s,   38.02/s  (0.531s,   37.69/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2700/3456 ( 78%)]  Loss:  0.551738 (0.5339)  Time: 0.524s,   38.19/s  (0.531s,   37.69/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2750/3456 ( 80%)]  Loss:  0.485874 (0.5334)  Time: 0.529s,   37.80/s  (0.531s,   37.70/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [2800/3456 ( 81%)]  Loss:  0.427717 (0.5329)  Time: 0.533s,   37.55/s  (0.530s,   37.70/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2850/3456 ( 82%)]  Loss:  0.589407 (0.5324)  Time: 0.524s,   38.20/s  (0.530s,   37.71/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [2900/3456 ( 84%)]  Loss:  0.526470 (0.5321)  Time: 0.526s,   38.03/s  (0.530s,   37.71/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [2950/3456 ( 85%)]  Loss:  0.619537 (0.5317)  Time: 0.523s,   38.21/s  (0.530s,   37.71/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [3000/3456 ( 87%)]  Loss:  0.458557 (0.5312)  Time: 0.524s,   38.18/s  (0.530s,   37.72/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [3050/3456 ( 88%)]  Loss:  0.389750 (0.5305)  Time: 0.534s,   37.45/s  (0.530s,   37.72/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [3100/3456 ( 90%)]  Loss:  0.515248 (0.5299)  Time: 0.525s,   38.06/s  (0.530s,   37.72/s)  LR: 3.995e-02  Data: 0.010 (0.011)\n",
      "Train: 7 [3150/3456 ( 91%)]  Loss:  0.465841 (0.5294)  Time: 0.529s,   37.78/s  (0.530s,   37.72/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n",
      "Train: 7 [3200/3456 ( 93%)]  Loss:  0.558820 (0.5289)  Time: 0.530s,   37.73/s  (0.530s,   37.72/s)  LR: 3.995e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 7 [3250/3456 ( 94%)]  Loss:  1.481962 (0.5533)  Time: 0.965s,   20.73/s  (0.535s,   37.41/s)  LR: 3.995e-02  Data: 0.446 (0.015)\n",
      "Train: 7 [3300/3456 ( 96%)]  Loss:  1.650387 (0.5667)  Time: 0.896s,   22.33/s  (0.540s,   37.01/s)  LR: 3.995e-02  Data: 0.377 (0.021)\n",
      "Train: 7 [3350/3456 ( 97%)]  Loss:  2.177628 (0.5782)  Time: 0.950s,   21.06/s  (0.546s,   36.63/s)  LR: 3.995e-02  Data: 0.417 (0.027)\n",
      "Train: 7 [3400/3456 ( 98%)]  Loss:  2.437256 (0.5899)  Time: 0.858s,   23.32/s  (0.551s,   36.28/s)  LR: 3.995e-02  Data: 0.326 (0.032)\n",
      "Train: 7 [3450/3456 (100%)]  Loss:  0.787583 (0.5973)  Time: 0.921s,   21.71/s  (0.556s,   35.96/s)  LR: 3.995e-02  Data: 0.405 (0.037)\n",
      "Train: 7 [3455/3456 (100%)]  Loss:  1.528546 (0.5982)  Time: 0.444s,   24.76/s  (0.556s,   19.77/s)  LR: 3.995e-02  Data: 0.037 (0.037)\n",
      "Test: [   0/147]  Time: 0.888 (0.888)  Loss:  2.5150 (2.5150)  \n",
      "Test: [  50/147]  Time: 0.282 (0.296)  Loss:  3.0708 (2.8456)  \n",
      "Test: [ 100/147]  Time: 0.280 (0.291)  Loss:  2.7236 (2.8416)  \n",
      "Test: [ 147/147]  Time: 0.307 (0.335)  Loss:  4.4912 (2.9734)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 8 [   0/3456 (  0%)]  Loss:  2.559509 (2.5595)  Time: 1.480s,   13.51/s  (1.480s,   13.51/s)  LR: 3.993e-02  Data: 0.867 (0.867)\n",
      "Train: 8 [  50/3456 (  1%)]  Loss:  0.536219 (0.9241)  Time: 0.548s,   36.47/s  (0.563s,   35.54/s)  LR: 3.993e-02  Data: 0.011 (0.028)\n",
      "Train: 8 [ 100/3456 (  3%)]  Loss:  0.640916 (0.7546)  Time: 0.547s,   36.57/s  (0.553s,   36.14/s)  LR: 3.993e-02  Data: 0.011 (0.019)\n",
      "Train: 8 [ 150/3456 (  4%)]  Loss:  0.566063 (0.6838)  Time: 0.545s,   36.68/s  (0.551s,   36.31/s)  LR: 3.993e-02  Data: 0.011 (0.016)\n",
      "Train: 8 [ 200/3456 (  6%)]  Loss:  0.542018 (0.6478)  Time: 0.547s,   36.58/s  (0.549s,   36.41/s)  LR: 3.993e-02  Data: 0.011 (0.015)\n",
      "Train: 8 [ 250/3456 (  7%)]  Loss:  0.509326 (0.6265)  Time: 0.527s,   37.98/s  (0.548s,   36.48/s)  LR: 3.993e-02  Data: 0.011 (0.014)\n",
      "Train: 8 [ 300/3456 (  9%)]  Loss:  0.587364 (0.6129)  Time: 0.548s,   36.51/s  (0.548s,   36.51/s)  LR: 3.993e-02  Data: 0.011 (0.014)\n",
      "Train: 8 [ 350/3456 ( 10%)]  Loss:  0.486073 (0.6010)  Time: 0.544s,   36.77/s  (0.548s,   36.53/s)  LR: 3.993e-02  Data: 0.011 (0.013)\n",
      "Train: 8 [ 400/3456 ( 12%)]  Loss:  0.508874 (0.5903)  Time: 0.543s,   36.84/s  (0.547s,   36.55/s)  LR: 3.993e-02  Data: 0.011 (0.013)\n",
      "Train: 8 [ 450/3456 ( 13%)]  Loss:  0.448861 (0.5836)  Time: 0.526s,   38.01/s  (0.545s,   36.68/s)  LR: 3.993e-02  Data: 0.011 (0.013)\n",
      "Train: 8 [ 500/3456 ( 14%)]  Loss:  0.511579 (0.5781)  Time: 0.525s,   38.06/s  (0.543s,   36.81/s)  LR: 3.993e-02  Data: 0.010 (0.012)\n",
      "Train: 8 [ 550/3456 ( 16%)]  Loss:  0.502417 (0.5714)  Time: 0.526s,   38.04/s  (0.542s,   36.92/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [ 600/3456 ( 17%)]  Loss:  0.543812 (0.5678)  Time: 0.525s,   38.11/s  (0.540s,   37.01/s)  LR: 3.993e-02  Data: 0.010 (0.012)\n",
      "Train: 8 [ 650/3456 ( 19%)]  Loss:  0.568586 (0.5637)  Time: 0.526s,   38.04/s  (0.539s,   37.08/s)  LR: 3.993e-02  Data: 0.010 (0.012)\n",
      "Train: 8 [ 700/3456 ( 20%)]  Loss:  0.464081 (0.5594)  Time: 0.528s,   37.90/s  (0.538s,   37.15/s)  LR: 3.993e-02  Data: 0.010 (0.012)\n",
      "Train: 8 [ 750/3456 ( 22%)]  Loss:  0.581766 (0.5566)  Time: 0.522s,   38.31/s  (0.538s,   37.15/s)  LR: 3.993e-02  Data: 0.010 (0.012)\n",
      "Train: 8 [ 800/3456 ( 23%)]  Loss:  0.595188 (0.5545)  Time: 0.546s,   36.63/s  (0.539s,   37.13/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [ 850/3456 ( 25%)]  Loss:  0.498612 (0.5522)  Time: 0.543s,   36.84/s  (0.539s,   37.10/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [ 900/3456 ( 26%)]  Loss:  0.500658 (0.5506)  Time: 0.544s,   36.79/s  (0.539s,   37.09/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [ 950/3456 ( 27%)]  Loss:  0.557819 (0.5482)  Time: 0.546s,   36.66/s  (0.539s,   37.08/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [1000/3456 ( 29%)]  Loss:  0.545463 (0.5461)  Time: 0.542s,   36.90/s  (0.540s,   37.06/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [1050/3456 ( 30%)]  Loss:  0.513608 (0.5440)  Time: 0.548s,   36.47/s  (0.540s,   37.05/s)  LR: 3.993e-02  Data: 0.011 (0.012)\n",
      "Train: 8 [1100/3456 ( 32%)]  Loss:  0.454443 (0.5428)  Time: 0.522s,   38.30/s  (0.539s,   37.09/s)  LR: 3.993e-02  Data: 0.010 (0.011)\n",
      "Train: 8 [1150/3456 ( 33%)]  Loss:  0.418479 (0.5409)  Time: 0.524s,   38.14/s  (0.539s,   37.13/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1200/3456 ( 35%)]  Loss:  0.555788 (0.5397)  Time: 0.525s,   38.06/s  (0.538s,   37.16/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1250/3456 ( 36%)]  Loss:  0.535425 (0.5388)  Time: 0.526s,   38.03/s  (0.538s,   37.19/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1300/3456 ( 38%)]  Loss:  0.525553 (0.5374)  Time: 0.527s,   37.92/s  (0.537s,   37.22/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1350/3456 ( 39%)]  Loss:  0.471919 (0.5365)  Time: 0.525s,   38.12/s  (0.537s,   37.24/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1400/3456 ( 41%)]  Loss:  0.473998 (0.5350)  Time: 0.525s,   38.12/s  (0.537s,   37.27/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1450/3456 ( 42%)]  Loss:  0.600454 (0.5341)  Time: 0.525s,   38.08/s  (0.536s,   37.30/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1500/3456 ( 43%)]  Loss:  0.499399 (0.5334)  Time: 0.525s,   38.10/s  (0.536s,   37.32/s)  LR: 3.993e-02  Data: 0.010 (0.011)\n",
      "Train: 8 [1550/3456 ( 45%)]  Loss:  0.433422 (0.5324)  Time: 0.527s,   37.96/s  (0.536s,   37.35/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1600/3456 ( 46%)]  Loss:  0.544995 (0.5312)  Time: 0.524s,   38.14/s  (0.535s,   37.36/s)  LR: 3.993e-02  Data: 0.010 (0.011)\n",
      "Train: 8 [1650/3456 ( 48%)]  Loss:  0.473439 (0.5305)  Time: 0.526s,   38.04/s  (0.535s,   37.38/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1700/3456 ( 49%)]  Loss:  0.362878 (0.5298)  Time: 0.533s,   37.53/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1750/3456 ( 51%)]  Loss:  0.456573 (0.5293)  Time: 0.543s,   36.85/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1800/3456 ( 52%)]  Loss:  0.605942 (0.5287)  Time: 0.526s,   38.04/s  (0.535s,   37.40/s)  LR: 3.993e-02  Data: 0.010 (0.011)\n",
      "Train: 8 [1850/3456 ( 54%)]  Loss:  0.494802 (0.5276)  Time: 0.525s,   38.09/s  (0.535s,   37.41/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1900/3456 ( 55%)]  Loss:  0.565906 (0.5270)  Time: 0.523s,   38.25/s  (0.534s,   37.43/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [1950/3456 ( 56%)]  Loss:  0.587893 (0.5267)  Time: 0.549s,   36.42/s  (0.534s,   37.44/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2000/3456 ( 58%)]  Loss:  0.467278 (0.5264)  Time: 0.551s,   36.29/s  (0.534s,   37.43/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2050/3456 ( 59%)]  Loss:  0.515366 (0.5262)  Time: 0.544s,   36.75/s  (0.534s,   37.42/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2100/3456 ( 61%)]  Loss:  0.468536 (0.5261)  Time: 0.540s,   37.04/s  (0.535s,   37.40/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2150/3456 ( 62%)]  Loss:  0.527033 (0.5254)  Time: 0.547s,   36.57/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2200/3456 ( 64%)]  Loss:  0.386706 (0.5243)  Time: 0.542s,   36.87/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2250/3456 ( 65%)]  Loss:  0.541653 (0.5239)  Time: 0.531s,   37.67/s  (0.535s,   37.38/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2300/3456 ( 67%)]  Loss:  0.451189 (0.5231)  Time: 0.526s,   38.03/s  (0.535s,   37.38/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2350/3456 ( 68%)]  Loss:  0.529469 (0.5224)  Time: 0.525s,   38.06/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2400/3456 ( 69%)]  Loss:  0.470234 (0.5222)  Time: 0.527s,   37.93/s  (0.535s,   37.41/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2450/3456 ( 71%)]  Loss:  0.508230 (0.5220)  Time: 0.527s,   37.97/s  (0.535s,   37.42/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2500/3456 ( 72%)]  Loss:  0.503063 (0.5217)  Time: 0.549s,   36.42/s  (0.534s,   37.42/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2550/3456 ( 74%)]  Loss:  0.507787 (0.5213)  Time: 0.544s,   36.74/s  (0.535s,   37.41/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2600/3456 ( 75%)]  Loss:  0.470347 (0.5209)  Time: 0.545s,   36.68/s  (0.535s,   37.39/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2650/3456 ( 77%)]  Loss:  0.628056 (0.5208)  Time: 0.550s,   36.34/s  (0.535s,   37.37/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2700/3456 ( 78%)]  Loss:  0.461772 (0.5205)  Time: 0.548s,   36.52/s  (0.535s,   37.36/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2750/3456 ( 80%)]  Loss:  0.533873 (0.5202)  Time: 0.541s,   36.94/s  (0.536s,   37.34/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2800/3456 ( 81%)]  Loss:  0.435054 (0.5197)  Time: 0.541s,   36.95/s  (0.536s,   37.33/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2850/3456 ( 82%)]  Loss:  0.537610 (0.5195)  Time: 0.545s,   36.68/s  (0.536s,   37.32/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2900/3456 ( 84%)]  Loss:  0.560880 (0.5193)  Time: 0.544s,   36.76/s  (0.536s,   37.30/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [2950/3456 ( 85%)]  Loss:  0.606610 (0.5191)  Time: 0.544s,   36.73/s  (0.536s,   37.29/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [3000/3456 ( 87%)]  Loss:  0.458310 (0.5189)  Time: 0.545s,   36.70/s  (0.536s,   37.28/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [3050/3456 ( 88%)]  Loss:  0.506848 (0.5184)  Time: 0.546s,   36.62/s  (0.537s,   37.27/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [3100/3456 ( 90%)]  Loss:  0.500694 (0.5180)  Time: 0.547s,   36.53/s  (0.537s,   37.26/s)  LR: 3.993e-02  Data: 0.011 (0.011)\n",
      "Train: 8 [3150/3456 ( 91%)]  Loss:  0.495255 (0.5175)  Time: 0.547s,   36.55/s  (0.537s,   37.25/s)  LR: 3.993e-02  Data: 0.012 (0.011)\n",
      "Train: 8 [3200/3456 ( 93%)]  Loss:  0.499963 (0.5173)  Time: 0.542s,   36.88/s  (0.537s,   37.24/s)  LR: 3.993e-02  Data: 0.012 (0.011)\n",
      "Train: 8 [3250/3456 ( 94%)]  Loss:  1.520659 (0.5288)  Time: 0.918s,   21.78/s  (0.541s,   36.95/s)  LR: 3.993e-02  Data: 0.383 (0.015)\n",
      "Train: 8 [3300/3456 ( 96%)]  Loss:  1.817593 (0.5410)  Time: 0.891s,   22.44/s  (0.547s,   36.57/s)  LR: 3.993e-02  Data: 0.361 (0.021)\n",
      "Train: 8 [3350/3456 ( 97%)]  Loss:  0.801495 (0.5479)  Time: 0.912s,   21.93/s  (0.552s,   36.23/s)  LR: 3.993e-02  Data: 0.382 (0.026)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 8 [3400/3456 ( 98%)]  Loss:  1.296025 (0.5586)  Time: 0.891s,   22.44/s  (0.557s,   35.91/s)  LR: 3.993e-02  Data: 0.361 (0.031)\n",
      "Train: 8 [3450/3456 (100%)]  Loss:  1.207570 (0.5660)  Time: 0.873s,   22.92/s  (0.562s,   35.61/s)  LR: 3.993e-02  Data: 0.338 (0.035)\n",
      "Train: 8 [3455/3456 (100%)]  Loss:  0.824537 (0.5667)  Time: 0.462s,   23.80/s  (0.562s,   19.57/s)  LR: 3.993e-02  Data: 0.039 (0.035)\n",
      "Test: [   0/147]  Time: 0.872 (0.872)  Loss:  2.3136 (2.3136)  \n",
      "Test: [  50/147]  Time: 0.278 (0.292)  Loss:  2.9714 (2.6742)  \n",
      "Test: [ 100/147]  Time: 0.281 (0.288)  Loss:  2.7717 (2.6634)  \n",
      "Test: [ 147/147]  Time: 0.300 (0.332)  Loss:  1.5685 (2.5601)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 9 [   0/3456 (  0%)]  Loss:  2.262227 (2.2622)  Time: 1.428s,   14.01/s  (1.428s,   14.01/s)  LR: 3.991e-02  Data: 0.811 (0.811)\n",
      "Train: 9 [  50/3456 (  1%)]  Loss:  0.561489 (0.8526)  Time: 0.526s,   38.05/s  (0.544s,   36.74/s)  LR: 3.991e-02  Data: 0.011 (0.026)\n",
      "Train: 9 [ 100/3456 (  3%)]  Loss:  0.671505 (0.7129)  Time: 0.528s,   37.89/s  (0.535s,   37.35/s)  LR: 3.991e-02  Data: 0.010 (0.018)\n",
      "Train: 9 [ 150/3456 (  4%)]  Loss:  0.548197 (0.6476)  Time: 0.528s,   37.87/s  (0.532s,   37.58/s)  LR: 3.991e-02  Data: 0.011 (0.016)\n",
      "Train: 9 [ 200/3456 (  6%)]  Loss:  0.506930 (0.6142)  Time: 0.530s,   37.77/s  (0.531s,   37.68/s)  LR: 3.991e-02  Data: 0.011 (0.014)\n",
      "Train: 9 [ 250/3456 (  7%)]  Loss:  0.548292 (0.5988)  Time: 0.524s,   38.14/s  (0.531s,   37.68/s)  LR: 3.991e-02  Data: 0.010 (0.014)\n",
      "Train: 9 [ 300/3456 (  9%)]  Loss:  0.544946 (0.5870)  Time: 0.525s,   38.10/s  (0.530s,   37.74/s)  LR: 3.991e-02  Data: 0.010 (0.013)\n",
      "Train: 9 [ 350/3456 ( 10%)]  Loss:  0.580581 (0.5783)  Time: 0.529s,   37.81/s  (0.530s,   37.77/s)  LR: 3.991e-02  Data: 0.011 (0.013)\n",
      "Train: 9 [ 400/3456 ( 12%)]  Loss:  0.464947 (0.5706)  Time: 0.542s,   36.88/s  (0.530s,   37.76/s)  LR: 3.991e-02  Data: 0.011 (0.013)\n",
      "Train: 9 [ 450/3456 ( 13%)]  Loss:  0.501737 (0.5657)  Time: 0.543s,   36.86/s  (0.531s,   37.63/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 500/3456 ( 14%)]  Loss:  0.507417 (0.5607)  Time: 0.542s,   36.87/s  (0.533s,   37.54/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 550/3456 ( 16%)]  Loss:  0.458900 (0.5551)  Time: 0.543s,   36.86/s  (0.534s,   37.46/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 600/3456 ( 17%)]  Loss:  0.581803 (0.5514)  Time: 0.528s,   37.88/s  (0.533s,   37.50/s)  LR: 3.991e-02  Data: 0.010 (0.012)\n",
      "Train: 9 [ 650/3456 ( 19%)]  Loss:  0.611069 (0.5480)  Time: 0.550s,   36.39/s  (0.534s,   37.46/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 700/3456 ( 20%)]  Loss:  0.490586 (0.5458)  Time: 0.543s,   36.84/s  (0.535s,   37.40/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 750/3456 ( 22%)]  Loss:  0.626141 (0.5444)  Time: 0.541s,   36.95/s  (0.535s,   37.35/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 800/3456 ( 23%)]  Loss:  0.522738 (0.5420)  Time: 0.540s,   37.02/s  (0.536s,   37.32/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 850/3456 ( 25%)]  Loss:  0.514537 (0.5396)  Time: 0.544s,   36.79/s  (0.536s,   37.29/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 900/3456 ( 26%)]  Loss:  0.496152 (0.5381)  Time: 0.525s,   38.08/s  (0.536s,   37.30/s)  LR: 3.991e-02  Data: 0.011 (0.012)\n",
      "Train: 9 [ 950/3456 ( 27%)]  Loss:  0.565909 (0.5361)  Time: 0.525s,   38.13/s  (0.536s,   37.33/s)  LR: 3.991e-02  Data: 0.010 (0.011)\n",
      "Train: 9 [1000/3456 ( 29%)]  Loss:  0.554948 (0.5343)  Time: 0.523s,   38.22/s  (0.535s,   37.37/s)  LR: 3.991e-02  Data: 0.010 (0.011)\n",
      "Train: 9 [1050/3456 ( 30%)]  Loss:  0.509256 (0.5328)  Time: 0.530s,   37.72/s  (0.535s,   37.40/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1100/3456 ( 32%)]  Loss:  0.560546 (0.5316)  Time: 0.529s,   37.79/s  (0.534s,   37.42/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1150/3456 ( 33%)]  Loss:  0.460646 (0.5299)  Time: 0.527s,   37.98/s  (0.534s,   37.44/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1200/3456 ( 35%)]  Loss:  0.512015 (0.5290)  Time: 0.549s,   36.41/s  (0.535s,   37.41/s)  LR: 3.991e-02  Data: 0.012 (0.011)\n",
      "Train: 9 [1250/3456 ( 36%)]  Loss:  0.648671 (0.5282)  Time: 0.546s,   36.65/s  (0.535s,   37.38/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1300/3456 ( 38%)]  Loss:  0.449662 (0.5265)  Time: 0.546s,   36.66/s  (0.535s,   37.36/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1350/3456 ( 39%)]  Loss:  0.530906 (0.5253)  Time: 0.549s,   36.42/s  (0.536s,   37.32/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1400/3456 ( 41%)]  Loss:  0.533669 (0.5245)  Time: 0.543s,   36.86/s  (0.536s,   37.30/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1450/3456 ( 42%)]  Loss:  0.580222 (0.5240)  Time: 0.546s,   36.65/s  (0.536s,   37.28/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1500/3456 ( 43%)]  Loss:  0.545971 (0.5235)  Time: 0.546s,   36.63/s  (0.537s,   37.26/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1550/3456 ( 45%)]  Loss:  0.442966 (0.5231)  Time: 0.527s,   37.94/s  (0.536s,   37.28/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1600/3456 ( 46%)]  Loss:  0.497700 (0.5222)  Time: 0.546s,   36.65/s  (0.536s,   37.30/s)  LR: 3.991e-02  Data: 0.012 (0.011)\n",
      "Train: 9 [1650/3456 ( 48%)]  Loss:  0.403756 (0.5216)  Time: 0.525s,   38.12/s  (0.536s,   37.32/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1700/3456 ( 49%)]  Loss:  0.448653 (0.5212)  Time: 0.548s,   36.52/s  (0.536s,   37.31/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1750/3456 ( 51%)]  Loss:  0.501469 (0.5209)  Time: 0.549s,   36.45/s  (0.536s,   37.29/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1800/3456 ( 52%)]  Loss:  0.507832 (0.5207)  Time: 0.549s,   36.43/s  (0.537s,   37.27/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1850/3456 ( 54%)]  Loss:  0.506808 (0.5197)  Time: 0.542s,   36.88/s  (0.537s,   37.25/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1900/3456 ( 55%)]  Loss:  0.496895 (0.5191)  Time: 0.544s,   36.78/s  (0.537s,   37.24/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [1950/3456 ( 56%)]  Loss:  0.544860 (0.5188)  Time: 0.546s,   36.65/s  (0.537s,   37.23/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2000/3456 ( 58%)]  Loss:  0.472993 (0.5184)  Time: 0.542s,   36.88/s  (0.537s,   37.21/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2050/3456 ( 59%)]  Loss:  0.466522 (0.5180)  Time: 0.527s,   37.94/s  (0.537s,   37.21/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2100/3456 ( 61%)]  Loss:  0.552869 (0.5176)  Time: 0.527s,   37.95/s  (0.537s,   37.23/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2150/3456 ( 62%)]  Loss:  0.509493 (0.5171)  Time: 0.541s,   36.98/s  (0.537s,   37.24/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2200/3456 ( 64%)]  Loss:  0.453000 (0.5161)  Time: 0.526s,   38.02/s  (0.537s,   37.23/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2250/3456 ( 65%)]  Loss:  0.536105 (0.5157)  Time: 0.525s,   38.10/s  (0.537s,   37.25/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2300/3456 ( 67%)]  Loss:  0.490155 (0.5152)  Time: 0.545s,   36.73/s  (0.537s,   37.24/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2350/3456 ( 68%)]  Loss:  0.512739 (0.5146)  Time: 0.541s,   36.95/s  (0.537s,   37.23/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2400/3456 ( 69%)]  Loss:  0.556275 (0.5144)  Time: 0.544s,   36.80/s  (0.537s,   37.22/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2450/3456 ( 71%)]  Loss:  0.389127 (0.5142)  Time: 0.543s,   36.80/s  (0.538s,   37.20/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2500/3456 ( 72%)]  Loss:  0.459142 (0.5140)  Time: 0.542s,   36.93/s  (0.538s,   37.19/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2550/3456 ( 74%)]  Loss:  0.524765 (0.5134)  Time: 0.542s,   36.92/s  (0.538s,   37.19/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2600/3456 ( 75%)]  Loss:  0.448640 (0.5131)  Time: 0.542s,   36.88/s  (0.538s,   37.18/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2650/3456 ( 77%)]  Loss:  0.617613 (0.5127)  Time: 0.542s,   36.92/s  (0.538s,   37.17/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2700/3456 ( 78%)]  Loss:  0.542489 (0.5123)  Time: 0.545s,   36.73/s  (0.538s,   37.16/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2750/3456 ( 80%)]  Loss:  0.532735 (0.5121)  Time: 0.526s,   37.99/s  (0.538s,   37.15/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2800/3456 ( 81%)]  Loss:  0.462159 (0.5119)  Time: 0.527s,   37.97/s  (0.538s,   37.17/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2850/3456 ( 82%)]  Loss:  0.536617 (0.5116)  Time: 0.528s,   37.85/s  (0.538s,   37.18/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [2900/3456 ( 84%)]  Loss:  0.531152 (0.5115)  Time: 0.526s,   38.06/s  (0.538s,   37.19/s)  LR: 3.991e-02  Data: 0.010 (0.011)\n",
      "Train: 9 [2950/3456 ( 85%)]  Loss:  0.526387 (0.5113)  Time: 0.527s,   37.97/s  (0.538s,   37.20/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [3000/3456 ( 87%)]  Loss:  0.490719 (0.5110)  Time: 0.529s,   37.83/s  (0.537s,   37.21/s)  LR: 3.991e-02  Data: 0.010 (0.011)\n",
      "Train: 9 [3050/3456 ( 88%)]  Loss:  0.421954 (0.5108)  Time: 0.532s,   37.61/s  (0.537s,   37.22/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [3100/3456 ( 90%)]  Loss:  0.455203 (0.5105)  Time: 0.531s,   37.70/s  (0.537s,   37.24/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n",
      "Train: 9 [3150/3456 ( 91%)]  Loss:  0.526584 (0.5101)  Time: 0.524s,   38.18/s  (0.537s,   37.24/s)  LR: 3.991e-02  Data: 0.010 (0.011)\n",
      "Train: 9 [3200/3456 ( 93%)]  Loss:  0.445087 (0.5097)  Time: 0.527s,   37.99/s  (0.537s,   37.25/s)  LR: 3.991e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 9 [3250/3456 ( 94%)]  Loss:  1.695723 (0.5246)  Time: 0.919s,   21.76/s  (0.541s,   36.97/s)  LR: 3.991e-02  Data: 0.407 (0.015)\n",
      "Train: 9 [3300/3456 ( 96%)]  Loss:  1.300756 (0.5349)  Time: 0.897s,   22.29/s  (0.547s,   36.59/s)  LR: 3.991e-02  Data: 0.380 (0.021)\n",
      "Train: 9 [3350/3456 ( 97%)]  Loss:  0.855039 (0.5424)  Time: 0.869s,   23.03/s  (0.552s,   36.24/s)  LR: 3.991e-02  Data: 0.337 (0.026)\n",
      "Train: 9 [3400/3456 ( 98%)]  Loss:  1.571495 (0.5502)  Time: 0.890s,   22.48/s  (0.557s,   35.91/s)  LR: 3.991e-02  Data: 0.358 (0.031)\n",
      "Train: 9 [3450/3456 (100%)]  Loss:  0.849137 (0.5558)  Time: 0.869s,   23.03/s  (0.562s,   35.60/s)  LR: 3.991e-02  Data: 0.338 (0.036)\n",
      "Train: 9 [3455/3456 (100%)]  Loss:  1.151210 (0.5564)  Time: 0.461s,   23.85/s  (0.562s,   19.57/s)  LR: 3.991e-02  Data: 0.037 (0.036)\n",
      "Test: [   0/147]  Time: 0.882 (0.882)  Loss:  2.3277 (2.3277)  \n",
      "Test: [  50/147]  Time: 0.281 (0.294)  Loss:  3.0101 (2.6995)  \n",
      "Test: [ 100/147]  Time: 0.284 (0.290)  Loss:  2.8549 (2.6871)  \n",
      "Test: [ 147/147]  Time: 0.304 (0.335)  Loss:  2.2168 (2.6275)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-3.pth.tar', 3.9160738780691817)\n",
      "\n",
      "Train: 10 [   0/3456 (  0%)]  Loss:  2.265889 (2.2659)  Time: 1.487s,   13.45/s  (1.487s,   13.45/s)  LR: 3.989e-02  Data: 0.868 (0.868)\n",
      "Train: 10 [  50/3456 (  1%)]  Loss:  0.485917 (0.8567)  Time: 0.543s,   36.84/s  (0.562s,   35.58/s)  LR: 3.989e-02  Data: 0.011 (0.028)\n",
      "Train: 10 [ 100/3456 (  3%)]  Loss:  0.595240 (0.7068)  Time: 0.549s,   36.42/s  (0.554s,   36.12/s)  LR: 3.989e-02  Data: 0.011 (0.019)\n",
      "Train: 10 [ 150/3456 (  4%)]  Loss:  0.605190 (0.6447)  Time: 0.525s,   38.11/s  (0.546s,   36.61/s)  LR: 3.989e-02  Data: 0.011 (0.016)\n",
      "Train: 10 [ 200/3456 (  6%)]  Loss:  0.526772 (0.6134)  Time: 0.526s,   38.03/s  (0.541s,   36.96/s)  LR: 3.989e-02  Data: 0.011 (0.015)\n",
      "Train: 10 [ 250/3456 (  7%)]  Loss:  0.538322 (0.5967)  Time: 0.525s,   38.10/s  (0.538s,   37.16/s)  LR: 3.989e-02  Data: 0.011 (0.014)\n",
      "Train: 10 [ 300/3456 (  9%)]  Loss:  0.493177 (0.5824)  Time: 0.527s,   37.97/s  (0.536s,   37.29/s)  LR: 3.989e-02  Data: 0.011 (0.013)\n",
      "Train: 10 [ 350/3456 ( 10%)]  Loss:  0.634850 (0.5713)  Time: 0.530s,   37.77/s  (0.535s,   37.39/s)  LR: 3.989e-02  Data: 0.011 (0.013)\n",
      "Train: 10 [ 400/3456 ( 12%)]  Loss:  0.527381 (0.5632)  Time: 0.528s,   37.86/s  (0.534s,   37.42/s)  LR: 3.989e-02  Data: 0.010 (0.013)\n",
      "Train: 10 [ 450/3456 ( 13%)]  Loss:  0.563632 (0.5578)  Time: 0.526s,   38.04/s  (0.533s,   37.49/s)  LR: 3.989e-02  Data: 0.010 (0.013)\n",
      "Train: 10 [ 500/3456 ( 14%)]  Loss:  0.504880 (0.5535)  Time: 0.529s,   37.81/s  (0.533s,   37.54/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 550/3456 ( 16%)]  Loss:  0.469565 (0.5487)  Time: 0.548s,   36.52/s  (0.533s,   37.52/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 600/3456 ( 17%)]  Loss:  0.523934 (0.5453)  Time: 0.540s,   37.04/s  (0.534s,   37.46/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 650/3456 ( 19%)]  Loss:  0.516915 (0.5435)  Time: 0.546s,   36.64/s  (0.535s,   37.41/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 700/3456 ( 20%)]  Loss:  0.420048 (0.5411)  Time: 0.542s,   36.87/s  (0.535s,   37.36/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 750/3456 ( 22%)]  Loss:  0.569008 (0.5392)  Time: 0.528s,   37.88/s  (0.536s,   37.33/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 800/3456 ( 23%)]  Loss:  0.621302 (0.5371)  Time: 0.526s,   38.01/s  (0.535s,   37.36/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 850/3456 ( 25%)]  Loss:  0.417270 (0.5350)  Time: 0.526s,   38.05/s  (0.535s,   37.40/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 900/3456 ( 26%)]  Loss:  0.516692 (0.5338)  Time: 0.527s,   37.95/s  (0.534s,   37.43/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [ 950/3456 ( 27%)]  Loss:  0.538423 (0.5316)  Time: 0.525s,   38.07/s  (0.534s,   37.47/s)  LR: 3.989e-02  Data: 0.011 (0.012)\n",
      "Train: 10 [1000/3456 ( 29%)]  Loss:  0.557361 (0.5303)  Time: 0.526s,   38.01/s  (0.533s,   37.50/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1050/3456 ( 30%)]  Loss:  0.499887 (0.5285)  Time: 0.525s,   38.09/s  (0.533s,   37.51/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1100/3456 ( 32%)]  Loss:  0.529194 (0.5275)  Time: 0.524s,   38.15/s  (0.533s,   37.53/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1150/3456 ( 33%)]  Loss:  0.497551 (0.5260)  Time: 0.527s,   37.98/s  (0.533s,   37.56/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1200/3456 ( 35%)]  Loss:  0.565185 (0.5250)  Time: 0.531s,   37.63/s  (0.532s,   37.57/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1250/3456 ( 36%)]  Loss:  0.572674 (0.5242)  Time: 0.526s,   38.03/s  (0.532s,   37.59/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1300/3456 ( 38%)]  Loss:  0.487006 (0.5230)  Time: 0.543s,   36.82/s  (0.532s,   37.56/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1350/3456 ( 39%)]  Loss:  0.522518 (0.5222)  Time: 0.543s,   36.81/s  (0.533s,   37.53/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1400/3456 ( 41%)]  Loss:  0.439675 (0.5214)  Time: 0.524s,   38.13/s  (0.533s,   37.53/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1450/3456 ( 42%)]  Loss:  0.584048 (0.5204)  Time: 0.528s,   37.87/s  (0.533s,   37.54/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1500/3456 ( 43%)]  Loss:  0.523250 (0.5201)  Time: 0.532s,   37.57/s  (0.533s,   37.54/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1550/3456 ( 45%)]  Loss:  0.386634 (0.5195)  Time: 0.527s,   37.93/s  (0.533s,   37.55/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1600/3456 ( 46%)]  Loss:  0.594504 (0.5185)  Time: 0.530s,   37.74/s  (0.532s,   37.56/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1650/3456 ( 48%)]  Loss:  0.370384 (0.5177)  Time: 0.526s,   38.01/s  (0.532s,   37.57/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1700/3456 ( 49%)]  Loss:  0.422448 (0.5172)  Time: 0.527s,   37.98/s  (0.532s,   37.58/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1750/3456 ( 51%)]  Loss:  0.455601 (0.5166)  Time: 0.525s,   38.12/s  (0.532s,   37.60/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1800/3456 ( 52%)]  Loss:  0.537905 (0.5164)  Time: 0.542s,   36.93/s  (0.532s,   37.57/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1850/3456 ( 54%)]  Loss:  0.510312 (0.5152)  Time: 0.523s,   38.24/s  (0.532s,   37.58/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1900/3456 ( 55%)]  Loss:  0.606585 (0.5149)  Time: 0.525s,   38.06/s  (0.532s,   37.59/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [1950/3456 ( 56%)]  Loss:  0.507333 (0.5146)  Time: 0.525s,   38.13/s  (0.532s,   37.59/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2000/3456 ( 58%)]  Loss:  0.476951 (0.5141)  Time: 0.526s,   38.01/s  (0.532s,   37.61/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2050/3456 ( 59%)]  Loss:  0.541829 (0.5138)  Time: 0.527s,   37.99/s  (0.532s,   37.62/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2100/3456 ( 61%)]  Loss:  0.492290 (0.5137)  Time: 0.525s,   38.07/s  (0.532s,   37.63/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2150/3456 ( 62%)]  Loss:  0.423860 (0.5133)  Time: 0.525s,   38.06/s  (0.531s,   37.64/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2200/3456 ( 64%)]  Loss:  0.404708 (0.5126)  Time: 0.524s,   38.14/s  (0.531s,   37.64/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2250/3456 ( 65%)]  Loss:  0.476243 (0.5120)  Time: 0.527s,   37.98/s  (0.531s,   37.65/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2300/3456 ( 67%)]  Loss:  0.487493 (0.5115)  Time: 0.526s,   38.01/s  (0.531s,   37.65/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2350/3456 ( 68%)]  Loss:  0.458628 (0.5109)  Time: 0.524s,   38.14/s  (0.531s,   37.66/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2400/3456 ( 69%)]  Loss:  0.482110 (0.5108)  Time: 0.526s,   38.03/s  (0.531s,   37.67/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2450/3456 ( 71%)]  Loss:  0.511706 (0.5107)  Time: 0.525s,   38.11/s  (0.531s,   37.68/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2500/3456 ( 72%)]  Loss:  0.428194 (0.5106)  Time: 0.526s,   38.06/s  (0.531s,   37.69/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2550/3456 ( 74%)]  Loss:  0.605360 (0.5100)  Time: 0.544s,   36.76/s  (0.531s,   37.67/s)  LR: 3.989e-02  Data: 0.012 (0.011)\n",
      "Train: 10 [2600/3456 ( 75%)]  Loss:  0.421010 (0.5098)  Time: 0.545s,   36.73/s  (0.531s,   37.65/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2650/3456 ( 77%)]  Loss:  0.563034 (0.5096)  Time: 0.541s,   36.94/s  (0.531s,   37.63/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2700/3456 ( 78%)]  Loss:  0.522490 (0.5092)  Time: 0.549s,   36.46/s  (0.532s,   37.62/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2750/3456 ( 80%)]  Loss:  0.502579 (0.5089)  Time: 0.548s,   36.49/s  (0.532s,   37.60/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2800/3456 ( 81%)]  Loss:  0.503880 (0.5086)  Time: 0.547s,   36.55/s  (0.532s,   37.58/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2850/3456 ( 82%)]  Loss:  0.562803 (0.5083)  Time: 0.544s,   36.78/s  (0.532s,   37.57/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2900/3456 ( 84%)]  Loss:  0.514037 (0.5081)  Time: 0.547s,   36.53/s  (0.533s,   37.55/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [2950/3456 ( 85%)]  Loss:  0.532559 (0.5080)  Time: 0.545s,   36.72/s  (0.533s,   37.53/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3000/3456 ( 87%)]  Loss:  0.446990 (0.5078)  Time: 0.542s,   36.91/s  (0.533s,   37.52/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3050/3456 ( 88%)]  Loss:  0.426665 (0.5074)  Time: 0.549s,   36.42/s  (0.533s,   37.50/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3100/3456 ( 90%)]  Loss:  0.469150 (0.5070)  Time: 0.540s,   37.04/s  (0.534s,   37.49/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3150/3456 ( 91%)]  Loss:  0.478054 (0.5068)  Time: 0.527s,   37.94/s  (0.534s,   37.48/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3200/3456 ( 93%)]  Loss:  0.520364 (0.5066)  Time: 0.528s,   37.91/s  (0.534s,   37.49/s)  LR: 3.989e-02  Data: 0.011 (0.011)\n",
      "Train: 10 [3250/3456 ( 94%)]  Loss:  1.046994 (0.5161)  Time: 0.932s,   21.45/s  (0.538s,   37.20/s)  LR: 3.989e-02  Data: 0.417 (0.015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 10 [3300/3456 ( 96%)]  Loss:  1.625292 (0.5348)  Time: 0.942s,   21.24/s  (0.543s,   36.82/s)  LR: 3.989e-02  Data: 0.425 (0.021)\n",
      "Train: 10 [3350/3456 ( 97%)]  Loss:  1.054451 (0.5413)  Time: 0.877s,   22.80/s  (0.548s,   36.47/s)  LR: 3.989e-02  Data: 0.362 (0.026)\n",
      "Train: 10 [3400/3456 ( 98%)]  Loss:  1.161199 (0.5500)  Time: 0.882s,   22.69/s  (0.553s,   36.14/s)  LR: 3.989e-02  Data: 0.364 (0.031)\n",
      "Train: 10 [3450/3456 (100%)]  Loss:  1.208770 (0.5550)  Time: 0.903s,   22.14/s  (0.558s,   35.83/s)  LR: 3.989e-02  Data: 0.370 (0.036)\n",
      "Train: 10 [3455/3456 (100%)]  Loss:  1.155355 (0.5557)  Time: 0.448s,   24.54/s  (0.558s,   19.70/s)  LR: 3.989e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.869 (0.869)  Loss:  2.5243 (2.5243)  \n",
      "Test: [  50/147]  Time: 0.268 (0.280)  Loss:  3.0184 (2.8477)  \n",
      "Test: [ 100/147]  Time: 0.267 (0.276)  Loss:  2.9117 (2.8449)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.322)  Loss:  1.6646 (2.7430)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-10.pth.tar', 2.7430243927079276)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-4.pth.tar', 3.236524259721911)\n",
      "\n",
      "Train: 11 [   0/3456 (  0%)]  Loss:  2.682690 (2.6827)  Time: 1.444s,   13.85/s  (1.444s,   13.85/s)  LR: 3.987e-02  Data: 0.826 (0.826)\n",
      "Train: 11 [  50/3456 (  1%)]  Loss:  0.459536 (0.8338)  Time: 0.526s,   38.00/s  (0.544s,   36.77/s)  LR: 3.987e-02  Data: 0.011 (0.027)\n",
      "Train: 11 [ 100/3456 (  3%)]  Loss:  0.631308 (0.7009)  Time: 0.526s,   37.99/s  (0.535s,   37.41/s)  LR: 3.987e-02  Data: 0.011 (0.019)\n",
      "Train: 11 [ 150/3456 (  4%)]  Loss:  0.604369 (0.6439)  Time: 0.525s,   38.10/s  (0.532s,   37.62/s)  LR: 3.987e-02  Data: 0.010 (0.016)\n",
      "Train: 11 [ 200/3456 (  6%)]  Loss:  0.626735 (0.6151)  Time: 0.524s,   38.14/s  (0.530s,   37.73/s)  LR: 3.987e-02  Data: 0.010 (0.015)\n",
      "Train: 11 [ 250/3456 (  7%)]  Loss:  0.525735 (0.5954)  Time: 0.526s,   38.01/s  (0.529s,   37.79/s)  LR: 3.987e-02  Data: 0.011 (0.014)\n",
      "Train: 11 [ 300/3456 (  9%)]  Loss:  0.476746 (0.5801)  Time: 0.525s,   38.12/s  (0.529s,   37.79/s)  LR: 3.987e-02  Data: 0.010 (0.013)\n",
      "Train: 11 [ 350/3456 ( 10%)]  Loss:  0.499327 (0.5683)  Time: 0.526s,   38.04/s  (0.529s,   37.83/s)  LR: 3.987e-02  Data: 0.011 (0.013)\n",
      "Train: 11 [ 400/3456 ( 12%)]  Loss:  0.411636 (0.5610)  Time: 0.526s,   38.02/s  (0.528s,   37.85/s)  LR: 3.987e-02  Data: 0.011 (0.013)\n",
      "Train: 11 [ 450/3456 ( 13%)]  Loss:  0.440552 (0.5561)  Time: 0.527s,   37.95/s  (0.528s,   37.87/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 500/3456 ( 14%)]  Loss:  0.487857 (0.5505)  Time: 0.525s,   38.10/s  (0.528s,   37.87/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 550/3456 ( 16%)]  Loss:  0.473036 (0.5456)  Time: 0.525s,   38.11/s  (0.528s,   37.88/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 600/3456 ( 17%)]  Loss:  0.549055 (0.5422)  Time: 0.523s,   38.21/s  (0.528s,   37.89/s)  LR: 3.987e-02  Data: 0.010 (0.012)\n",
      "Train: 11 [ 650/3456 ( 19%)]  Loss:  0.614804 (0.5404)  Time: 0.530s,   37.76/s  (0.528s,   37.89/s)  LR: 3.987e-02  Data: 0.010 (0.012)\n",
      "Train: 11 [ 700/3456 ( 20%)]  Loss:  0.415944 (0.5370)  Time: 0.527s,   37.93/s  (0.528s,   37.88/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 750/3456 ( 22%)]  Loss:  0.604505 (0.5349)  Time: 0.544s,   36.76/s  (0.528s,   37.87/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 800/3456 ( 23%)]  Loss:  0.553965 (0.5329)  Time: 0.548s,   36.52/s  (0.529s,   37.80/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 850/3456 ( 25%)]  Loss:  0.509012 (0.5310)  Time: 0.544s,   36.77/s  (0.530s,   37.73/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 900/3456 ( 26%)]  Loss:  0.435292 (0.5301)  Time: 0.541s,   36.94/s  (0.531s,   37.67/s)  LR: 3.987e-02  Data: 0.011 (0.012)\n",
      "Train: 11 [ 950/3456 ( 27%)]  Loss:  0.617604 (0.5288)  Time: 0.544s,   36.75/s  (0.532s,   37.62/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1000/3456 ( 29%)]  Loss:  0.579221 (0.5274)  Time: 0.546s,   36.65/s  (0.532s,   37.57/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1050/3456 ( 30%)]  Loss:  0.440847 (0.5259)  Time: 0.545s,   36.72/s  (0.533s,   37.53/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1100/3456 ( 32%)]  Loss:  0.487201 (0.5248)  Time: 0.543s,   36.86/s  (0.533s,   37.49/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1150/3456 ( 33%)]  Loss:  0.491864 (0.5236)  Time: 0.530s,   37.71/s  (0.533s,   37.50/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1200/3456 ( 35%)]  Loss:  0.475009 (0.5224)  Time: 0.544s,   36.75/s  (0.534s,   37.47/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1250/3456 ( 36%)]  Loss:  0.610369 (0.5218)  Time: 0.542s,   36.92/s  (0.534s,   37.44/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1300/3456 ( 38%)]  Loss:  0.489005 (0.5203)  Time: 0.545s,   36.70/s  (0.534s,   37.42/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1350/3456 ( 39%)]  Loss:  0.563722 (0.5193)  Time: 0.541s,   36.98/s  (0.535s,   37.40/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1400/3456 ( 41%)]  Loss:  0.519833 (0.5186)  Time: 0.546s,   36.60/s  (0.535s,   37.37/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1450/3456 ( 42%)]  Loss:  0.603476 (0.5181)  Time: 0.541s,   36.98/s  (0.536s,   37.34/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1500/3456 ( 43%)]  Loss:  0.481901 (0.5177)  Time: 0.548s,   36.49/s  (0.536s,   37.32/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1550/3456 ( 45%)]  Loss:  0.380184 (0.5174)  Time: 0.546s,   36.60/s  (0.536s,   37.30/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1600/3456 ( 46%)]  Loss:  0.511653 (0.5165)  Time: 0.541s,   36.95/s  (0.536s,   37.29/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1650/3456 ( 48%)]  Loss:  0.422946 (0.5160)  Time: 0.550s,   36.35/s  (0.537s,   37.27/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1700/3456 ( 49%)]  Loss:  0.357133 (0.5156)  Time: 0.542s,   36.87/s  (0.537s,   37.25/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1750/3456 ( 51%)]  Loss:  0.446596 (0.5150)  Time: 0.541s,   36.96/s  (0.537s,   37.24/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1800/3456 ( 52%)]  Loss:  0.568063 (0.5150)  Time: 0.542s,   36.88/s  (0.537s,   37.22/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1850/3456 ( 54%)]  Loss:  0.558516 (0.5143)  Time: 0.543s,   36.86/s  (0.537s,   37.21/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1900/3456 ( 55%)]  Loss:  0.532912 (0.5138)  Time: 0.547s,   36.57/s  (0.538s,   37.20/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [1950/3456 ( 56%)]  Loss:  0.525251 (0.5136)  Time: 0.548s,   36.52/s  (0.538s,   37.19/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2000/3456 ( 58%)]  Loss:  0.471141 (0.5135)  Time: 0.548s,   36.48/s  (0.538s,   37.18/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2050/3456 ( 59%)]  Loss:  0.503764 (0.5131)  Time: 0.547s,   36.57/s  (0.538s,   37.17/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2100/3456 ( 61%)]  Loss:  0.557358 (0.5130)  Time: 0.542s,   36.89/s  (0.538s,   37.16/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2150/3456 ( 62%)]  Loss:  0.449992 (0.5122)  Time: 0.545s,   36.72/s  (0.538s,   37.14/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2200/3456 ( 64%)]  Loss:  0.411225 (0.5113)  Time: 0.527s,   37.97/s  (0.538s,   37.16/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2250/3456 ( 65%)]  Loss:  0.481478 (0.5108)  Time: 0.526s,   38.04/s  (0.538s,   37.18/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2300/3456 ( 67%)]  Loss:  0.508452 (0.5102)  Time: 0.532s,   37.62/s  (0.538s,   37.19/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2350/3456 ( 68%)]  Loss:  0.451231 (0.5097)  Time: 0.546s,   36.62/s  (0.538s,   37.20/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2400/3456 ( 69%)]  Loss:  0.511098 (0.5097)  Time: 0.528s,   37.85/s  (0.537s,   37.21/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2450/3456 ( 71%)]  Loss:  0.546733 (0.5094)  Time: 0.525s,   38.09/s  (0.537s,   37.23/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2500/3456 ( 72%)]  Loss:  0.395867 (0.5093)  Time: 0.546s,   36.60/s  (0.537s,   37.22/s)  LR: 3.987e-02  Data: 0.012 (0.011)\n",
      "Train: 11 [2550/3456 ( 74%)]  Loss:  0.493158 (0.5090)  Time: 0.545s,   36.70/s  (0.537s,   37.22/s)  LR: 3.987e-02  Data: 0.012 (0.011)\n",
      "Train: 11 [2600/3456 ( 75%)]  Loss:  0.411598 (0.5086)  Time: 0.542s,   36.90/s  (0.538s,   37.21/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2650/3456 ( 77%)]  Loss:  0.559294 (0.5085)  Time: 0.548s,   36.49/s  (0.538s,   37.20/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2700/3456 ( 78%)]  Loss:  0.525950 (0.5081)  Time: 0.538s,   37.15/s  (0.538s,   37.19/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2750/3456 ( 80%)]  Loss:  0.476748 (0.5079)  Time: 0.540s,   37.05/s  (0.538s,   37.18/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2800/3456 ( 81%)]  Loss:  0.454586 (0.5076)  Time: 0.542s,   36.93/s  (0.538s,   37.18/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2850/3456 ( 82%)]  Loss:  0.527270 (0.5073)  Time: 0.525s,   38.09/s  (0.538s,   37.18/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2900/3456 ( 84%)]  Loss:  0.513337 (0.5071)  Time: 0.531s,   37.69/s  (0.538s,   37.19/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [2950/3456 ( 85%)]  Loss:  0.496874 (0.5069)  Time: 0.526s,   38.02/s  (0.538s,   37.20/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3000/3456 ( 87%)]  Loss:  0.484268 (0.5068)  Time: 0.527s,   37.97/s  (0.537s,   37.21/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3050/3456 ( 88%)]  Loss:  0.441736 (0.5065)  Time: 0.524s,   38.14/s  (0.537s,   37.22/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3100/3456 ( 90%)]  Loss:  0.483005 (0.5062)  Time: 0.528s,   37.85/s  (0.537s,   37.23/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3150/3456 ( 91%)]  Loss:  0.443371 (0.5059)  Time: 0.529s,   37.82/s  (0.537s,   37.24/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3200/3456 ( 93%)]  Loss:  0.517301 (0.5058)  Time: 0.532s,   37.57/s  (0.537s,   37.25/s)  LR: 3.987e-02  Data: 0.011 (0.011)\n",
      "Train: 11 [3250/3456 ( 94%)]  Loss:  1.437239 (0.5182)  Time: 0.977s,   20.47/s  (0.541s,   36.96/s)  LR: 3.987e-02  Data: 0.457 (0.015)\n",
      "Train: 11 [3300/3456 ( 96%)]  Loss:  1.218103 (0.5268)  Time: 0.867s,   23.08/s  (0.547s,   36.58/s)  LR: 3.987e-02  Data: 0.351 (0.021)\n",
      "Train: 11 [3350/3456 ( 97%)]  Loss:  0.671843 (0.5317)  Time: 0.909s,   22.00/s  (0.552s,   36.24/s)  LR: 3.987e-02  Data: 0.394 (0.026)\n",
      "Train: 11 [3400/3456 ( 98%)]  Loss:  1.269930 (0.5375)  Time: 0.873s,   22.90/s  (0.557s,   35.91/s)  LR: 3.987e-02  Data: 0.355 (0.032)\n",
      "Train: 11 [3450/3456 (100%)]  Loss:  0.678607 (0.5425)  Time: 0.889s,   22.49/s  (0.562s,   35.60/s)  LR: 3.987e-02  Data: 0.372 (0.037)\n",
      "Train: 11 [3455/3456 (100%)]  Loss:  1.198985 (0.5430)  Time: 0.448s,   24.54/s  (0.562s,   19.57/s)  LR: 3.987e-02  Data: 0.038 (0.037)\n",
      "Test: [   0/147]  Time: 0.862 (0.862)  Loss:  2.0931 (2.0931)  \n",
      "Test: [  50/147]  Time: 0.264 (0.277)  Loss:  2.8104 (2.5317)  \n",
      "Test: [ 100/147]  Time: 0.278 (0.278)  Loss:  2.5995 (2.5254)  \n",
      "Test: [ 147/147]  Time: 0.299 (0.325)  Loss:  0.9929 (2.3857)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-10.pth.tar', 2.7430243927079276)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-5.pth.tar', 3.1229220181703568)\n",
      "\n",
      "Train: 12 [   0/3456 (  0%)]  Loss:  2.463545 (2.4635)  Time: 1.434s,   13.95/s  (1.434s,   13.95/s)  LR: 3.984e-02  Data: 0.840 (0.840)\n",
      "Train: 12 [  50/3456 (  1%)]  Loss:  0.532427 (0.7626)  Time: 0.524s,   38.16/s  (0.543s,   36.81/s)  LR: 3.984e-02  Data: 0.010 (0.027)\n",
      "Train: 12 [ 100/3456 (  3%)]  Loss:  0.591098 (0.6582)  Time: 0.523s,   38.21/s  (0.535s,   37.38/s)  LR: 3.984e-02  Data: 0.011 (0.019)\n",
      "Train: 12 [ 150/3456 (  4%)]  Loss:  0.492345 (0.6075)  Time: 0.531s,   37.64/s  (0.533s,   37.49/s)  LR: 3.984e-02  Data: 0.011 (0.016)\n",
      "Train: 12 [ 200/3456 (  6%)]  Loss:  0.602781 (0.5855)  Time: 0.529s,   37.78/s  (0.531s,   37.63/s)  LR: 3.984e-02  Data: 0.010 (0.015)\n",
      "Train: 12 [ 250/3456 (  7%)]  Loss:  0.514249 (0.5722)  Time: 0.532s,   37.57/s  (0.530s,   37.73/s)  LR: 3.984e-02  Data: 0.010 (0.014)\n",
      "Train: 12 [ 300/3456 (  9%)]  Loss:  0.523378 (0.5623)  Time: 0.525s,   38.08/s  (0.530s,   37.77/s)  LR: 3.984e-02  Data: 0.011 (0.013)\n",
      "Train: 12 [ 350/3456 ( 10%)]  Loss:  0.555804 (0.5548)  Time: 0.531s,   37.69/s  (0.529s,   37.79/s)  LR: 3.984e-02  Data: 0.011 (0.013)\n",
      "Train: 12 [ 400/3456 ( 12%)]  Loss:  0.454906 (0.5487)  Time: 0.524s,   38.19/s  (0.529s,   37.78/s)  LR: 3.984e-02  Data: 0.010 (0.013)\n",
      "Train: 12 [ 450/3456 ( 13%)]  Loss:  0.521058 (0.5452)  Time: 0.524s,   38.14/s  (0.530s,   37.74/s)  LR: 3.984e-02  Data: 0.010 (0.012)\n",
      "Train: 12 [ 500/3456 ( 14%)]  Loss:  0.490457 (0.5411)  Time: 0.523s,   38.24/s  (0.529s,   37.77/s)  LR: 3.984e-02  Data: 0.010 (0.012)\n",
      "Train: 12 [ 550/3456 ( 16%)]  Loss:  0.468319 (0.5366)  Time: 0.527s,   37.94/s  (0.529s,   37.80/s)  LR: 3.984e-02  Data: 0.011 (0.012)\n",
      "Train: 12 [ 600/3456 ( 17%)]  Loss:  0.536240 (0.5336)  Time: 0.525s,   38.08/s  (0.529s,   37.82/s)  LR: 3.984e-02  Data: 0.011 (0.012)\n",
      "Train: 12 [ 650/3456 ( 19%)]  Loss:  0.705920 (0.5318)  Time: 0.526s,   38.01/s  (0.529s,   37.84/s)  LR: 3.984e-02  Data: 0.011 (0.012)\n",
      "Train: 12 [ 700/3456 ( 20%)]  Loss:  0.501303 (0.5291)  Time: 0.526s,   38.04/s  (0.528s,   37.85/s)  LR: 3.984e-02  Data: 0.010 (0.012)\n",
      "Train: 12 [ 750/3456 ( 22%)]  Loss:  0.531503 (0.5272)  Time: 0.524s,   38.16/s  (0.528s,   37.87/s)  LR: 3.984e-02  Data: 0.010 (0.012)\n",
      "Train: 12 [ 800/3456 ( 23%)]  Loss:  0.567047 (0.5255)  Time: 0.526s,   38.02/s  (0.528s,   37.88/s)  LR: 3.984e-02  Data: 0.011 (0.012)\n",
      "Train: 12 [ 850/3456 ( 25%)]  Loss:  0.503416 (0.5239)  Time: 0.547s,   36.56/s  (0.529s,   37.82/s)  LR: 3.984e-02  Data: 0.011 (0.012)\n",
      "Train: 12 [ 900/3456 ( 26%)]  Loss:  0.444720 (0.5230)  Time: 0.548s,   36.51/s  (0.530s,   37.76/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [ 950/3456 ( 27%)]  Loss:  0.532052 (0.5211)  Time: 0.543s,   36.81/s  (0.531s,   37.70/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1000/3456 ( 29%)]  Loss:  0.571621 (0.5197)  Time: 0.544s,   36.77/s  (0.531s,   37.65/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1050/3456 ( 30%)]  Loss:  0.507128 (0.5191)  Time: 0.524s,   38.13/s  (0.531s,   37.65/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1100/3456 ( 32%)]  Loss:  0.566819 (0.5181)  Time: 0.527s,   37.92/s  (0.531s,   37.66/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1150/3456 ( 33%)]  Loss:  0.536455 (0.5165)  Time: 0.526s,   38.02/s  (0.531s,   37.68/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1200/3456 ( 35%)]  Loss:  0.509733 (0.5158)  Time: 0.530s,   37.74/s  (0.531s,   37.69/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1250/3456 ( 36%)]  Loss:  0.453474 (0.5151)  Time: 0.531s,   37.66/s  (0.531s,   37.69/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1300/3456 ( 38%)]  Loss:  0.535653 (0.5139)  Time: 0.528s,   37.90/s  (0.531s,   37.70/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1350/3456 ( 39%)]  Loss:  0.533206 (0.5132)  Time: 0.532s,   37.59/s  (0.530s,   37.70/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1400/3456 ( 41%)]  Loss:  0.499154 (0.5125)  Time: 0.526s,   38.02/s  (0.530s,   37.71/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1450/3456 ( 42%)]  Loss:  0.584363 (0.5120)  Time: 0.527s,   37.92/s  (0.530s,   37.72/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1500/3456 ( 43%)]  Loss:  0.492038 (0.5116)  Time: 0.530s,   37.71/s  (0.530s,   37.72/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1550/3456 ( 45%)]  Loss:  0.355497 (0.5115)  Time: 0.523s,   38.23/s  (0.530s,   37.73/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1600/3456 ( 46%)]  Loss:  0.517858 (0.5108)  Time: 0.525s,   38.11/s  (0.530s,   37.72/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1650/3456 ( 48%)]  Loss:  0.388651 (0.5099)  Time: 0.545s,   36.67/s  (0.530s,   37.70/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1700/3456 ( 49%)]  Loss:  0.413424 (0.5094)  Time: 0.545s,   36.70/s  (0.531s,   37.67/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1750/3456 ( 51%)]  Loss:  0.498581 (0.5088)  Time: 0.543s,   36.81/s  (0.531s,   37.65/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1800/3456 ( 52%)]  Loss:  0.534126 (0.5087)  Time: 0.546s,   36.64/s  (0.532s,   37.62/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1850/3456 ( 54%)]  Loss:  0.547406 (0.5081)  Time: 0.526s,   38.03/s  (0.532s,   37.60/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [1900/3456 ( 55%)]  Loss:  0.536825 (0.5073)  Time: 0.524s,   38.16/s  (0.532s,   37.61/s)  LR: 3.984e-02  Data: 0.010 (0.011)\n",
      "Train: 12 [1950/3456 ( 56%)]  Loss:  0.595253 (0.5074)  Time: 0.525s,   38.08/s  (0.532s,   37.62/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2000/3456 ( 58%)]  Loss:  0.454104 (0.5073)  Time: 0.543s,   36.84/s  (0.532s,   37.61/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2050/3456 ( 59%)]  Loss:  0.540280 (0.5071)  Time: 0.543s,   36.85/s  (0.532s,   37.59/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2100/3456 ( 61%)]  Loss:  0.506969 (0.5069)  Time: 0.546s,   36.64/s  (0.532s,   37.57/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2150/3456 ( 62%)]  Loss:  0.447331 (0.5064)  Time: 0.545s,   36.71/s  (0.533s,   37.55/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2200/3456 ( 64%)]  Loss:  0.458434 (0.5056)  Time: 0.545s,   36.67/s  (0.533s,   37.53/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2250/3456 ( 65%)]  Loss:  0.528939 (0.5049)  Time: 0.526s,   38.05/s  (0.533s,   37.53/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2300/3456 ( 67%)]  Loss:  0.441989 (0.5043)  Time: 0.525s,   38.12/s  (0.533s,   37.54/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2350/3456 ( 68%)]  Loss:  0.488349 (0.5039)  Time: 0.544s,   36.76/s  (0.533s,   37.54/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2400/3456 ( 69%)]  Loss:  0.515979 (0.5042)  Time: 0.546s,   36.66/s  (0.533s,   37.53/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2450/3456 ( 71%)]  Loss:  0.535491 (0.5041)  Time: 0.542s,   36.89/s  (0.533s,   37.51/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2500/3456 ( 72%)]  Loss:  0.437087 (0.5040)  Time: 0.546s,   36.65/s  (0.533s,   37.49/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2550/3456 ( 74%)]  Loss:  0.540442 (0.5036)  Time: 0.548s,   36.48/s  (0.534s,   37.48/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2600/3456 ( 75%)]  Loss:  0.450089 (0.5033)  Time: 0.548s,   36.49/s  (0.534s,   37.46/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2650/3456 ( 77%)]  Loss:  0.564651 (0.5032)  Time: 0.550s,   36.38/s  (0.534s,   37.45/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2700/3456 ( 78%)]  Loss:  0.489131 (0.5028)  Time: 0.545s,   36.67/s  (0.534s,   37.43/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2750/3456 ( 80%)]  Loss:  0.519437 (0.5025)  Time: 0.542s,   36.91/s  (0.535s,   37.41/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2800/3456 ( 81%)]  Loss:  0.433012 (0.5023)  Time: 0.544s,   36.79/s  (0.535s,   37.40/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2850/3456 ( 82%)]  Loss:  0.521858 (0.5019)  Time: 0.544s,   36.75/s  (0.535s,   37.39/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2900/3456 ( 84%)]  Loss:  0.560517 (0.5018)  Time: 0.545s,   36.72/s  (0.535s,   37.39/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [2950/3456 ( 85%)]  Loss:  0.586222 (0.5016)  Time: 0.532s,   37.58/s  (0.535s,   37.39/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [3000/3456 ( 87%)]  Loss:  0.459178 (0.5015)  Time: 0.525s,   38.11/s  (0.535s,   37.40/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [3050/3456 ( 88%)]  Loss:  0.417268 (0.5012)  Time: 0.549s,   36.44/s  (0.535s,   37.41/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [3100/3456 ( 90%)]  Loss:  0.502912 (0.5009)  Time: 0.542s,   36.92/s  (0.535s,   37.40/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n",
      "Train: 12 [3150/3456 ( 91%)]  Loss:  0.450359 (0.5004)  Time: 0.542s,   36.90/s  (0.535s,   37.39/s)  LR: 3.984e-02  Data: 0.012 (0.011)\n",
      "Train: 12 [3200/3456 ( 93%)]  Loss:  0.474194 (0.5002)  Time: 0.543s,   36.83/s  (0.535s,   37.38/s)  LR: 3.984e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 12 [3250/3456 ( 94%)]  Loss:  1.149247 (0.5116)  Time: 0.946s,   21.15/s  (0.539s,   37.09/s)  LR: 3.984e-02  Data: 0.413 (0.015)\n",
      "Train: 12 [3300/3456 ( 96%)]  Loss:  0.873721 (0.5196)  Time: 0.881s,   22.71/s  (0.545s,   36.70/s)  LR: 3.984e-02  Data: 0.348 (0.021)\n",
      "Train: 12 [3350/3456 ( 97%)]  Loss:  0.960530 (0.5252)  Time: 0.916s,   21.82/s  (0.550s,   36.35/s)  LR: 3.984e-02  Data: 0.382 (0.026)\n",
      "Train: 12 [3400/3456 ( 98%)]  Loss:  1.126794 (0.5310)  Time: 0.919s,   21.77/s  (0.555s,   36.03/s)  LR: 3.984e-02  Data: 0.385 (0.031)\n",
      "Train: 12 [3450/3456 (100%)]  Loss:  0.838117 (0.5345)  Time: 0.880s,   22.73/s  (0.560s,   35.72/s)  LR: 3.984e-02  Data: 0.351 (0.035)\n",
      "Train: 12 [3455/3456 (100%)]  Loss:  0.630152 (0.5350)  Time: 0.465s,   23.63/s  (0.560s,   19.63/s)  LR: 3.984e-02  Data: 0.039 (0.036)\n",
      "Test: [   0/147]  Time: 0.880 (0.880)  Loss:  2.2905 (2.2905)  \n",
      "Test: [  50/147]  Time: 0.285 (0.294)  Loss:  2.8333 (2.5667)  \n",
      "Test: [ 100/147]  Time: 0.285 (0.289)  Loss:  2.6314 (2.5565)  \n",
      "Test: [ 147/147]  Time: 0.303 (0.333)  Loss:  1.7085 (2.4650)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-10.pth.tar', 2.7430243927079276)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-7.pth.tar', 2.973448793630342)\n",
      "\n",
      "Train: 13 [   0/3456 (  0%)]  Loss:  2.794406 (2.7944)  Time: 1.425s,   14.03/s  (1.425s,   14.03/s)  LR: 3.982e-02  Data: 0.800 (0.800)\n",
      "Train: 13 [  50/3456 (  1%)]  Loss:  0.459591 (0.7692)  Time: 0.524s,   38.14/s  (0.544s,   36.77/s)  LR: 3.982e-02  Data: 0.011 (0.026)\n",
      "Train: 13 [ 100/3456 (  3%)]  Loss:  0.607314 (0.6543)  Time: 0.529s,   37.78/s  (0.536s,   37.34/s)  LR: 3.982e-02  Data: 0.011 (0.019)\n",
      "Train: 13 [ 150/3456 (  4%)]  Loss:  0.502015 (0.6045)  Time: 0.547s,   36.59/s  (0.533s,   37.52/s)  LR: 3.982e-02  Data: 0.011 (0.016)\n",
      "Train: 13 [ 200/3456 (  6%)]  Loss:  0.523939 (0.5804)  Time: 0.527s,   37.93/s  (0.535s,   37.38/s)  LR: 3.982e-02  Data: 0.011 (0.015)\n",
      "Train: 13 [ 250/3456 (  7%)]  Loss:  0.525034 (0.5668)  Time: 0.525s,   38.06/s  (0.534s,   37.49/s)  LR: 3.982e-02  Data: 0.010 (0.014)\n",
      "Train: 13 [ 300/3456 (  9%)]  Loss:  0.573579 (0.5561)  Time: 0.526s,   38.04/s  (0.532s,   37.57/s)  LR: 3.982e-02  Data: 0.011 (0.013)\n",
      "Train: 13 [ 350/3456 ( 10%)]  Loss:  0.519903 (0.5488)  Time: 0.525s,   38.07/s  (0.531s,   37.63/s)  LR: 3.982e-02  Data: 0.011 (0.013)\n",
      "Train: 13 [ 400/3456 ( 12%)]  Loss:  0.453068 (0.5428)  Time: 0.526s,   38.06/s  (0.531s,   37.68/s)  LR: 3.982e-02  Data: 0.011 (0.013)\n",
      "Train: 13 [ 450/3456 ( 13%)]  Loss:  0.529382 (0.5400)  Time: 0.548s,   36.49/s  (0.531s,   37.69/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 500/3456 ( 14%)]  Loss:  0.554753 (0.5362)  Time: 0.533s,   37.55/s  (0.531s,   37.63/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 550/3456 ( 16%)]  Loss:  0.467274 (0.5328)  Time: 0.548s,   36.52/s  (0.533s,   37.55/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 600/3456 ( 17%)]  Loss:  0.527788 (0.5284)  Time: 0.546s,   36.64/s  (0.534s,   37.48/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 650/3456 ( 19%)]  Loss:  0.509513 (0.5267)  Time: 0.543s,   36.86/s  (0.535s,   37.41/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 700/3456 ( 20%)]  Loss:  0.418665 (0.5236)  Time: 0.547s,   36.53/s  (0.535s,   37.36/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 750/3456 ( 22%)]  Loss:  0.609737 (0.5217)  Time: 0.547s,   36.55/s  (0.536s,   37.32/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 800/3456 ( 23%)]  Loss:  0.548893 (0.5199)  Time: 0.542s,   36.93/s  (0.536s,   37.28/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 850/3456 ( 25%)]  Loss:  0.521417 (0.5187)  Time: 0.545s,   36.69/s  (0.537s,   37.25/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 900/3456 ( 26%)]  Loss:  0.442526 (0.5184)  Time: 0.544s,   36.76/s  (0.537s,   37.21/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [ 950/3456 ( 27%)]  Loss:  0.514114 (0.5168)  Time: 0.527s,   37.98/s  (0.537s,   37.24/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [1000/3456 ( 29%)]  Loss:  0.539177 (0.5156)  Time: 0.532s,   37.58/s  (0.537s,   37.27/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [1050/3456 ( 30%)]  Loss:  0.477729 (0.5148)  Time: 0.525s,   38.08/s  (0.536s,   37.30/s)  LR: 3.982e-02  Data: 0.011 (0.012)\n",
      "Train: 13 [1100/3456 ( 32%)]  Loss:  0.529193 (0.5142)  Time: 0.530s,   37.71/s  (0.536s,   37.32/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1150/3456 ( 33%)]  Loss:  0.446115 (0.5130)  Time: 0.529s,   37.81/s  (0.536s,   37.35/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1200/3456 ( 35%)]  Loss:  0.505918 (0.5120)  Time: 0.526s,   38.05/s  (0.535s,   37.37/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1250/3456 ( 36%)]  Loss:  0.618410 (0.5114)  Time: 0.546s,   36.61/s  (0.535s,   37.37/s)  LR: 3.982e-02  Data: 0.012 (0.011)\n",
      "Train: 13 [1300/3456 ( 38%)]  Loss:  0.501257 (0.5101)  Time: 0.548s,   36.53/s  (0.535s,   37.35/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1350/3456 ( 39%)]  Loss:  0.502333 (0.5096)  Time: 0.541s,   36.94/s  (0.536s,   37.32/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1400/3456 ( 41%)]  Loss:  0.427498 (0.5088)  Time: 0.544s,   36.75/s  (0.536s,   37.30/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1450/3456 ( 42%)]  Loss:  0.607951 (0.5082)  Time: 0.525s,   38.12/s  (0.536s,   37.32/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1500/3456 ( 43%)]  Loss:  0.480291 (0.5077)  Time: 0.524s,   38.19/s  (0.536s,   37.34/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1550/3456 ( 45%)]  Loss:  0.384083 (0.5073)  Time: 0.544s,   36.77/s  (0.536s,   37.33/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1600/3456 ( 46%)]  Loss:  0.501906 (0.5066)  Time: 0.548s,   36.53/s  (0.536s,   37.31/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1650/3456 ( 48%)]  Loss:  0.405265 (0.5064)  Time: 0.545s,   36.73/s  (0.536s,   37.29/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1700/3456 ( 49%)]  Loss:  0.367262 (0.5059)  Time: 0.543s,   36.84/s  (0.537s,   37.28/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1750/3456 ( 51%)]  Loss:  0.429898 (0.5057)  Time: 0.541s,   36.99/s  (0.537s,   37.27/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1800/3456 ( 52%)]  Loss:  0.574054 (0.5056)  Time: 0.525s,   38.12/s  (0.537s,   37.27/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1850/3456 ( 54%)]  Loss:  0.549386 (0.5048)  Time: 0.526s,   38.01/s  (0.536s,   37.29/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1900/3456 ( 55%)]  Loss:  0.538952 (0.5045)  Time: 0.527s,   37.97/s  (0.536s,   37.31/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [1950/3456 ( 56%)]  Loss:  0.523156 (0.5042)  Time: 0.527s,   37.94/s  (0.536s,   37.33/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2000/3456 ( 58%)]  Loss:  0.421183 (0.5039)  Time: 0.525s,   38.11/s  (0.535s,   37.35/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2050/3456 ( 59%)]  Loss:  0.536047 (0.5039)  Time: 0.543s,   36.84/s  (0.536s,   37.34/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2100/3456 ( 61%)]  Loss:  0.457364 (0.5037)  Time: 0.545s,   36.71/s  (0.536s,   37.32/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2150/3456 ( 62%)]  Loss:  0.548891 (0.5032)  Time: 0.543s,   36.86/s  (0.536s,   37.30/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2200/3456 ( 64%)]  Loss:  0.390832 (0.5024)  Time: 0.547s,   36.60/s  (0.536s,   37.29/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2250/3456 ( 65%)]  Loss:  0.467778 (0.5022)  Time: 0.547s,   36.54/s  (0.537s,   37.28/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2300/3456 ( 67%)]  Loss:  0.472968 (0.5017)  Time: 0.548s,   36.50/s  (0.537s,   37.26/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2350/3456 ( 68%)]  Loss:  0.471678 (0.5014)  Time: 0.547s,   36.55/s  (0.537s,   37.25/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2400/3456 ( 69%)]  Loss:  0.444336 (0.5015)  Time: 0.547s,   36.55/s  (0.537s,   37.23/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2450/3456 ( 71%)]  Loss:  0.465492 (0.5014)  Time: 0.547s,   36.57/s  (0.537s,   37.22/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2500/3456 ( 72%)]  Loss:  0.525332 (0.5013)  Time: 0.545s,   36.69/s  (0.538s,   37.21/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2550/3456 ( 74%)]  Loss:  0.467981 (0.5009)  Time: 0.545s,   36.67/s  (0.538s,   37.20/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2600/3456 ( 75%)]  Loss:  0.553902 (0.5008)  Time: 0.544s,   36.80/s  (0.538s,   37.19/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2650/3456 ( 77%)]  Loss:  0.590680 (0.5007)  Time: 0.540s,   37.02/s  (0.538s,   37.18/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2700/3456 ( 78%)]  Loss:  0.562926 (0.5006)  Time: 0.543s,   36.84/s  (0.538s,   37.17/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2750/3456 ( 80%)]  Loss:  0.527518 (0.5006)  Time: 0.546s,   36.60/s  (0.538s,   37.16/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2800/3456 ( 81%)]  Loss:  0.427429 (0.5004)  Time: 0.542s,   36.92/s  (0.538s,   37.15/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2850/3456 ( 82%)]  Loss:  0.558898 (0.5004)  Time: 0.531s,   37.67/s  (0.538s,   37.16/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2900/3456 ( 84%)]  Loss:  0.567089 (0.5004)  Time: 0.525s,   38.06/s  (0.538s,   37.17/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [2950/3456 ( 85%)]  Loss:  0.543138 (0.5004)  Time: 0.529s,   37.83/s  (0.538s,   37.19/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [3000/3456 ( 87%)]  Loss:  0.435816 (0.5003)  Time: 0.529s,   37.80/s  (0.538s,   37.20/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [3050/3456 ( 88%)]  Loss:  0.358651 (0.4999)  Time: 0.527s,   37.96/s  (0.537s,   37.21/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [3100/3456 ( 90%)]  Loss:  0.504022 (0.4995)  Time: 0.529s,   37.82/s  (0.537s,   37.22/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [3150/3456 ( 91%)]  Loss:  0.483814 (0.4993)  Time: 0.546s,   36.62/s  (0.537s,   37.21/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n",
      "Train: 13 [3200/3456 ( 93%)]  Loss:  0.493187 (0.4993)  Time: 0.543s,   36.84/s  (0.538s,   37.21/s)  LR: 3.982e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 13 [3250/3456 ( 94%)]  Loss:  2.165872 (0.5100)  Time: 0.963s,   20.77/s  (0.542s,   36.92/s)  LR: 3.982e-02  Data: 0.428 (0.015)\n",
      "Train: 13 [3300/3456 ( 96%)]  Loss:  0.659838 (0.5170)  Time: 0.901s,   22.19/s  (0.547s,   36.54/s)  LR: 3.982e-02  Data: 0.368 (0.021)\n",
      "Train: 13 [3350/3456 ( 97%)]  Loss:  0.814681 (0.5225)  Time: 0.899s,   22.25/s  (0.553s,   36.20/s)  LR: 3.982e-02  Data: 0.384 (0.026)\n",
      "Train: 13 [3400/3456 ( 98%)]  Loss:  1.358283 (0.5278)  Time: 0.896s,   22.33/s  (0.558s,   35.87/s)  LR: 3.982e-02  Data: 0.377 (0.031)\n",
      "Train: 13 [3450/3456 (100%)]  Loss:  0.631253 (0.5309)  Time: 0.885s,   22.60/s  (0.562s,   35.56/s)  LR: 3.982e-02  Data: 0.351 (0.036)\n",
      "Train: 13 [3455/3456 (100%)]  Loss:  1.175908 (0.5314)  Time: 0.459s,   23.98/s  (0.563s,   19.55/s)  LR: 3.982e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.883 (0.883)  Loss:  2.2876 (2.2876)  \n",
      "Test: [  50/147]  Time: 0.280 (0.295)  Loss:  2.9108 (2.6248)  \n",
      "Test: [ 100/147]  Time: 0.280 (0.289)  Loss:  2.6880 (2.6188)  \n",
      "Test: [ 147/147]  Time: 0.302 (0.334)  Loss:  0.5778 (2.4392)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-10.pth.tar', 2.7430243927079276)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-2.pth.tar', 2.78026394747399)\n",
      "\n",
      "Train: 14 [   0/3456 (  0%)]  Loss:  2.142945 (2.1429)  Time: 1.452s,   13.78/s  (1.452s,   13.78/s)  LR: 3.979e-02  Data: 0.807 (0.807)\n",
      "Train: 14 [  50/3456 (  1%)]  Loss:  0.443409 (0.7452)  Time: 0.545s,   36.71/s  (0.563s,   35.53/s)  LR: 3.979e-02  Data: 0.011 (0.027)\n",
      "Train: 14 [ 100/3456 (  3%)]  Loss:  0.660081 (0.6497)  Time: 0.525s,   38.08/s  (0.546s,   36.61/s)  LR: 3.979e-02  Data: 0.011 (0.019)\n",
      "Train: 14 [ 150/3456 (  4%)]  Loss:  0.445783 (0.5996)  Time: 0.526s,   38.02/s  (0.541s,   36.95/s)  LR: 3.979e-02  Data: 0.011 (0.016)\n",
      "Train: 14 [ 200/3456 (  6%)]  Loss:  0.596904 (0.5781)  Time: 0.523s,   38.21/s  (0.537s,   37.21/s)  LR: 3.979e-02  Data: 0.011 (0.015)\n",
      "Train: 14 [ 250/3456 (  7%)]  Loss:  0.571743 (0.5634)  Time: 0.542s,   36.89/s  (0.536s,   37.33/s)  LR: 3.979e-02  Data: 0.011 (0.014)\n",
      "Train: 14 [ 300/3456 (  9%)]  Loss:  0.549595 (0.5531)  Time: 0.545s,   36.71/s  (0.537s,   37.23/s)  LR: 3.979e-02  Data: 0.011 (0.013)\n",
      "Train: 14 [ 350/3456 ( 10%)]  Loss:  0.531931 (0.5452)  Time: 0.547s,   36.58/s  (0.538s,   37.16/s)  LR: 3.979e-02  Data: 0.011 (0.013)\n",
      "Train: 14 [ 400/3456 ( 12%)]  Loss:  0.577718 (0.5401)  Time: 0.543s,   36.85/s  (0.539s,   37.11/s)  LR: 3.979e-02  Data: 0.011 (0.013)\n",
      "Train: 14 [ 450/3456 ( 13%)]  Loss:  0.516612 (0.5372)  Time: 0.545s,   36.69/s  (0.540s,   37.07/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 500/3456 ( 14%)]  Loss:  0.528142 (0.5334)  Time: 0.528s,   37.87/s  (0.539s,   37.13/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 550/3456 ( 16%)]  Loss:  0.544173 (0.5293)  Time: 0.525s,   38.07/s  (0.538s,   37.18/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 600/3456 ( 17%)]  Loss:  0.504043 (0.5268)  Time: 0.531s,   37.65/s  (0.537s,   37.24/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 650/3456 ( 19%)]  Loss:  0.581508 (0.5252)  Time: 0.543s,   36.86/s  (0.537s,   37.27/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 700/3456 ( 20%)]  Loss:  0.406132 (0.5224)  Time: 0.550s,   36.39/s  (0.537s,   37.22/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 750/3456 ( 22%)]  Loss:  0.538322 (0.5208)  Time: 0.525s,   38.11/s  (0.537s,   37.25/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 800/3456 ( 23%)]  Loss:  0.504920 (0.5184)  Time: 0.525s,   38.12/s  (0.536s,   37.30/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 850/3456 ( 25%)]  Loss:  0.548789 (0.5166)  Time: 0.525s,   38.08/s  (0.535s,   37.35/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 900/3456 ( 26%)]  Loss:  0.488522 (0.5161)  Time: 0.524s,   38.18/s  (0.535s,   37.39/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [ 950/3456 ( 27%)]  Loss:  0.470358 (0.5144)  Time: 0.525s,   38.13/s  (0.534s,   37.42/s)  LR: 3.979e-02  Data: 0.011 (0.012)\n",
      "Train: 14 [1000/3456 ( 29%)]  Loss:  0.493318 (0.5134)  Time: 0.524s,   38.13/s  (0.534s,   37.46/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1050/3456 ( 30%)]  Loss:  0.508116 (0.5124)  Time: 0.526s,   38.03/s  (0.534s,   37.49/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1100/3456 ( 32%)]  Loss:  0.498213 (0.5116)  Time: 0.525s,   38.13/s  (0.533s,   37.52/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1150/3456 ( 33%)]  Loss:  0.484824 (0.5109)  Time: 0.525s,   38.11/s  (0.533s,   37.54/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1200/3456 ( 35%)]  Loss:  0.469753 (0.5098)  Time: 0.541s,   36.95/s  (0.533s,   37.51/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1250/3456 ( 36%)]  Loss:  0.503781 (0.5087)  Time: 0.546s,   36.63/s  (0.534s,   37.48/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1300/3456 ( 38%)]  Loss:  0.495512 (0.5078)  Time: 0.545s,   36.73/s  (0.534s,   37.45/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1350/3456 ( 39%)]  Loss:  0.604665 (0.5075)  Time: 0.542s,   36.93/s  (0.535s,   37.42/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1400/3456 ( 41%)]  Loss:  0.494238 (0.5069)  Time: 0.527s,   37.95/s  (0.535s,   37.40/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1450/3456 ( 42%)]  Loss:  0.632108 (0.5067)  Time: 0.523s,   38.21/s  (0.534s,   37.42/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1500/3456 ( 43%)]  Loss:  0.527928 (0.5066)  Time: 0.524s,   38.15/s  (0.534s,   37.44/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1550/3456 ( 45%)]  Loss:  0.428952 (0.5065)  Time: 0.529s,   37.80/s  (0.534s,   37.46/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1600/3456 ( 46%)]  Loss:  0.466508 (0.5059)  Time: 0.524s,   38.17/s  (0.534s,   37.47/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1650/3456 ( 48%)]  Loss:  0.442302 (0.5054)  Time: 0.564s,   35.43/s  (0.534s,   37.48/s)  LR: 3.979e-02  Data: 0.015 (0.011)\n",
      "Train: 14 [1700/3456 ( 49%)]  Loss:  0.407942 (0.5053)  Time: 0.525s,   38.07/s  (0.534s,   37.48/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1750/3456 ( 51%)]  Loss:  0.504907 (0.5051)  Time: 0.524s,   38.13/s  (0.533s,   37.49/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1800/3456 ( 52%)]  Loss:  0.543346 (0.5048)  Time: 0.526s,   38.05/s  (0.533s,   37.50/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1850/3456 ( 54%)]  Loss:  0.563931 (0.5039)  Time: 0.524s,   38.14/s  (0.533s,   37.52/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1900/3456 ( 55%)]  Loss:  0.532978 (0.5037)  Time: 0.525s,   38.09/s  (0.533s,   37.53/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [1950/3456 ( 56%)]  Loss:  0.538786 (0.5035)  Time: 0.526s,   38.04/s  (0.533s,   37.54/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2000/3456 ( 58%)]  Loss:  0.438728 (0.5032)  Time: 0.687s,   29.09/s  (0.533s,   37.55/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2050/3456 ( 59%)]  Loss:  0.494125 (0.5031)  Time: 0.528s,   37.91/s  (0.533s,   37.56/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2100/3456 ( 61%)]  Loss:  0.522440 (0.5029)  Time: 0.547s,   36.54/s  (0.533s,   37.54/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2150/3456 ( 62%)]  Loss:  0.443829 (0.5025)  Time: 0.525s,   38.10/s  (0.533s,   37.55/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2200/3456 ( 64%)]  Loss:  0.505622 (0.5018)  Time: 0.527s,   37.92/s  (0.533s,   37.55/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2250/3456 ( 65%)]  Loss:  0.520070 (0.5012)  Time: 0.527s,   37.99/s  (0.532s,   37.56/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2300/3456 ( 67%)]  Loss:  0.467503 (0.5008)  Time: 0.527s,   37.94/s  (0.532s,   37.57/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2350/3456 ( 68%)]  Loss:  0.450674 (0.5001)  Time: 0.528s,   37.89/s  (0.532s,   37.58/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2400/3456 ( 69%)]  Loss:  0.452350 (0.5002)  Time: 0.542s,   36.88/s  (0.533s,   37.55/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2450/3456 ( 71%)]  Loss:  0.496852 (0.5002)  Time: 0.543s,   36.85/s  (0.533s,   37.53/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2500/3456 ( 72%)]  Loss:  0.441319 (0.4999)  Time: 0.548s,   36.50/s  (0.533s,   37.51/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2550/3456 ( 74%)]  Loss:  0.446804 (0.4996)  Time: 0.548s,   36.51/s  (0.533s,   37.50/s)  LR: 3.979e-02  Data: 0.012 (0.011)\n",
      "Train: 14 [2600/3456 ( 75%)]  Loss:  0.481649 (0.4994)  Time: 0.546s,   36.65/s  (0.534s,   37.48/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2650/3456 ( 77%)]  Loss:  0.535787 (0.4994)  Time: 0.524s,   38.19/s  (0.534s,   37.47/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2700/3456 ( 78%)]  Loss:  0.523599 (0.4994)  Time: 0.530s,   37.74/s  (0.534s,   37.48/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2750/3456 ( 80%)]  Loss:  0.480285 (0.4994)  Time: 0.528s,   37.85/s  (0.533s,   37.49/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2800/3456 ( 81%)]  Loss:  0.447315 (0.4991)  Time: 0.524s,   38.20/s  (0.533s,   37.50/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2850/3456 ( 82%)]  Loss:  0.579664 (0.4988)  Time: 0.525s,   38.11/s  (0.533s,   37.50/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2900/3456 ( 84%)]  Loss:  0.537363 (0.4988)  Time: 0.526s,   38.02/s  (0.533s,   37.51/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [2950/3456 ( 85%)]  Loss:  0.558166 (0.4987)  Time: 0.530s,   37.70/s  (0.533s,   37.52/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3000/3456 ( 87%)]  Loss:  0.477933 (0.4988)  Time: 0.524s,   38.15/s  (0.533s,   37.53/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3050/3456 ( 88%)]  Loss:  0.387136 (0.4985)  Time: 0.525s,   38.09/s  (0.533s,   37.54/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3100/3456 ( 90%)]  Loss:  0.532889 (0.4982)  Time: 0.526s,   38.03/s  (0.533s,   37.54/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3150/3456 ( 91%)]  Loss:  0.416703 (0.4980)  Time: 0.525s,   38.09/s  (0.533s,   37.55/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3200/3456 ( 93%)]  Loss:  0.475493 (0.4978)  Time: 0.527s,   37.98/s  (0.532s,   37.56/s)  LR: 3.979e-02  Data: 0.011 (0.011)\n",
      "Train: 14 [3250/3456 ( 94%)]  Loss:  0.795466 (0.5033)  Time: 0.963s,   20.76/s  (0.537s,   37.27/s)  LR: 3.979e-02  Data: 0.448 (0.015)\n",
      "Train: 14 [3300/3456 ( 96%)]  Loss:  1.179739 (0.5088)  Time: 0.897s,   22.30/s  (0.542s,   36.88/s)  LR: 3.979e-02  Data: 0.383 (0.021)\n",
      "Train: 14 [3350/3456 ( 97%)]  Loss:  0.991239 (0.5128)  Time: 0.912s,   21.93/s  (0.548s,   36.53/s)  LR: 3.979e-02  Data: 0.396 (0.026)\n",
      "Train: 14 [3400/3456 ( 98%)]  Loss:  1.037614 (0.5183)  Time: 0.897s,   22.29/s  (0.553s,   36.19/s)  LR: 3.979e-02  Data: 0.364 (0.031)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 14 [3450/3456 (100%)]  Loss:  0.300488 (0.5230)  Time: 0.905s,   22.09/s  (0.557s,   35.88/s)  LR: 3.979e-02  Data: 0.375 (0.036)\n",
      "Train: 14 [3455/3456 (100%)]  Loss:  1.228898 (0.5235)  Time: 0.464s,   23.72/s  (0.558s,   19.73/s)  LR: 3.979e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.876 (0.876)  Loss:  2.0846 (2.0846)  \n",
      "Test: [  50/147]  Time: 0.284 (0.293)  Loss:  2.6987 (2.3870)  \n",
      "Test: [ 100/147]  Time: 0.271 (0.283)  Loss:  2.4782 (2.3710)  \n",
      "Test: [ 147/147]  Time: 0.288 (0.327)  Loss:  1.0198 (2.2393)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-10.pth.tar', 2.7430243927079276)\n",
      "\n",
      "Train: 15 [   0/3456 (  0%)]  Loss:  2.364319 (2.3643)  Time: 1.440s,   13.89/s  (1.440s,   13.89/s)  LR: 3.975e-02  Data: 0.801 (0.801)\n",
      "Train: 15 [  50/3456 (  1%)]  Loss:  0.490330 (0.7161)  Time: 0.532s,   37.63/s  (0.544s,   36.73/s)  LR: 3.975e-02  Data: 0.011 (0.026)\n",
      "Train: 15 [ 100/3456 (  3%)]  Loss:  0.580954 (0.6289)  Time: 0.525s,   38.12/s  (0.536s,   37.34/s)  LR: 3.975e-02  Data: 0.011 (0.018)\n",
      "Train: 15 [ 150/3456 (  4%)]  Loss:  0.453077 (0.5896)  Time: 0.528s,   37.88/s  (0.532s,   37.56/s)  LR: 3.975e-02  Data: 0.011 (0.016)\n",
      "Train: 15 [ 200/3456 (  6%)]  Loss:  0.559776 (0.5683)  Time: 0.525s,   38.10/s  (0.531s,   37.66/s)  LR: 3.975e-02  Data: 0.011 (0.015)\n",
      "Train: 15 [ 250/3456 (  7%)]  Loss:  0.464286 (0.5576)  Time: 0.525s,   38.07/s  (0.530s,   37.73/s)  LR: 3.975e-02  Data: 0.011 (0.014)\n",
      "Train: 15 [ 300/3456 (  9%)]  Loss:  0.609396 (0.5500)  Time: 0.528s,   37.90/s  (0.529s,   37.78/s)  LR: 3.975e-02  Data: 0.011 (0.013)\n",
      "Train: 15 [ 350/3456 ( 10%)]  Loss:  0.475744 (0.5418)  Time: 0.528s,   37.84/s  (0.529s,   37.81/s)  LR: 3.975e-02  Data: 0.011 (0.013)\n",
      "Train: 15 [ 400/3456 ( 12%)]  Loss:  0.539255 (0.5353)  Time: 0.542s,   36.93/s  (0.529s,   37.83/s)  LR: 3.975e-02  Data: 0.011 (0.013)\n",
      "Train: 15 [ 450/3456 ( 13%)]  Loss:  0.572151 (0.5330)  Time: 0.547s,   36.60/s  (0.531s,   37.68/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 500/3456 ( 14%)]  Loss:  0.508025 (0.5292)  Time: 0.526s,   38.04/s  (0.531s,   37.65/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 550/3456 ( 16%)]  Loss:  0.480574 (0.5250)  Time: 0.525s,   38.11/s  (0.531s,   37.69/s)  LR: 3.975e-02  Data: 0.010 (0.012)\n",
      "Train: 15 [ 600/3456 ( 17%)]  Loss:  0.608389 (0.5222)  Time: 0.524s,   38.17/s  (0.531s,   37.69/s)  LR: 3.975e-02  Data: 0.010 (0.012)\n",
      "Train: 15 [ 650/3456 ( 19%)]  Loss:  0.603094 (0.5205)  Time: 0.526s,   37.99/s  (0.530s,   37.72/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 700/3456 ( 20%)]  Loss:  0.403244 (0.5178)  Time: 0.526s,   38.02/s  (0.530s,   37.74/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 750/3456 ( 22%)]  Loss:  0.660207 (0.5173)  Time: 0.549s,   36.41/s  (0.530s,   37.76/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 800/3456 ( 23%)]  Loss:  0.546435 (0.5154)  Time: 0.527s,   37.97/s  (0.530s,   37.77/s)  LR: 3.975e-02  Data: 0.010 (0.012)\n",
      "Train: 15 [ 850/3456 ( 25%)]  Loss:  0.535887 (0.5143)  Time: 0.524s,   38.15/s  (0.529s,   37.77/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 900/3456 ( 26%)]  Loss:  0.469282 (0.5130)  Time: 0.525s,   38.07/s  (0.529s,   37.79/s)  LR: 3.975e-02  Data: 0.011 (0.012)\n",
      "Train: 15 [ 950/3456 ( 27%)]  Loss:  0.532934 (0.5116)  Time: 0.526s,   38.02/s  (0.529s,   37.80/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1000/3456 ( 29%)]  Loss:  0.467606 (0.5109)  Time: 0.525s,   38.07/s  (0.529s,   37.81/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1050/3456 ( 30%)]  Loss:  0.520021 (0.5106)  Time: 0.543s,   36.85/s  (0.530s,   37.77/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1100/3456 ( 32%)]  Loss:  0.591134 (0.5099)  Time: 0.547s,   36.56/s  (0.530s,   37.72/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1150/3456 ( 33%)]  Loss:  0.462802 (0.5088)  Time: 0.541s,   36.94/s  (0.531s,   37.68/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1200/3456 ( 35%)]  Loss:  0.531341 (0.5078)  Time: 0.540s,   37.07/s  (0.531s,   37.64/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1250/3456 ( 36%)]  Loss:  0.449135 (0.5068)  Time: 0.547s,   36.55/s  (0.532s,   37.60/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1300/3456 ( 38%)]  Loss:  0.542820 (0.5057)  Time: 0.540s,   37.04/s  (0.532s,   37.57/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1350/3456 ( 39%)]  Loss:  0.538769 (0.5052)  Time: 0.547s,   36.59/s  (0.533s,   37.54/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1400/3456 ( 41%)]  Loss:  0.469309 (0.5047)  Time: 0.545s,   36.73/s  (0.533s,   37.51/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1450/3456 ( 42%)]  Loss:  0.663577 (0.5045)  Time: 0.525s,   38.12/s  (0.533s,   37.52/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1500/3456 ( 43%)]  Loss:  0.490478 (0.5044)  Time: 0.542s,   36.92/s  (0.533s,   37.50/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1550/3456 ( 45%)]  Loss:  0.383618 (0.5043)  Time: 0.546s,   36.62/s  (0.534s,   37.48/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1600/3456 ( 46%)]  Loss:  0.453631 (0.5037)  Time: 0.546s,   36.64/s  (0.534s,   37.45/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1650/3456 ( 48%)]  Loss:  0.414255 (0.5029)  Time: 0.541s,   36.97/s  (0.534s,   37.43/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1700/3456 ( 49%)]  Loss:  0.352110 (0.5026)  Time: 0.545s,   36.71/s  (0.535s,   37.41/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1750/3456 ( 51%)]  Loss:  0.494107 (0.5024)  Time: 0.544s,   36.75/s  (0.535s,   37.39/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1800/3456 ( 52%)]  Loss:  0.482203 (0.5023)  Time: 0.541s,   36.94/s  (0.535s,   37.37/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1850/3456 ( 54%)]  Loss:  0.531216 (0.5016)  Time: 0.547s,   36.57/s  (0.535s,   37.36/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1900/3456 ( 55%)]  Loss:  0.495009 (0.5011)  Time: 0.545s,   36.67/s  (0.536s,   37.33/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [1950/3456 ( 56%)]  Loss:  0.535807 (0.5008)  Time: 0.548s,   36.51/s  (0.536s,   37.32/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2000/3456 ( 58%)]  Loss:  0.520428 (0.5003)  Time: 0.547s,   36.53/s  (0.536s,   37.31/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2050/3456 ( 59%)]  Loss:  0.477007 (0.5001)  Time: 0.548s,   36.50/s  (0.536s,   37.29/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2100/3456 ( 61%)]  Loss:  0.515280 (0.5000)  Time: 0.542s,   36.88/s  (0.537s,   37.28/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2150/3456 ( 62%)]  Loss:  0.389770 (0.4998)  Time: 0.542s,   36.93/s  (0.537s,   37.27/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2200/3456 ( 64%)]  Loss:  0.465565 (0.4992)  Time: 0.549s,   36.43/s  (0.537s,   37.25/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2250/3456 ( 65%)]  Loss:  0.493846 (0.4988)  Time: 0.547s,   36.57/s  (0.537s,   37.24/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2300/3456 ( 67%)]  Loss:  0.506645 (0.4984)  Time: 0.525s,   38.11/s  (0.537s,   37.25/s)  LR: 3.975e-02  Data: 0.010 (0.011)\n",
      "Train: 15 [2350/3456 ( 68%)]  Loss:  0.461594 (0.4982)  Time: 0.526s,   38.01/s  (0.537s,   37.26/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2400/3456 ( 69%)]  Loss:  0.577649 (0.4983)  Time: 0.526s,   38.00/s  (0.536s,   37.28/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2450/3456 ( 71%)]  Loss:  0.574296 (0.4985)  Time: 0.526s,   38.02/s  (0.536s,   37.29/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2500/3456 ( 72%)]  Loss:  0.395828 (0.4983)  Time: 0.525s,   38.08/s  (0.536s,   37.31/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2550/3456 ( 74%)]  Loss:  0.462478 (0.4981)  Time: 0.529s,   37.84/s  (0.536s,   37.32/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2600/3456 ( 75%)]  Loss:  0.474225 (0.4978)  Time: 0.524s,   38.17/s  (0.536s,   37.33/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2650/3456 ( 77%)]  Loss:  0.613348 (0.4978)  Time: 0.526s,   38.05/s  (0.536s,   37.34/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2700/3456 ( 78%)]  Loss:  0.466924 (0.4977)  Time: 0.523s,   38.23/s  (0.535s,   37.35/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2750/3456 ( 80%)]  Loss:  0.576093 (0.4976)  Time: 0.526s,   38.04/s  (0.535s,   37.37/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2800/3456 ( 81%)]  Loss:  0.405732 (0.4974)  Time: 0.542s,   36.93/s  (0.535s,   37.36/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2850/3456 ( 82%)]  Loss:  0.464501 (0.4971)  Time: 0.544s,   36.75/s  (0.536s,   37.35/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2900/3456 ( 84%)]  Loss:  0.617060 (0.4974)  Time: 0.546s,   36.60/s  (0.536s,   37.34/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [2950/3456 ( 85%)]  Loss:  0.512239 (0.4971)  Time: 0.526s,   38.06/s  (0.536s,   37.35/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3000/3456 ( 87%)]  Loss:  0.441549 (0.4971)  Time: 0.542s,   36.93/s  (0.535s,   37.35/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3050/3456 ( 88%)]  Loss:  0.392827 (0.4967)  Time: 0.544s,   36.77/s  (0.536s,   37.34/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3100/3456 ( 90%)]  Loss:  0.501056 (0.4966)  Time: 0.542s,   36.88/s  (0.536s,   37.33/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3150/3456 ( 91%)]  Loss:  0.534707 (0.4964)  Time: 0.541s,   37.00/s  (0.536s,   37.32/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3200/3456 ( 93%)]  Loss:  0.447888 (0.4962)  Time: 0.543s,   36.84/s  (0.536s,   37.32/s)  LR: 3.975e-02  Data: 0.011 (0.011)\n",
      "Train: 15 [3250/3456 ( 94%)]  Loss:  1.147288 (0.5023)  Time: 0.923s,   21.66/s  (0.540s,   37.04/s)  LR: 3.975e-02  Data: 0.395 (0.015)\n",
      "Train: 15 [3300/3456 ( 96%)]  Loss:  1.124315 (0.5082)  Time: 0.881s,   22.71/s  (0.546s,   36.66/s)  LR: 3.975e-02  Data: 0.350 (0.020)\n",
      "Train: 15 [3350/3456 ( 97%)]  Loss:  0.947057 (0.5116)  Time: 0.880s,   22.73/s  (0.551s,   36.32/s)  LR: 3.975e-02  Data: 0.349 (0.025)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 15 [3400/3456 ( 98%)]  Loss:  1.126088 (0.5167)  Time: 0.859s,   23.28/s  (0.556s,   35.99/s)  LR: 3.975e-02  Data: 0.322 (0.030)\n",
      "Train: 15 [3450/3456 (100%)]  Loss:  0.854162 (0.5198)  Time: 0.896s,   22.32/s  (0.560s,   35.68/s)  LR: 3.975e-02  Data: 0.361 (0.035)\n",
      "Train: 15 [3455/3456 (100%)]  Loss:  0.807509 (0.5202)  Time: 0.464s,   23.70/s  (0.561s,   19.62/s)  LR: 3.975e-02  Data: 0.038 (0.035)\n",
      "Test: [   0/147]  Time: 0.879 (0.879)  Loss:  2.8134 (2.8134)  \n",
      "Test: [  50/147]  Time: 0.265 (0.279)  Loss:  3.1029 (2.9226)  \n",
      "Test: [ 100/147]  Time: 0.274 (0.275)  Loss:  2.9560 (2.9099)  \n",
      "Test: [ 147/147]  Time: 0.285 (0.321)  Loss:  1.3738 (2.7636)  \n",
      "Train: 16 [   0/3456 (  0%)]  Loss:  2.493759 (2.4938)  Time: 1.460s,   13.70/s  (1.460s,   13.70/s)  LR: 3.972e-02  Data: 0.859 (0.859)\n",
      "Train: 16 [  50/3456 (  1%)]  Loss:  0.568961 (0.8813)  Time: 0.524s,   38.20/s  (0.543s,   36.80/s)  LR: 3.972e-02  Data: 0.011 (0.027)\n",
      "Train: 16 [ 100/3456 (  3%)]  Loss:  0.555347 (0.7193)  Time: 0.524s,   38.20/s  (0.535s,   37.40/s)  LR: 3.972e-02  Data: 0.011 (0.019)\n",
      "Train: 16 [ 150/3456 (  4%)]  Loss:  0.481991 (0.6520)  Time: 0.528s,   37.84/s  (0.532s,   37.59/s)  LR: 3.972e-02  Data: 0.011 (0.016)\n",
      "Train: 16 [ 200/3456 (  6%)]  Loss:  0.537587 (0.6168)  Time: 0.523s,   38.21/s  (0.530s,   37.70/s)  LR: 3.972e-02  Data: 0.011 (0.015)\n",
      "Train: 16 [ 250/3456 (  7%)]  Loss:  0.542662 (0.5964)  Time: 0.531s,   37.64/s  (0.530s,   37.77/s)  LR: 3.972e-02  Data: 0.011 (0.014)\n",
      "Train: 16 [ 300/3456 (  9%)]  Loss:  0.559038 (0.5823)  Time: 0.529s,   37.82/s  (0.529s,   37.79/s)  LR: 3.972e-02  Data: 0.011 (0.013)\n",
      "Train: 16 [ 350/3456 ( 10%)]  Loss:  0.536669 (0.5710)  Time: 0.523s,   38.22/s  (0.529s,   37.82/s)  LR: 3.972e-02  Data: 0.010 (0.013)\n",
      "Train: 16 [ 400/3456 ( 12%)]  Loss:  0.436767 (0.5624)  Time: 0.526s,   38.01/s  (0.528s,   37.85/s)  LR: 3.972e-02  Data: 0.010 (0.013)\n",
      "Train: 16 [ 450/3456 ( 13%)]  Loss:  0.452995 (0.5580)  Time: 0.525s,   38.08/s  (0.528s,   37.86/s)  LR: 3.972e-02  Data: 0.010 (0.012)\n",
      "Train: 16 [ 500/3456 ( 14%)]  Loss:  0.491035 (0.5529)  Time: 0.527s,   37.92/s  (0.528s,   37.87/s)  LR: 3.972e-02  Data: 0.010 (0.012)\n",
      "Train: 16 [ 550/3456 ( 16%)]  Loss:  0.465650 (0.5477)  Time: 0.529s,   37.82/s  (0.528s,   37.88/s)  LR: 3.972e-02  Data: 0.010 (0.012)\n",
      "Train: 16 [ 600/3456 ( 17%)]  Loss:  0.522547 (0.5438)  Time: 0.528s,   37.88/s  (0.528s,   37.88/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 650/3456 ( 19%)]  Loss:  0.561507 (0.5409)  Time: 0.525s,   38.11/s  (0.528s,   37.88/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 700/3456 ( 20%)]  Loss:  0.387955 (0.5372)  Time: 0.525s,   38.07/s  (0.528s,   37.89/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 750/3456 ( 22%)]  Loss:  0.528170 (0.5341)  Time: 0.528s,   37.88/s  (0.528s,   37.89/s)  LR: 3.972e-02  Data: 0.010 (0.012)\n",
      "Train: 16 [ 800/3456 ( 23%)]  Loss:  0.514391 (0.5316)  Time: 0.525s,   38.10/s  (0.528s,   37.90/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 850/3456 ( 25%)]  Loss:  0.518494 (0.5293)  Time: 0.525s,   38.13/s  (0.528s,   37.91/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 900/3456 ( 26%)]  Loss:  0.540144 (0.5280)  Time: 0.687s,   29.12/s  (0.528s,   37.91/s)  LR: 3.972e-02  Data: 0.011 (0.012)\n",
      "Train: 16 [ 950/3456 ( 27%)]  Loss:  0.529490 (0.5264)  Time: 0.525s,   38.10/s  (0.527s,   37.92/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1000/3456 ( 29%)]  Loss:  0.495590 (0.5248)  Time: 0.529s,   37.79/s  (0.527s,   37.93/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1050/3456 ( 30%)]  Loss:  0.459484 (0.5240)  Time: 0.543s,   36.86/s  (0.528s,   37.89/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1100/3456 ( 32%)]  Loss:  0.417798 (0.5228)  Time: 0.544s,   36.75/s  (0.529s,   37.84/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1150/3456 ( 33%)]  Loss:  0.404112 (0.5210)  Time: 0.546s,   36.62/s  (0.529s,   37.79/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1200/3456 ( 35%)]  Loss:  0.554189 (0.5199)  Time: 0.547s,   36.57/s  (0.530s,   37.75/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1250/3456 ( 36%)]  Loss:  0.553432 (0.5195)  Time: 0.525s,   38.09/s  (0.530s,   37.75/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1300/3456 ( 38%)]  Loss:  0.508588 (0.5181)  Time: 0.542s,   36.88/s  (0.530s,   37.71/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1350/3456 ( 39%)]  Loss:  0.556886 (0.5171)  Time: 0.543s,   36.82/s  (0.531s,   37.66/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1400/3456 ( 41%)]  Loss:  0.456338 (0.5158)  Time: 0.527s,   37.96/s  (0.531s,   37.65/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1450/3456 ( 42%)]  Loss:  0.504272 (0.5146)  Time: 0.524s,   38.19/s  (0.532s,   37.63/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1500/3456 ( 43%)]  Loss:  0.499290 (0.5136)  Time: 0.526s,   38.04/s  (0.531s,   37.63/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1550/3456 ( 45%)]  Loss:  0.469531 (0.5135)  Time: 0.528s,   37.86/s  (0.531s,   37.64/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1600/3456 ( 46%)]  Loss:  0.504865 (0.5126)  Time: 0.525s,   38.09/s  (0.531s,   37.65/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1650/3456 ( 48%)]  Loss:  0.394417 (0.5122)  Time: 0.524s,   38.17/s  (0.531s,   37.65/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1700/3456 ( 49%)]  Loss:  0.443028 (0.5114)  Time: 0.523s,   38.27/s  (0.531s,   37.67/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1750/3456 ( 51%)]  Loss:  0.450023 (0.5111)  Time: 0.529s,   37.79/s  (0.531s,   37.67/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1800/3456 ( 52%)]  Loss:  0.534585 (0.5108)  Time: 0.553s,   36.15/s  (0.531s,   37.67/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1850/3456 ( 54%)]  Loss:  0.540791 (0.5096)  Time: 0.547s,   36.55/s  (0.531s,   37.64/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1900/3456 ( 55%)]  Loss:  0.615351 (0.5094)  Time: 0.524s,   38.15/s  (0.531s,   37.65/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [1950/3456 ( 56%)]  Loss:  0.539553 (0.5086)  Time: 0.525s,   38.12/s  (0.531s,   37.66/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2000/3456 ( 58%)]  Loss:  0.447539 (0.5084)  Time: 0.525s,   38.09/s  (0.531s,   37.67/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2050/3456 ( 59%)]  Loss:  0.456063 (0.5084)  Time: 0.524s,   38.19/s  (0.531s,   37.68/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2100/3456 ( 61%)]  Loss:  0.471372 (0.5082)  Time: 0.542s,   36.92/s  (0.531s,   37.66/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2150/3456 ( 62%)]  Loss:  0.478269 (0.5078)  Time: 0.542s,   36.91/s  (0.531s,   37.64/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2200/3456 ( 64%)]  Loss:  0.546982 (0.5070)  Time: 0.541s,   36.95/s  (0.532s,   37.61/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2250/3456 ( 65%)]  Loss:  0.498803 (0.5063)  Time: 0.541s,   36.95/s  (0.532s,   37.59/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2300/3456 ( 67%)]  Loss:  0.447708 (0.5055)  Time: 0.544s,   36.78/s  (0.532s,   37.57/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2350/3456 ( 68%)]  Loss:  0.389317 (0.5049)  Time: 0.550s,   36.39/s  (0.533s,   37.55/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2400/3456 ( 69%)]  Loss:  0.504347 (0.5047)  Time: 0.546s,   36.63/s  (0.533s,   37.53/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2450/3456 ( 71%)]  Loss:  0.439326 (0.5043)  Time: 0.542s,   36.87/s  (0.533s,   37.51/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2500/3456 ( 72%)]  Loss:  0.367087 (0.5041)  Time: 0.547s,   36.54/s  (0.533s,   37.49/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2550/3456 ( 74%)]  Loss:  0.473735 (0.5037)  Time: 0.528s,   37.87/s  (0.533s,   37.49/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2600/3456 ( 75%)]  Loss:  0.448499 (0.5034)  Time: 0.528s,   37.85/s  (0.533s,   37.50/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2650/3456 ( 77%)]  Loss:  0.568877 (0.5031)  Time: 0.529s,   37.79/s  (0.533s,   37.51/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2700/3456 ( 78%)]  Loss:  0.455674 (0.5028)  Time: 0.527s,   37.98/s  (0.533s,   37.52/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2750/3456 ( 80%)]  Loss:  0.515520 (0.5025)  Time: 0.525s,   38.12/s  (0.533s,   37.52/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2800/3456 ( 81%)]  Loss:  0.391954 (0.5021)  Time: 0.530s,   37.75/s  (0.533s,   37.53/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2850/3456 ( 82%)]  Loss:  0.496439 (0.5017)  Time: 0.530s,   37.76/s  (0.533s,   37.53/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2900/3456 ( 84%)]  Loss:  0.511638 (0.5014)  Time: 0.531s,   37.69/s  (0.533s,   37.54/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [2950/3456 ( 85%)]  Loss:  0.547995 (0.5011)  Time: 0.524s,   38.20/s  (0.533s,   37.54/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3000/3456 ( 87%)]  Loss:  0.435468 (0.5009)  Time: 0.533s,   37.55/s  (0.533s,   37.55/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3050/3456 ( 88%)]  Loss:  0.382745 (0.5005)  Time: 0.528s,   37.90/s  (0.533s,   37.55/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3100/3456 ( 90%)]  Loss:  0.524691 (0.5001)  Time: 0.524s,   38.18/s  (0.533s,   37.56/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3150/3456 ( 91%)]  Loss:  0.425770 (0.4998)  Time: 0.530s,   37.74/s  (0.532s,   37.56/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3200/3456 ( 93%)]  Loss:  0.511975 (0.4994)  Time: 0.530s,   37.71/s  (0.532s,   37.57/s)  LR: 3.972e-02  Data: 0.011 (0.011)\n",
      "Train: 16 [3250/3456 ( 94%)]  Loss:  1.246632 (0.5050)  Time: 0.961s,   20.81/s  (0.537s,   37.27/s)  LR: 3.972e-02  Data: 0.442 (0.015)\n",
      "Train: 16 [3300/3456 ( 96%)]  Loss:  1.320186 (0.5112)  Time: 0.906s,   22.08/s  (0.542s,   36.87/s)  LR: 3.972e-02  Data: 0.393 (0.021)\n",
      "Train: 16 [3350/3456 ( 97%)]  Loss:  0.814625 (0.5152)  Time: 0.870s,   23.00/s  (0.548s,   36.52/s)  LR: 3.972e-02  Data: 0.354 (0.027)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 16 [3400/3456 ( 98%)]  Loss:  1.242395 (0.5215)  Time: 0.882s,   22.67/s  (0.553s,   36.18/s)  LR: 3.972e-02  Data: 0.366 (0.032)\n",
      "Train: 16 [3450/3456 (100%)]  Loss:  0.699516 (0.5243)  Time: 0.880s,   22.72/s  (0.558s,   35.86/s)  LR: 3.972e-02  Data: 0.347 (0.037)\n",
      "Train: 16 [3455/3456 (100%)]  Loss:  0.928231 (0.5245)  Time: 0.466s,   23.62/s  (0.558s,   19.71/s)  LR: 3.972e-02  Data: 0.039 (0.037)\n",
      "Test: [   0/147]  Time: 0.877 (0.877)  Loss:  2.3168 (2.3168)  \n",
      "Test: [  50/147]  Time: 0.283 (0.294)  Loss:  2.8489 (2.6512)  \n",
      "Test: [ 100/147]  Time: 0.281 (0.289)  Loss:  2.5992 (2.6493)  \n",
      "Test: [ 147/147]  Time: 0.302 (0.333)  Loss:  2.3865 (2.6084)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-16.pth.tar', 2.6083568930625916)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-6.pth.tar', 2.731274248377697)\n",
      "\n",
      "Train: 17 [   0/3456 (  0%)]  Loss:  2.814766 (2.8148)  Time: 1.416s,   14.13/s  (1.416s,   14.13/s)  LR: 3.968e-02  Data: 0.773 (0.773)\n",
      "Train: 17 [  50/3456 (  1%)]  Loss:  0.458368 (0.8184)  Time: 0.545s,   36.73/s  (0.563s,   35.50/s)  LR: 3.968e-02  Data: 0.012 (0.027)\n",
      "Train: 17 [ 100/3456 (  3%)]  Loss:  0.620049 (0.6817)  Time: 0.544s,   36.74/s  (0.555s,   36.05/s)  LR: 3.968e-02  Data: 0.011 (0.019)\n",
      "Train: 17 [ 150/3456 (  4%)]  Loss:  0.524619 (0.6235)  Time: 0.543s,   36.85/s  (0.553s,   36.16/s)  LR: 3.968e-02  Data: 0.011 (0.016)\n",
      "Train: 17 [ 200/3456 (  6%)]  Loss:  0.548150 (0.5939)  Time: 0.548s,   36.52/s  (0.552s,   36.26/s)  LR: 3.968e-02  Data: 0.011 (0.015)\n",
      "Train: 17 [ 250/3456 (  7%)]  Loss:  0.441259 (0.5760)  Time: 0.545s,   36.69/s  (0.550s,   36.35/s)  LR: 3.968e-02  Data: 0.011 (0.014)\n",
      "Train: 17 [ 300/3456 (  9%)]  Loss:  0.633024 (0.5632)  Time: 0.548s,   36.53/s  (0.550s,   36.39/s)  LR: 3.968e-02  Data: 0.011 (0.014)\n",
      "Train: 17 [ 350/3456 ( 10%)]  Loss:  0.494612 (0.5538)  Time: 0.545s,   36.68/s  (0.549s,   36.43/s)  LR: 3.968e-02  Data: 0.011 (0.013)\n",
      "Train: 17 [ 400/3456 ( 12%)]  Loss:  0.448248 (0.5453)  Time: 0.551s,   36.28/s  (0.549s,   36.44/s)  LR: 3.968e-02  Data: 0.011 (0.013)\n",
      "Train: 17 [ 450/3456 ( 13%)]  Loss:  0.462578 (0.5409)  Time: 0.550s,   36.35/s  (0.549s,   36.45/s)  LR: 3.968e-02  Data: 0.011 (0.013)\n",
      "Train: 17 [ 500/3456 ( 14%)]  Loss:  0.455580 (0.5365)  Time: 0.526s,   38.01/s  (0.548s,   36.50/s)  LR: 3.968e-02  Data: 0.011 (0.013)\n",
      "Train: 17 [ 550/3456 ( 16%)]  Loss:  0.461809 (0.5309)  Time: 0.525s,   38.10/s  (0.546s,   36.60/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 600/3456 ( 17%)]  Loss:  0.477004 (0.5281)  Time: 0.544s,   36.76/s  (0.546s,   36.66/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 650/3456 ( 19%)]  Loss:  0.640077 (0.5259)  Time: 0.543s,   36.81/s  (0.546s,   36.66/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 700/3456 ( 20%)]  Loss:  0.410756 (0.5238)  Time: 0.548s,   36.48/s  (0.545s,   36.67/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 750/3456 ( 22%)]  Loss:  0.624642 (0.5217)  Time: 0.547s,   36.56/s  (0.545s,   36.67/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 800/3456 ( 23%)]  Loss:  0.631594 (0.5205)  Time: 0.527s,   37.98/s  (0.545s,   36.73/s)  LR: 3.968e-02  Data: 0.010 (0.012)\n",
      "Train: 17 [ 850/3456 ( 25%)]  Loss:  0.453334 (0.5191)  Time: 0.526s,   38.06/s  (0.544s,   36.79/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 900/3456 ( 26%)]  Loss:  0.540460 (0.5184)  Time: 0.530s,   37.76/s  (0.543s,   36.86/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [ 950/3456 ( 27%)]  Loss:  0.612783 (0.5171)  Time: 0.526s,   37.99/s  (0.542s,   36.91/s)  LR: 3.968e-02  Data: 0.010 (0.012)\n",
      "Train: 17 [1000/3456 ( 29%)]  Loss:  0.451610 (0.5160)  Time: 0.526s,   37.99/s  (0.541s,   36.96/s)  LR: 3.968e-02  Data: 0.010 (0.012)\n",
      "Train: 17 [1050/3456 ( 30%)]  Loss:  0.461225 (0.5143)  Time: 0.525s,   38.13/s  (0.540s,   37.01/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [1100/3456 ( 32%)]  Loss:  0.568487 (0.5139)  Time: 0.526s,   38.02/s  (0.540s,   37.05/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [1150/3456 ( 33%)]  Loss:  0.509334 (0.5126)  Time: 0.525s,   38.11/s  (0.539s,   37.09/s)  LR: 3.968e-02  Data: 0.011 (0.012)\n",
      "Train: 17 [1200/3456 ( 35%)]  Loss:  0.534338 (0.5112)  Time: 0.525s,   38.12/s  (0.539s,   37.13/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1250/3456 ( 36%)]  Loss:  0.514975 (0.5101)  Time: 0.541s,   36.97/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1300/3456 ( 38%)]  Loss:  0.562255 (0.5090)  Time: 0.546s,   36.64/s  (0.539s,   37.14/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1350/3456 ( 39%)]  Loss:  0.552813 (0.5079)  Time: 0.546s,   36.62/s  (0.539s,   37.11/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1400/3456 ( 41%)]  Loss:  0.517190 (0.5070)  Time: 0.547s,   36.56/s  (0.539s,   37.09/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1450/3456 ( 42%)]  Loss:  0.504583 (0.5061)  Time: 0.541s,   36.94/s  (0.539s,   37.08/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1500/3456 ( 43%)]  Loss:  0.513564 (0.5058)  Time: 0.549s,   36.46/s  (0.540s,   37.07/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1550/3456 ( 45%)]  Loss:  0.430708 (0.5054)  Time: 0.544s,   36.76/s  (0.540s,   37.06/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1600/3456 ( 46%)]  Loss:  0.485683 (0.5045)  Time: 0.544s,   36.78/s  (0.540s,   37.04/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1650/3456 ( 48%)]  Loss:  0.408473 (0.5038)  Time: 0.543s,   36.84/s  (0.540s,   37.04/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1700/3456 ( 49%)]  Loss:  0.330741 (0.5036)  Time: 0.526s,   38.02/s  (0.540s,   37.04/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1750/3456 ( 51%)]  Loss:  0.503691 (0.5031)  Time: 0.527s,   37.94/s  (0.540s,   37.06/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1800/3456 ( 52%)]  Loss:  0.523331 (0.5033)  Time: 0.528s,   37.85/s  (0.539s,   37.08/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1850/3456 ( 54%)]  Loss:  0.465403 (0.5022)  Time: 0.534s,   37.44/s  (0.539s,   37.09/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1900/3456 ( 55%)]  Loss:  0.506608 (0.5018)  Time: 0.528s,   37.86/s  (0.539s,   37.11/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [1950/3456 ( 56%)]  Loss:  0.500948 (0.5015)  Time: 0.529s,   37.79/s  (0.539s,   37.13/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2000/3456 ( 58%)]  Loss:  0.548824 (0.5014)  Time: 0.531s,   37.68/s  (0.538s,   37.14/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2050/3456 ( 59%)]  Loss:  0.538292 (0.5015)  Time: 0.530s,   37.76/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2100/3456 ( 61%)]  Loss:  0.582512 (0.5015)  Time: 0.528s,   37.87/s  (0.538s,   37.17/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2150/3456 ( 62%)]  Loss:  0.526708 (0.5013)  Time: 0.529s,   37.80/s  (0.538s,   37.18/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2200/3456 ( 64%)]  Loss:  0.398087 (0.5005)  Time: 0.532s,   37.57/s  (0.538s,   37.19/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2250/3456 ( 65%)]  Loss:  0.525094 (0.5001)  Time: 0.532s,   37.59/s  (0.538s,   37.20/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2300/3456 ( 67%)]  Loss:  0.441579 (0.4997)  Time: 0.546s,   36.64/s  (0.538s,   37.20/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2350/3456 ( 68%)]  Loss:  0.507445 (0.4995)  Time: 0.551s,   36.29/s  (0.538s,   37.19/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2400/3456 ( 69%)]  Loss:  0.530559 (0.4997)  Time: 0.545s,   36.71/s  (0.538s,   37.17/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2450/3456 ( 71%)]  Loss:  0.450456 (0.4994)  Time: 0.544s,   36.77/s  (0.538s,   37.16/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2500/3456 ( 72%)]  Loss:  0.409652 (0.4992)  Time: 0.551s,   36.32/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2550/3456 ( 74%)]  Loss:  0.548921 (0.4988)  Time: 0.547s,   36.53/s  (0.539s,   37.14/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2600/3456 ( 75%)]  Loss:  0.448374 (0.4988)  Time: 0.526s,   38.00/s  (0.538s,   37.14/s)  LR: 3.968e-02  Data: 0.010 (0.011)\n",
      "Train: 17 [2650/3456 ( 77%)]  Loss:  0.590267 (0.4986)  Time: 0.528s,   37.86/s  (0.538s,   37.16/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2700/3456 ( 78%)]  Loss:  0.457131 (0.4984)  Time: 0.526s,   38.03/s  (0.538s,   37.17/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2750/3456 ( 80%)]  Loss:  0.528183 (0.4983)  Time: 0.527s,   37.95/s  (0.538s,   37.18/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2800/3456 ( 81%)]  Loss:  0.378635 (0.4979)  Time: 0.526s,   38.05/s  (0.538s,   37.19/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2850/3456 ( 82%)]  Loss:  0.583136 (0.4978)  Time: 0.544s,   36.74/s  (0.538s,   37.19/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2900/3456 ( 84%)]  Loss:  0.525526 (0.4975)  Time: 0.551s,   36.29/s  (0.538s,   37.17/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [2950/3456 ( 85%)]  Loss:  0.632657 (0.4974)  Time: 0.545s,   36.73/s  (0.538s,   37.16/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3000/3456 ( 87%)]  Loss:  0.451847 (0.4973)  Time: 0.547s,   36.57/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3050/3456 ( 88%)]  Loss:  0.412717 (0.4971)  Time: 0.548s,   36.48/s  (0.539s,   37.14/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3100/3456 ( 90%)]  Loss:  0.494455 (0.4968)  Time: 0.525s,   38.08/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3150/3456 ( 91%)]  Loss:  0.439662 (0.4965)  Time: 0.526s,   38.05/s  (0.538s,   37.15/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3200/3456 ( 93%)]  Loss:  0.465935 (0.4963)  Time: 0.532s,   37.62/s  (0.538s,   37.16/s)  LR: 3.968e-02  Data: 0.011 (0.011)\n",
      "Train: 17 [3250/3456 ( 94%)]  Loss:  1.187896 (0.5022)  Time: 0.976s,   20.49/s  (0.542s,   36.88/s)  LR: 3.968e-02  Data: 0.442 (0.015)\n",
      "Train: 17 [3300/3456 ( 96%)]  Loss:  0.985566 (0.5077)  Time: 0.906s,   22.06/s  (0.548s,   36.49/s)  LR: 3.968e-02  Data: 0.372 (0.021)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 17 [3350/3456 ( 97%)]  Loss:  1.034692 (0.5136)  Time: 0.874s,   22.89/s  (0.553s,   36.13/s)  LR: 3.968e-02  Data: 0.337 (0.026)\n",
      "Train: 17 [3400/3456 ( 98%)]  Loss:  0.872980 (0.5171)  Time: 0.892s,   22.41/s  (0.559s,   35.80/s)  LR: 3.968e-02  Data: 0.359 (0.031)\n",
      "Train: 17 [3450/3456 (100%)]  Loss:  0.741120 (0.5200)  Time: 0.907s,   22.05/s  (0.563s,   35.50/s)  LR: 3.968e-02  Data: 0.383 (0.036)\n",
      "Train: 17 [3455/3456 (100%)]  Loss:  1.004684 (0.5204)  Time: 0.464s,   23.70/s  (0.564s,   19.51/s)  LR: 3.968e-02  Data: 0.041 (0.036)\n",
      "Test: [   0/147]  Time: 0.880 (0.880)  Loss:  2.2162 (2.2162)  \n",
      "Test: [  50/147]  Time: 0.264 (0.277)  Loss:  2.7937 (2.4988)  \n",
      "Test: [ 100/147]  Time: 0.278 (0.277)  Loss:  2.5628 (2.4904)  \n",
      "Test: [ 147/147]  Time: 0.302 (0.327)  Loss:  0.6195 (2.3276)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-16.pth.tar', 2.6083568930625916)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-9.pth.tar', 2.6275179756654277)\n",
      "\n",
      "Train: 18 [   0/3456 (  0%)]  Loss:  2.592927 (2.5929)  Time: 1.500s,   13.33/s  (1.500s,   13.33/s)  LR: 3.965e-02  Data: 0.878 (0.878)\n",
      "Train: 18 [  50/3456 (  1%)]  Loss:  0.388937 (0.7801)  Time: 0.545s,   36.73/s  (0.565s,   35.41/s)  LR: 3.965e-02  Data: 0.011 (0.028)\n",
      "Train: 18 [ 100/3456 (  3%)]  Loss:  0.622276 (0.6576)  Time: 0.549s,   36.45/s  (0.555s,   36.05/s)  LR: 3.965e-02  Data: 0.011 (0.020)\n",
      "Train: 18 [ 150/3456 (  4%)]  Loss:  0.536991 (0.6083)  Time: 0.541s,   36.94/s  (0.551s,   36.28/s)  LR: 3.965e-02  Data: 0.011 (0.017)\n",
      "Train: 18 [ 200/3456 (  6%)]  Loss:  0.519266 (0.5823)  Time: 0.542s,   36.89/s  (0.549s,   36.40/s)  LR: 3.965e-02  Data: 0.011 (0.015)\n",
      "Train: 18 [ 250/3456 (  7%)]  Loss:  0.459631 (0.5665)  Time: 0.525s,   38.09/s  (0.548s,   36.49/s)  LR: 3.965e-02  Data: 0.011 (0.014)\n",
      "Train: 18 [ 300/3456 (  9%)]  Loss:  0.538221 (0.5575)  Time: 0.526s,   38.01/s  (0.545s,   36.69/s)  LR: 3.965e-02  Data: 0.011 (0.014)\n",
      "Train: 18 [ 350/3456 ( 10%)]  Loss:  0.477238 (0.5487)  Time: 0.527s,   37.96/s  (0.542s,   36.87/s)  LR: 3.965e-02  Data: 0.011 (0.013)\n",
      "Train: 18 [ 400/3456 ( 12%)]  Loss:  0.449652 (0.5423)  Time: 0.527s,   37.97/s  (0.541s,   36.98/s)  LR: 3.965e-02  Data: 0.011 (0.013)\n",
      "Train: 18 [ 450/3456 ( 13%)]  Loss:  0.515680 (0.5388)  Time: 0.527s,   37.97/s  (0.540s,   37.07/s)  LR: 3.965e-02  Data: 0.011 (0.013)\n",
      "Train: 18 [ 500/3456 ( 14%)]  Loss:  0.491200 (0.5346)  Time: 0.528s,   37.85/s  (0.538s,   37.15/s)  LR: 3.965e-02  Data: 0.011 (0.013)\n",
      "Train: 18 [ 550/3456 ( 16%)]  Loss:  0.505204 (0.5297)  Time: 0.527s,   37.98/s  (0.538s,   37.21/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 600/3456 ( 17%)]  Loss:  0.506941 (0.5257)  Time: 0.524s,   38.16/s  (0.537s,   37.26/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 650/3456 ( 19%)]  Loss:  0.517204 (0.5238)  Time: 0.526s,   38.02/s  (0.536s,   37.31/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 700/3456 ( 20%)]  Loss:  0.402095 (0.5208)  Time: 0.527s,   37.98/s  (0.536s,   37.34/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 750/3456 ( 22%)]  Loss:  0.499961 (0.5190)  Time: 0.525s,   38.06/s  (0.535s,   37.37/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 800/3456 ( 23%)]  Loss:  0.555481 (0.5177)  Time: 0.550s,   36.39/s  (0.536s,   37.35/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 850/3456 ( 25%)]  Loss:  0.439187 (0.5161)  Time: 0.548s,   36.49/s  (0.536s,   37.30/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 900/3456 ( 26%)]  Loss:  0.470838 (0.5159)  Time: 0.547s,   36.55/s  (0.537s,   37.26/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [ 950/3456 ( 27%)]  Loss:  0.509140 (0.5144)  Time: 0.545s,   36.73/s  (0.537s,   37.23/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1000/3456 ( 29%)]  Loss:  0.540543 (0.5133)  Time: 0.545s,   36.71/s  (0.538s,   37.20/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1050/3456 ( 30%)]  Loss:  0.500335 (0.5125)  Time: 0.551s,   36.28/s  (0.538s,   37.17/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1100/3456 ( 32%)]  Loss:  0.583735 (0.5122)  Time: 0.550s,   36.34/s  (0.539s,   37.13/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1150/3456 ( 33%)]  Loss:  0.421226 (0.5110)  Time: 0.527s,   37.97/s  (0.539s,   37.12/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1200/3456 ( 35%)]  Loss:  0.445773 (0.5102)  Time: 0.531s,   37.67/s  (0.538s,   37.15/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1250/3456 ( 36%)]  Loss:  0.549908 (0.5093)  Time: 0.526s,   38.05/s  (0.538s,   37.18/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1300/3456 ( 38%)]  Loss:  0.579126 (0.5085)  Time: 0.530s,   37.75/s  (0.538s,   37.21/s)  LR: 3.965e-02  Data: 0.011 (0.012)\n",
      "Train: 18 [1350/3456 ( 39%)]  Loss:  0.577682 (0.5078)  Time: 0.528s,   37.88/s  (0.537s,   37.23/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1400/3456 ( 41%)]  Loss:  0.512676 (0.5073)  Time: 0.529s,   37.79/s  (0.537s,   37.26/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1450/3456 ( 42%)]  Loss:  0.600792 (0.5068)  Time: 0.524s,   38.14/s  (0.536s,   37.28/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1500/3456 ( 43%)]  Loss:  0.554982 (0.5065)  Time: 0.528s,   37.86/s  (0.536s,   37.29/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1550/3456 ( 45%)]  Loss:  0.410296 (0.5058)  Time: 0.525s,   38.08/s  (0.536s,   37.31/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1600/3456 ( 46%)]  Loss:  0.473519 (0.5052)  Time: 0.528s,   37.89/s  (0.536s,   37.33/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1650/3456 ( 48%)]  Loss:  0.392165 (0.5048)  Time: 0.526s,   37.99/s  (0.536s,   37.34/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1700/3456 ( 49%)]  Loss:  0.380286 (0.5045)  Time: 0.526s,   38.00/s  (0.535s,   37.36/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1750/3456 ( 51%)]  Loss:  0.503944 (0.5042)  Time: 0.526s,   38.03/s  (0.535s,   37.37/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1800/3456 ( 52%)]  Loss:  0.486209 (0.5043)  Time: 0.529s,   37.83/s  (0.535s,   37.38/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1850/3456 ( 54%)]  Loss:  0.458076 (0.5036)  Time: 0.529s,   37.81/s  (0.535s,   37.39/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1900/3456 ( 55%)]  Loss:  0.536165 (0.5031)  Time: 0.534s,   37.44/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [1950/3456 ( 56%)]  Loss:  0.443752 (0.5029)  Time: 0.527s,   37.94/s  (0.535s,   37.41/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2000/3456 ( 58%)]  Loss:  0.414915 (0.5028)  Time: 0.529s,   37.82/s  (0.534s,   37.42/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2050/3456 ( 59%)]  Loss:  0.497227 (0.5024)  Time: 0.527s,   37.97/s  (0.534s,   37.43/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2100/3456 ( 61%)]  Loss:  0.489047 (0.5024)  Time: 0.529s,   37.82/s  (0.534s,   37.43/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2150/3456 ( 62%)]  Loss:  0.497639 (0.5021)  Time: 0.531s,   37.64/s  (0.534s,   37.44/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2200/3456 ( 64%)]  Loss:  0.441280 (0.5011)  Time: 0.547s,   36.54/s  (0.535s,   37.42/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2250/3456 ( 65%)]  Loss:  0.531518 (0.5006)  Time: 0.550s,   36.35/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2300/3456 ( 67%)]  Loss:  0.430177 (0.5000)  Time: 0.547s,   36.58/s  (0.535s,   37.38/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2350/3456 ( 68%)]  Loss:  0.442941 (0.4997)  Time: 0.546s,   36.66/s  (0.535s,   37.36/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2400/3456 ( 69%)]  Loss:  0.469307 (0.4996)  Time: 0.527s,   37.98/s  (0.535s,   37.37/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2450/3456 ( 71%)]  Loss:  0.526098 (0.4996)  Time: 0.525s,   38.12/s  (0.535s,   37.38/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2500/3456 ( 72%)]  Loss:  0.384845 (0.4993)  Time: 0.532s,   37.60/s  (0.535s,   37.39/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2550/3456 ( 74%)]  Loss:  0.448698 (0.4988)  Time: 0.527s,   37.96/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2600/3456 ( 75%)]  Loss:  0.451873 (0.4985)  Time: 0.532s,   37.63/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2650/3456 ( 77%)]  Loss:  0.623927 (0.4983)  Time: 0.547s,   36.57/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2700/3456 ( 78%)]  Loss:  0.433662 (0.4980)  Time: 0.528s,   37.88/s  (0.535s,   37.39/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2750/3456 ( 80%)]  Loss:  0.514997 (0.4979)  Time: 0.546s,   36.66/s  (0.535s,   37.38/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2800/3456 ( 81%)]  Loss:  0.414352 (0.4977)  Time: 0.542s,   36.89/s  (0.535s,   37.36/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2850/3456 ( 82%)]  Loss:  0.521113 (0.4975)  Time: 0.543s,   36.82/s  (0.535s,   37.35/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2900/3456 ( 84%)]  Loss:  0.614188 (0.4975)  Time: 0.524s,   38.14/s  (0.535s,   37.36/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [2950/3456 ( 85%)]  Loss:  0.576319 (0.4972)  Time: 0.525s,   38.09/s  (0.535s,   37.37/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3000/3456 ( 87%)]  Loss:  0.421841 (0.4970)  Time: 0.528s,   37.91/s  (0.535s,   37.38/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3050/3456 ( 88%)]  Loss:  0.436900 (0.4967)  Time: 0.528s,   37.91/s  (0.535s,   37.39/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3100/3456 ( 90%)]  Loss:  0.515017 (0.4964)  Time: 0.525s,   38.07/s  (0.535s,   37.40/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3150/3456 ( 91%)]  Loss:  0.431039 (0.4962)  Time: 0.528s,   37.91/s  (0.535s,   37.41/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3200/3456 ( 93%)]  Loss:  0.471674 (0.4962)  Time: 0.523s,   38.22/s  (0.535s,   37.42/s)  LR: 3.965e-02  Data: 0.011 (0.011)\n",
      "Train: 18 [3250/3456 ( 94%)]  Loss:  1.868886 (0.5040)  Time: 0.964s,   20.74/s  (0.539s,   37.12/s)  LR: 3.965e-02  Data: 0.447 (0.015)\n",
      "Train: 18 [3300/3456 ( 96%)]  Loss:  1.119520 (0.5099)  Time: 0.923s,   21.67/s  (0.545s,   36.73/s)  LR: 3.965e-02  Data: 0.406 (0.021)\n",
      "Train: 18 [3350/3456 ( 97%)]  Loss:  0.768249 (0.5135)  Time: 0.899s,   22.25/s  (0.550s,   36.37/s)  LR: 3.965e-02  Data: 0.385 (0.027)\n",
      "Train: 18 [3400/3456 ( 98%)]  Loss:  1.338667 (0.5173)  Time: 0.906s,   22.08/s  (0.555s,   36.03/s)  LR: 3.965e-02  Data: 0.387 (0.032)\n",
      "Train: 18 [3450/3456 (100%)]  Loss:  0.793787 (0.5196)  Time: 0.893s,   22.40/s  (0.560s,   35.70/s)  LR: 3.965e-02  Data: 0.378 (0.037)\n",
      "Train: 18 [3455/3456 (100%)]  Loss:  1.008854 (0.5199)  Time: 0.447s,   24.63/s  (0.560s,   19.63/s)  LR: 3.965e-02  Data: 0.038 (0.038)\n",
      "Test: [   0/147]  Time: 0.869 (0.869)  Loss:  2.2792 (2.2792)  \n",
      "Test: [  50/147]  Time: 0.265 (0.279)  Loss:  2.8612 (2.6136)  \n",
      "Test: [ 100/147]  Time: 0.267 (0.275)  Loss:  2.6719 (2.6095)  \n",
      "Test: [ 147/147]  Time: 0.285 (0.321)  Loss:  1.1613 (2.4642)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-16.pth.tar', 2.6083568930625916)\n",
      "\n",
      "Train: 19 [   0/3456 (  0%)]  Loss:  2.454544 (2.4545)  Time: 1.425s,   14.03/s  (1.425s,   14.03/s)  LR: 3.961e-02  Data: 0.809 (0.809)\n",
      "Train: 19 [  50/3456 (  1%)]  Loss:  0.486828 (0.7806)  Time: 0.524s,   38.19/s  (0.544s,   36.75/s)  LR: 3.961e-02  Data: 0.011 (0.026)\n",
      "Train: 19 [ 100/3456 (  3%)]  Loss:  0.500641 (0.6604)  Time: 0.524s,   38.16/s  (0.536s,   37.33/s)  LR: 3.961e-02  Data: 0.011 (0.019)\n",
      "Train: 19 [ 150/3456 (  4%)]  Loss:  0.587252 (0.6098)  Time: 0.528s,   37.89/s  (0.534s,   37.47/s)  LR: 3.961e-02  Data: 0.011 (0.016)\n",
      "Train: 19 [ 200/3456 (  6%)]  Loss:  0.557211 (0.5859)  Time: 0.526s,   38.06/s  (0.532s,   37.58/s)  LR: 3.961e-02  Data: 0.011 (0.015)\n",
      "Train: 19 [ 250/3456 (  7%)]  Loss:  0.502943 (0.5712)  Time: 0.532s,   37.58/s  (0.531s,   37.64/s)  LR: 3.961e-02  Data: 0.011 (0.014)\n",
      "Train: 19 [ 300/3456 (  9%)]  Loss:  0.577588 (0.5602)  Time: 0.530s,   37.74/s  (0.531s,   37.69/s)  LR: 3.961e-02  Data: 0.011 (0.013)\n",
      "Train: 19 [ 350/3456 ( 10%)]  Loss:  0.523071 (0.5519)  Time: 0.529s,   37.83/s  (0.530s,   37.71/s)  LR: 3.961e-02  Data: 0.011 (0.013)\n",
      "Train: 19 [ 400/3456 ( 12%)]  Loss:  0.506296 (0.5439)  Time: 0.526s,   38.03/s  (0.530s,   37.72/s)  LR: 3.961e-02  Data: 0.011 (0.013)\n",
      "Train: 19 [ 450/3456 ( 13%)]  Loss:  0.541611 (0.5406)  Time: 0.528s,   37.85/s  (0.530s,   37.75/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 500/3456 ( 14%)]  Loss:  0.534615 (0.5360)  Time: 0.526s,   38.03/s  (0.530s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 550/3456 ( 16%)]  Loss:  0.464836 (0.5316)  Time: 0.525s,   38.10/s  (0.529s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 600/3456 ( 17%)]  Loss:  0.607969 (0.5285)  Time: 0.526s,   37.99/s  (0.529s,   37.80/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 650/3456 ( 19%)]  Loss:  0.626409 (0.5270)  Time: 0.526s,   37.99/s  (0.529s,   37.80/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 700/3456 ( 20%)]  Loss:  0.414247 (0.5239)  Time: 0.528s,   37.90/s  (0.529s,   37.81/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 750/3456 ( 22%)]  Loss:  0.544579 (0.5223)  Time: 0.527s,   37.97/s  (0.529s,   37.82/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 800/3456 ( 23%)]  Loss:  0.544392 (0.5210)  Time: 0.527s,   37.94/s  (0.529s,   37.81/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 850/3456 ( 25%)]  Loss:  0.466405 (0.5193)  Time: 0.527s,   37.94/s  (0.529s,   37.82/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 900/3456 ( 26%)]  Loss:  0.453881 (0.5181)  Time: 0.529s,   37.81/s  (0.529s,   37.83/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [ 950/3456 ( 27%)]  Loss:  0.579071 (0.5167)  Time: 0.525s,   38.13/s  (0.529s,   37.83/s)  LR: 3.961e-02  Data: 0.011 (0.012)\n",
      "Train: 19 [1000/3456 ( 29%)]  Loss:  0.427975 (0.5153)  Time: 0.528s,   37.90/s  (0.529s,   37.83/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1050/3456 ( 30%)]  Loss:  0.525394 (0.5146)  Time: 0.542s,   36.90/s  (0.529s,   37.79/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1100/3456 ( 32%)]  Loss:  0.532400 (0.5140)  Time: 0.529s,   37.82/s  (0.529s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1150/3456 ( 33%)]  Loss:  0.465260 (0.5126)  Time: 0.532s,   37.59/s  (0.529s,   37.78/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1200/3456 ( 35%)]  Loss:  0.479433 (0.5116)  Time: 0.524s,   38.19/s  (0.529s,   37.78/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1250/3456 ( 36%)]  Loss:  0.489320 (0.5110)  Time: 0.543s,   36.83/s  (0.529s,   37.78/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1300/3456 ( 38%)]  Loss:  0.486801 (0.5096)  Time: 0.545s,   36.71/s  (0.530s,   37.75/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1350/3456 ( 39%)]  Loss:  0.499899 (0.5087)  Time: 0.524s,   38.17/s  (0.530s,   37.76/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1400/3456 ( 41%)]  Loss:  0.401324 (0.5079)  Time: 0.525s,   38.12/s  (0.529s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1450/3456 ( 42%)]  Loss:  0.566340 (0.5070)  Time: 0.524s,   38.15/s  (0.529s,   37.79/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1500/3456 ( 43%)]  Loss:  0.453099 (0.5065)  Time: 0.525s,   38.13/s  (0.529s,   37.80/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1550/3456 ( 45%)]  Loss:  0.432639 (0.5062)  Time: 0.524s,   38.14/s  (0.529s,   37.81/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1600/3456 ( 46%)]  Loss:  0.466336 (0.5054)  Time: 0.525s,   38.11/s  (0.529s,   37.82/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1650/3456 ( 48%)]  Loss:  0.414228 (0.5051)  Time: 0.523s,   38.21/s  (0.529s,   37.82/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1700/3456 ( 49%)]  Loss:  0.503065 (0.5048)  Time: 0.524s,   38.15/s  (0.529s,   37.83/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1750/3456 ( 51%)]  Loss:  0.487496 (0.5042)  Time: 0.524s,   38.17/s  (0.529s,   37.84/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1800/3456 ( 52%)]  Loss:  0.529005 (0.5040)  Time: 0.523s,   38.22/s  (0.528s,   37.85/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1850/3456 ( 54%)]  Loss:  0.457687 (0.5030)  Time: 0.524s,   38.17/s  (0.528s,   37.85/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [1900/3456 ( 55%)]  Loss:  0.515847 (0.5026)  Time: 0.524s,   38.20/s  (0.528s,   37.86/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [1950/3456 ( 56%)]  Loss:  0.579945 (0.5021)  Time: 0.525s,   38.11/s  (0.528s,   37.87/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2000/3456 ( 58%)]  Loss:  0.462068 (0.5019)  Time: 0.525s,   38.08/s  (0.528s,   37.88/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2050/3456 ( 59%)]  Loss:  0.591740 (0.5018)  Time: 0.529s,   37.80/s  (0.528s,   37.87/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2100/3456 ( 61%)]  Loss:  0.553356 (0.5016)  Time: 0.546s,   36.61/s  (0.529s,   37.84/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2150/3456 ( 62%)]  Loss:  0.360982 (0.5014)  Time: 0.542s,   36.88/s  (0.529s,   37.82/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2200/3456 ( 64%)]  Loss:  0.414320 (0.5006)  Time: 0.546s,   36.66/s  (0.529s,   37.79/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2250/3456 ( 65%)]  Loss:  0.485197 (0.5001)  Time: 0.543s,   36.80/s  (0.530s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2300/3456 ( 67%)]  Loss:  0.540218 (0.4995)  Time: 0.529s,   37.83/s  (0.530s,   37.75/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2350/3456 ( 68%)]  Loss:  0.534447 (0.4991)  Time: 0.531s,   37.69/s  (0.530s,   37.75/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2400/3456 ( 69%)]  Loss:  0.492681 (0.4990)  Time: 0.525s,   38.11/s  (0.530s,   37.76/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2450/3456 ( 71%)]  Loss:  0.483929 (0.4989)  Time: 0.531s,   37.66/s  (0.530s,   37.76/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2500/3456 ( 72%)]  Loss:  0.428308 (0.4988)  Time: 0.524s,   38.14/s  (0.530s,   37.76/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2550/3456 ( 74%)]  Loss:  0.505683 (0.4985)  Time: 0.531s,   37.65/s  (0.530s,   37.77/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2600/3456 ( 75%)]  Loss:  0.506779 (0.4984)  Time: 0.548s,   36.48/s  (0.530s,   37.75/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2650/3456 ( 77%)]  Loss:  0.508825 (0.4981)  Time: 0.547s,   36.54/s  (0.530s,   37.73/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2700/3456 ( 78%)]  Loss:  0.511602 (0.4977)  Time: 0.548s,   36.48/s  (0.530s,   37.71/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2750/3456 ( 80%)]  Loss:  0.451306 (0.4976)  Time: 0.543s,   36.83/s  (0.531s,   37.69/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2800/3456 ( 81%)]  Loss:  0.411516 (0.4974)  Time: 0.524s,   38.19/s  (0.531s,   37.70/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2850/3456 ( 82%)]  Loss:  0.531517 (0.4970)  Time: 0.521s,   38.38/s  (0.530s,   37.71/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2900/3456 ( 84%)]  Loss:  0.532777 (0.4969)  Time: 0.542s,   36.91/s  (0.530s,   37.71/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [2950/3456 ( 85%)]  Loss:  0.613508 (0.4966)  Time: 0.529s,   37.80/s  (0.530s,   37.71/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [3000/3456 ( 87%)]  Loss:  0.451826 (0.4964)  Time: 0.527s,   37.98/s  (0.530s,   37.72/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [3050/3456 ( 88%)]  Loss:  0.387472 (0.4960)  Time: 0.525s,   38.10/s  (0.530s,   37.72/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [3100/3456 ( 90%)]  Loss:  0.468704 (0.4956)  Time: 0.541s,   37.00/s  (0.530s,   37.73/s)  LR: 3.961e-02  Data: 0.010 (0.011)\n",
      "Train: 19 [3150/3456 ( 91%)]  Loss:  0.396074 (0.4953)  Time: 0.528s,   37.89/s  (0.530s,   37.73/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [3200/3456 ( 93%)]  Loss:  0.515623 (0.4952)  Time: 0.524s,   38.14/s  (0.530s,   37.73/s)  LR: 3.961e-02  Data: 0.011 (0.011)\n",
      "Train: 19 [3250/3456 ( 94%)]  Loss:  0.653663 (0.5001)  Time: 0.914s,   21.88/s  (0.534s,   37.44/s)  LR: 3.961e-02  Data: 0.400 (0.015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 19 [3300/3456 ( 96%)]  Loss:  0.838709 (0.5053)  Time: 0.910s,   21.99/s  (0.540s,   37.04/s)  LR: 3.961e-02  Data: 0.373 (0.021)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 19 [3350/3456 ( 97%)]  Loss:  0.805106 (0.5096)  Time: 0.894s,   22.38/s  (0.545s,   36.68/s)  LR: 3.961e-02  Data: 0.379 (0.026)\n",
      "Train: 19 [3400/3456 ( 98%)]  Loss:  1.358881 (0.5130)  Time: 0.928s,   21.55/s  (0.550s,   36.34/s)  LR: 3.961e-02  Data: 0.408 (0.031)\n",
      "Train: 19 [3450/3456 (100%)]  Loss:  0.706757 (0.5156)  Time: 0.899s,   22.24/s  (0.555s,   36.02/s)  LR: 3.961e-02  Data: 0.386 (0.036)\n",
      "Train: 19 [3455/3456 (100%)]  Loss:  0.956058 (0.5161)  Time: 0.442s,   24.87/s  (0.555s,   19.80/s)  LR: 3.961e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.874 (0.874)  Loss:  2.4010 (2.4010)  \n",
      "Test: [  50/147]  Time: 0.265 (0.279)  Loss:  2.8130 (2.6470)  \n",
      "Test: [ 100/147]  Time: 0.264 (0.272)  Loss:  2.6515 (2.6435)  \n",
      "Test: [ 147/147]  Time: 0.304 (0.319)  Loss:  0.7779 (2.4770)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-19.pth.tar', 2.476996954429794)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-1.pth.tar', 2.560471292283084)\n",
      "\n",
      "Train: 20 [   0/3456 (  0%)]  Loss:  2.709165 (2.7092)  Time: 1.421s,   14.07/s  (1.421s,   14.07/s)  LR: 3.956e-02  Data: 0.792 (0.792)\n",
      "Train: 20 [  50/3456 (  1%)]  Loss:  0.498963 (0.7697)  Time: 0.540s,   37.03/s  (0.565s,   35.38/s)  LR: 3.956e-02  Data: 0.011 (0.026)\n",
      "Train: 20 [ 100/3456 (  3%)]  Loss:  0.549510 (0.6567)  Time: 0.542s,   36.88/s  (0.551s,   36.27/s)  LR: 3.956e-02  Data: 0.011 (0.019)\n",
      "Train: 20 [ 150/3456 (  4%)]  Loss:  0.493508 (0.6085)  Time: 0.543s,   36.84/s  (0.549s,   36.42/s)  LR: 3.956e-02  Data: 0.011 (0.016)\n",
      "Train: 20 [ 200/3456 (  6%)]  Loss:  0.567338 (0.5807)  Time: 0.525s,   38.09/s  (0.545s,   36.71/s)  LR: 3.956e-02  Data: 0.011 (0.015)\n",
      "Train: 20 [ 250/3456 (  7%)]  Loss:  0.450160 (0.5683)  Time: 0.526s,   38.00/s  (0.541s,   36.97/s)  LR: 3.956e-02  Data: 0.011 (0.014)\n",
      "Train: 20 [ 300/3456 (  9%)]  Loss:  0.571007 (0.5591)  Time: 0.531s,   37.68/s  (0.538s,   37.14/s)  LR: 3.956e-02  Data: 0.011 (0.013)\n",
      "Train: 20 [ 350/3456 ( 10%)]  Loss:  0.500606 (0.5518)  Time: 0.527s,   37.94/s  (0.537s,   37.26/s)  LR: 3.956e-02  Data: 0.011 (0.013)\n",
      "Train: 20 [ 400/3456 ( 12%)]  Loss:  0.416204 (0.5458)  Time: 0.527s,   37.92/s  (0.535s,   37.35/s)  LR: 3.956e-02  Data: 0.011 (0.013)\n",
      "Train: 20 [ 450/3456 ( 13%)]  Loss:  0.518751 (0.5423)  Time: 0.526s,   38.02/s  (0.535s,   37.40/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 500/3456 ( 14%)]  Loss:  0.537421 (0.5384)  Time: 0.526s,   38.02/s  (0.534s,   37.45/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 550/3456 ( 16%)]  Loss:  0.443566 (0.5330)  Time: 0.528s,   37.88/s  (0.533s,   37.50/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 600/3456 ( 17%)]  Loss:  0.500710 (0.5293)  Time: 0.529s,   37.79/s  (0.533s,   37.54/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 650/3456 ( 19%)]  Loss:  0.558703 (0.5265)  Time: 0.523s,   38.25/s  (0.532s,   37.58/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 700/3456 ( 20%)]  Loss:  0.412677 (0.5232)  Time: 0.529s,   37.80/s  (0.532s,   37.61/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 750/3456 ( 22%)]  Loss:  0.542261 (0.5214)  Time: 0.545s,   36.69/s  (0.531s,   37.63/s)  LR: 3.956e-02  Data: 0.012 (0.012)\n",
      "Train: 20 [ 800/3456 ( 23%)]  Loss:  0.479412 (0.5202)  Time: 0.529s,   37.83/s  (0.531s,   37.64/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 850/3456 ( 25%)]  Loss:  0.572408 (0.5196)  Time: 0.528s,   37.88/s  (0.531s,   37.66/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 900/3456 ( 26%)]  Loss:  0.470488 (0.5188)  Time: 0.530s,   37.76/s  (0.531s,   37.67/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [ 950/3456 ( 27%)]  Loss:  0.510294 (0.5170)  Time: 0.532s,   37.56/s  (0.531s,   37.68/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [1000/3456 ( 29%)]  Loss:  0.472788 (0.5158)  Time: 0.548s,   36.52/s  (0.531s,   37.66/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [1050/3456 ( 30%)]  Loss:  0.412572 (0.5149)  Time: 0.547s,   36.59/s  (0.532s,   37.62/s)  LR: 3.956e-02  Data: 0.011 (0.012)\n",
      "Train: 20 [1100/3456 ( 32%)]  Loss:  0.580967 (0.5143)  Time: 0.546s,   36.64/s  (0.532s,   37.57/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1150/3456 ( 33%)]  Loss:  0.433757 (0.5128)  Time: 0.542s,   36.88/s  (0.533s,   37.54/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1200/3456 ( 35%)]  Loss:  0.471980 (0.5118)  Time: 0.544s,   36.74/s  (0.533s,   37.49/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1250/3456 ( 36%)]  Loss:  0.524476 (0.5110)  Time: 0.547s,   36.55/s  (0.534s,   37.46/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1300/3456 ( 38%)]  Loss:  0.451416 (0.5094)  Time: 0.547s,   36.56/s  (0.534s,   37.43/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1350/3456 ( 39%)]  Loss:  0.431052 (0.5086)  Time: 0.543s,   36.80/s  (0.535s,   37.40/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1400/3456 ( 41%)]  Loss:  0.487781 (0.5077)  Time: 0.541s,   36.94/s  (0.535s,   37.38/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1450/3456 ( 42%)]  Loss:  0.659075 (0.5072)  Time: 0.526s,   38.04/s  (0.535s,   37.38/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1500/3456 ( 43%)]  Loss:  0.460378 (0.5070)  Time: 0.541s,   36.99/s  (0.535s,   37.37/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1550/3456 ( 45%)]  Loss:  0.481671 (0.5066)  Time: 0.542s,   36.89/s  (0.535s,   37.35/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1600/3456 ( 46%)]  Loss:  0.528005 (0.5058)  Time: 0.544s,   36.73/s  (0.536s,   37.33/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1650/3456 ( 48%)]  Loss:  0.386314 (0.5053)  Time: 0.548s,   36.50/s  (0.536s,   37.32/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1700/3456 ( 49%)]  Loss:  0.356541 (0.5049)  Time: 0.542s,   36.87/s  (0.536s,   37.30/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1750/3456 ( 51%)]  Loss:  0.520152 (0.5043)  Time: 0.544s,   36.79/s  (0.536s,   37.28/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1800/3456 ( 52%)]  Loss:  0.535363 (0.5042)  Time: 0.526s,   38.06/s  (0.536s,   37.28/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1850/3456 ( 54%)]  Loss:  0.557103 (0.5034)  Time: 0.522s,   38.29/s  (0.536s,   37.30/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1900/3456 ( 55%)]  Loss:  0.502696 (0.5031)  Time: 0.532s,   37.61/s  (0.536s,   37.31/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [1950/3456 ( 56%)]  Loss:  0.403209 (0.5027)  Time: 0.525s,   38.06/s  (0.536s,   37.33/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2000/3456 ( 58%)]  Loss:  0.463859 (0.5026)  Time: 0.527s,   37.96/s  (0.536s,   37.35/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2050/3456 ( 59%)]  Loss:  0.472744 (0.5023)  Time: 0.541s,   36.97/s  (0.535s,   37.36/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2100/3456 ( 61%)]  Loss:  0.499558 (0.5022)  Time: 0.549s,   36.43/s  (0.536s,   37.34/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2150/3456 ( 62%)]  Loss:  0.525354 (0.5021)  Time: 0.543s,   36.86/s  (0.536s,   37.33/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2200/3456 ( 64%)]  Loss:  0.405609 (0.5013)  Time: 0.545s,   36.71/s  (0.536s,   37.31/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2250/3456 ( 65%)]  Loss:  0.510863 (0.5008)  Time: 0.545s,   36.71/s  (0.536s,   37.30/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2300/3456 ( 67%)]  Loss:  0.429762 (0.5004)  Time: 0.525s,   38.07/s  (0.536s,   37.31/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2350/3456 ( 68%)]  Loss:  0.502424 (0.5001)  Time: 0.545s,   36.72/s  (0.536s,   37.31/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2400/3456 ( 69%)]  Loss:  0.602964 (0.5003)  Time: 0.542s,   36.88/s  (0.536s,   37.30/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2450/3456 ( 71%)]  Loss:  0.526458 (0.5002)  Time: 0.526s,   38.04/s  (0.536s,   37.32/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2500/3456 ( 72%)]  Loss:  0.495527 (0.5001)  Time: 0.527s,   37.93/s  (0.536s,   37.33/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2550/3456 ( 74%)]  Loss:  0.493769 (0.4997)  Time: 0.523s,   38.25/s  (0.536s,   37.35/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2600/3456 ( 75%)]  Loss:  0.457066 (0.4995)  Time: 0.523s,   38.22/s  (0.535s,   37.36/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2650/3456 ( 77%)]  Loss:  0.542719 (0.4994)  Time: 0.530s,   37.74/s  (0.535s,   37.37/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2700/3456 ( 78%)]  Loss:  0.506288 (0.4992)  Time: 0.527s,   37.95/s  (0.535s,   37.38/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2750/3456 ( 80%)]  Loss:  0.464962 (0.4991)  Time: 0.528s,   37.85/s  (0.535s,   37.39/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2800/3456 ( 81%)]  Loss:  0.433403 (0.4987)  Time: 0.528s,   37.88/s  (0.535s,   37.40/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2850/3456 ( 82%)]  Loss:  0.541033 (0.4985)  Time: 0.528s,   37.91/s  (0.535s,   37.41/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2900/3456 ( 84%)]  Loss:  0.476756 (0.4984)  Time: 0.526s,   38.00/s  (0.534s,   37.42/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [2950/3456 ( 85%)]  Loss:  0.542635 (0.4982)  Time: 0.528s,   37.86/s  (0.534s,   37.43/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [3000/3456 ( 87%)]  Loss:  0.395888 (0.4979)  Time: 0.547s,   36.57/s  (0.534s,   37.44/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [3050/3456 ( 88%)]  Loss:  0.385844 (0.4976)  Time: 0.540s,   37.02/s  (0.534s,   37.43/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [3100/3456 ( 90%)]  Loss:  0.563013 (0.4972)  Time: 0.545s,   36.72/s  (0.535s,   37.41/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [3150/3456 ( 91%)]  Loss:  0.507362 (0.4970)  Time: 0.542s,   36.87/s  (0.535s,   37.40/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n",
      "Train: 20 [3200/3456 ( 93%)]  Loss:  0.468658 (0.4969)  Time: 0.543s,   36.81/s  (0.535s,   37.39/s)  LR: 3.956e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 20 [3250/3456 ( 94%)]  Loss:  0.905155 (0.5028)  Time: 1.013s,   19.74/s  (0.539s,   37.09/s)  LR: 3.956e-02  Data: 0.472 (0.015)\n",
      "Train: 20 [3300/3456 ( 96%)]  Loss:  0.606145 (0.5083)  Time: 0.941s,   21.26/s  (0.545s,   36.69/s)  LR: 3.956e-02  Data: 0.401 (0.021)\n",
      "Train: 20 [3350/3456 ( 97%)]  Loss:  0.544648 (0.5117)  Time: 0.902s,   22.17/s  (0.550s,   36.33/s)  LR: 3.956e-02  Data: 0.387 (0.026)\n",
      "Train: 20 [3400/3456 ( 98%)]  Loss:  0.934465 (0.5153)  Time: 0.881s,   22.71/s  (0.556s,   36.00/s)  LR: 3.956e-02  Data: 0.366 (0.031)\n",
      "Train: 20 [3450/3456 (100%)]  Loss:  0.759066 (0.5176)  Time: 0.908s,   22.03/s  (0.560s,   35.69/s)  LR: 3.956e-02  Data: 0.391 (0.036)\n",
      "Train: 20 [3455/3456 (100%)]  Loss:  0.613630 (0.5180)  Time: 0.451s,   24.39/s  (0.561s,   19.62/s)  LR: 3.956e-02  Data: 0.039 (0.037)\n",
      "Test: [   0/147]  Time: 0.864 (0.864)  Loss:  2.4277 (2.4277)  \n",
      "Test: [  50/147]  Time: 0.266 (0.280)  Loss:  2.8085 (2.6470)  \n",
      "Test: [ 100/147]  Time: 0.270 (0.275)  Loss:  2.6817 (2.6368)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.320)  Loss:  1.1262 (2.4920)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-19.pth.tar', 2.476996954429794)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-20.pth.tar', 2.4920134991407394)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-8.pth.tar', 2.560091979600288)\n",
      "\n",
      "Train: 21 [   0/3456 (  0%)]  Loss:  2.428733 (2.4287)  Time: 1.432s,   13.97/s  (1.432s,   13.97/s)  LR: 3.952e-02  Data: 0.832 (0.832)\n",
      "Train: 21 [  50/3456 (  1%)]  Loss:  0.486326 (0.7535)  Time: 0.525s,   38.09/s  (0.544s,   36.79/s)  LR: 3.952e-02  Data: 0.011 (0.027)\n",
      "Train: 21 [ 100/3456 (  3%)]  Loss:  0.523593 (0.6414)  Time: 0.525s,   38.11/s  (0.535s,   37.42/s)  LR: 3.952e-02  Data: 0.010 (0.019)\n",
      "Train: 21 [ 150/3456 (  4%)]  Loss:  0.518603 (0.5914)  Time: 0.526s,   38.02/s  (0.532s,   37.62/s)  LR: 3.952e-02  Data: 0.011 (0.016)\n",
      "Train: 21 [ 200/3456 (  6%)]  Loss:  0.568129 (0.5685)  Time: 0.527s,   37.95/s  (0.530s,   37.73/s)  LR: 3.952e-02  Data: 0.011 (0.015)\n",
      "Train: 21 [ 250/3456 (  7%)]  Loss:  0.464597 (0.5577)  Time: 0.525s,   38.08/s  (0.530s,   37.72/s)  LR: 3.952e-02  Data: 0.011 (0.014)\n",
      "Train: 21 [ 300/3456 (  9%)]  Loss:  0.558014 (0.5500)  Time: 0.533s,   37.50/s  (0.530s,   37.77/s)  LR: 3.952e-02  Data: 0.010 (0.013)\n",
      "Train: 21 [ 350/3456 ( 10%)]  Loss:  0.525434 (0.5427)  Time: 0.530s,   37.73/s  (0.529s,   37.79/s)  LR: 3.952e-02  Data: 0.010 (0.013)\n",
      "Train: 21 [ 400/3456 ( 12%)]  Loss:  0.535166 (0.5372)  Time: 0.525s,   38.13/s  (0.529s,   37.81/s)  LR: 3.952e-02  Data: 0.010 (0.013)\n",
      "Train: 21 [ 450/3456 ( 13%)]  Loss:  0.462403 (0.5344)  Time: 0.530s,   37.77/s  (0.529s,   37.83/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 500/3456 ( 14%)]  Loss:  0.453640 (0.5311)  Time: 0.525s,   38.09/s  (0.529s,   37.81/s)  LR: 3.952e-02  Data: 0.010 (0.012)\n",
      "Train: 21 [ 550/3456 ( 16%)]  Loss:  0.536795 (0.5272)  Time: 0.527s,   37.95/s  (0.529s,   37.82/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 600/3456 ( 17%)]  Loss:  0.544812 (0.5243)  Time: 0.526s,   38.03/s  (0.529s,   37.82/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 650/3456 ( 19%)]  Loss:  0.585978 (0.5219)  Time: 0.542s,   36.88/s  (0.529s,   37.78/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 700/3456 ( 20%)]  Loss:  0.442635 (0.5192)  Time: 0.545s,   36.71/s  (0.530s,   37.71/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 750/3456 ( 22%)]  Loss:  0.582075 (0.5174)  Time: 0.544s,   36.74/s  (0.531s,   37.64/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 800/3456 ( 23%)]  Loss:  0.543515 (0.5162)  Time: 0.541s,   36.97/s  (0.532s,   37.58/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 850/3456 ( 25%)]  Loss:  0.485790 (0.5144)  Time: 0.545s,   36.72/s  (0.533s,   37.53/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 900/3456 ( 26%)]  Loss:  0.473983 (0.5137)  Time: 0.544s,   36.80/s  (0.534s,   37.48/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [ 950/3456 ( 27%)]  Loss:  0.534431 (0.5122)  Time: 0.542s,   36.91/s  (0.534s,   37.42/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [1000/3456 ( 29%)]  Loss:  0.534186 (0.5111)  Time: 0.550s,   36.35/s  (0.535s,   37.38/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [1050/3456 ( 30%)]  Loss:  0.485714 (0.5102)  Time: 0.544s,   36.76/s  (0.536s,   37.35/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [1100/3456 ( 32%)]  Loss:  0.514107 (0.5101)  Time: 0.544s,   36.78/s  (0.536s,   37.31/s)  LR: 3.952e-02  Data: 0.011 (0.012)\n",
      "Train: 21 [1150/3456 ( 33%)]  Loss:  0.452948 (0.5086)  Time: 0.543s,   36.81/s  (0.536s,   37.29/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1200/3456 ( 35%)]  Loss:  0.423764 (0.5079)  Time: 0.526s,   38.05/s  (0.536s,   37.31/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1250/3456 ( 36%)]  Loss:  0.599240 (0.5077)  Time: 0.526s,   38.02/s  (0.536s,   37.34/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1300/3456 ( 38%)]  Loss:  0.490615 (0.5068)  Time: 0.530s,   37.71/s  (0.535s,   37.36/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1350/3456 ( 39%)]  Loss:  0.494070 (0.5061)  Time: 0.526s,   38.01/s  (0.535s,   37.37/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1400/3456 ( 41%)]  Loss:  0.495924 (0.5055)  Time: 0.526s,   38.03/s  (0.535s,   37.40/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1450/3456 ( 42%)]  Loss:  0.554620 (0.5048)  Time: 0.525s,   38.08/s  (0.535s,   37.41/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1500/3456 ( 43%)]  Loss:  0.449933 (0.5048)  Time: 0.527s,   37.97/s  (0.534s,   37.43/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1550/3456 ( 45%)]  Loss:  0.496303 (0.5044)  Time: 0.526s,   38.01/s  (0.534s,   37.45/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1600/3456 ( 46%)]  Loss:  0.523571 (0.5038)  Time: 0.525s,   38.07/s  (0.534s,   37.47/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1650/3456 ( 48%)]  Loss:  0.399361 (0.5032)  Time: 0.526s,   38.05/s  (0.534s,   37.49/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1700/3456 ( 49%)]  Loss:  0.394062 (0.5029)  Time: 0.525s,   38.07/s  (0.533s,   37.50/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1750/3456 ( 51%)]  Loss:  0.445488 (0.5025)  Time: 0.525s,   38.11/s  (0.533s,   37.51/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1800/3456 ( 52%)]  Loss:  0.575302 (0.5026)  Time: 0.530s,   37.74/s  (0.533s,   37.53/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1850/3456 ( 54%)]  Loss:  0.488981 (0.5017)  Time: 0.525s,   38.12/s  (0.533s,   37.54/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1900/3456 ( 55%)]  Loss:  0.571501 (0.5013)  Time: 0.525s,   38.08/s  (0.533s,   37.55/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [1950/3456 ( 56%)]  Loss:  0.525873 (0.5011)  Time: 0.542s,   36.88/s  (0.533s,   37.54/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2000/3456 ( 58%)]  Loss:  0.479200 (0.5009)  Time: 0.524s,   38.14/s  (0.533s,   37.54/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2050/3456 ( 59%)]  Loss:  0.514387 (0.5009)  Time: 0.527s,   37.95/s  (0.533s,   37.55/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2100/3456 ( 61%)]  Loss:  0.406800 (0.5007)  Time: 0.527s,   37.96/s  (0.532s,   37.56/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2150/3456 ( 62%)]  Loss:  0.495779 (0.5002)  Time: 0.527s,   37.96/s  (0.532s,   37.57/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2200/3456 ( 64%)]  Loss:  0.481318 (0.4993)  Time: 0.528s,   37.87/s  (0.532s,   37.58/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2250/3456 ( 65%)]  Loss:  0.483825 (0.4988)  Time: 0.542s,   36.91/s  (0.532s,   37.58/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2300/3456 ( 67%)]  Loss:  0.538149 (0.4983)  Time: 0.549s,   36.42/s  (0.533s,   37.55/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2350/3456 ( 68%)]  Loss:  0.447771 (0.4979)  Time: 0.542s,   36.89/s  (0.533s,   37.54/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2400/3456 ( 69%)]  Loss:  0.550190 (0.4978)  Time: 0.543s,   36.83/s  (0.533s,   37.52/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2450/3456 ( 71%)]  Loss:  0.463428 (0.4976)  Time: 0.548s,   36.49/s  (0.533s,   37.50/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2500/3456 ( 72%)]  Loss:  0.472769 (0.4976)  Time: 0.550s,   36.39/s  (0.534s,   37.48/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2550/3456 ( 74%)]  Loss:  0.511531 (0.4972)  Time: 0.549s,   36.43/s  (0.534s,   37.46/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2600/3456 ( 75%)]  Loss:  0.432213 (0.4972)  Time: 0.543s,   36.83/s  (0.534s,   37.44/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2650/3456 ( 77%)]  Loss:  0.577055 (0.4971)  Time: 0.549s,   36.46/s  (0.534s,   37.43/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2700/3456 ( 78%)]  Loss:  0.436313 (0.4969)  Time: 0.550s,   36.37/s  (0.535s,   37.41/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2750/3456 ( 80%)]  Loss:  0.486386 (0.4966)  Time: 0.526s,   37.99/s  (0.535s,   37.40/s)  LR: 3.952e-02  Data: 0.012 (0.011)\n",
      "Train: 21 [2800/3456 ( 81%)]  Loss:  0.440983 (0.4963)  Time: 0.531s,   37.67/s  (0.535s,   37.41/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2850/3456 ( 82%)]  Loss:  0.563802 (0.4961)  Time: 0.528s,   37.88/s  (0.534s,   37.42/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2900/3456 ( 84%)]  Loss:  0.520570 (0.4962)  Time: 0.525s,   38.08/s  (0.534s,   37.43/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [2950/3456 ( 85%)]  Loss:  0.471910 (0.4960)  Time: 0.524s,   38.17/s  (0.534s,   37.44/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [3000/3456 ( 87%)]  Loss:  0.388481 (0.4961)  Time: 0.525s,   38.10/s  (0.534s,   37.44/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [3050/3456 ( 88%)]  Loss:  0.428476 (0.4958)  Time: 0.527s,   37.92/s  (0.534s,   37.45/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [3100/3456 ( 90%)]  Loss:  0.454618 (0.4956)  Time: 0.541s,   36.95/s  (0.534s,   37.46/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [3150/3456 ( 91%)]  Loss:  0.435168 (0.4953)  Time: 0.544s,   36.74/s  (0.534s,   37.44/s)  LR: 3.952e-02  Data: 0.011 (0.011)\n",
      "Train: 21 [3200/3456 ( 93%)]  Loss:  0.476325 (0.4951)  Time: 0.546s,   36.63/s  (0.534s,   37.45/s)  LR: 3.952e-02  Data: 0.012 (0.011)\n",
      "Train: 21 [3250/3456 ( 94%)]  Loss:  1.326976 (0.5000)  Time: 0.948s,   21.10/s  (0.538s,   37.14/s)  LR: 3.952e-02  Data: 0.410 (0.015)\n",
      "Train: 21 [3300/3456 ( 96%)]  Loss:  1.435788 (0.5053)  Time: 0.881s,   22.71/s  (0.544s,   36.76/s)  LR: 3.952e-02  Data: 0.347 (0.021)\n",
      "Train: 21 [3350/3456 ( 97%)]  Loss:  0.538893 (0.5088)  Time: 0.933s,   21.43/s  (0.549s,   36.41/s)  LR: 3.952e-02  Data: 0.397 (0.026)\n",
      "Train: 21 [3400/3456 ( 98%)]  Loss:  0.859798 (0.5132)  Time: 0.932s,   21.46/s  (0.554s,   36.07/s)  LR: 3.952e-02  Data: 0.396 (0.031)\n",
      "Train: 21 [3450/3456 (100%)]  Loss:  0.872396 (0.5157)  Time: 0.890s,   22.48/s  (0.559s,   35.76/s)  LR: 3.952e-02  Data: 0.357 (0.036)\n",
      "Train: 21 [3455/3456 (100%)]  Loss:  0.845392 (0.5161)  Time: 0.460s,   23.90/s  (0.560s,   19.66/s)  LR: 3.952e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.876 (0.876)  Loss:  2.1911 (2.1911)  \n",
      "Test: [  50/147]  Time: 0.278 (0.297)  Loss:  2.7435 (2.5042)  \n",
      "Test: [ 100/147]  Time: 0.285 (0.289)  Loss:  2.5390 (2.4973)  \n",
      "Test: [ 147/147]  Time: 0.306 (0.333)  Loss:  1.6101 (2.4101)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-19.pth.tar', 2.476996954429794)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-20.pth.tar', 2.4920134991407394)\n",
      "\n",
      "Train: 22 [   0/3456 (  0%)]  Loss:  2.282848 (2.2828)  Time: 1.406s,   14.23/s  (1.406s,   14.23/s)  LR: 3.947e-02  Data: 0.762 (0.762)\n",
      "Train: 22 [  50/3456 (  1%)]  Loss:  0.417507 (0.7527)  Time: 0.524s,   38.16/s  (0.550s,   36.37/s)  LR: 3.947e-02  Data: 0.010 (0.026)\n",
      "Train: 22 [ 100/3456 (  3%)]  Loss:  0.578546 (0.6516)  Time: 0.523s,   38.21/s  (0.538s,   37.15/s)  LR: 3.947e-02  Data: 0.010 (0.018)\n",
      "Train: 22 [ 150/3456 (  4%)]  Loss:  0.571058 (0.6031)  Time: 0.530s,   37.75/s  (0.535s,   37.42/s)  LR: 3.947e-02  Data: 0.011 (0.016)\n",
      "Train: 22 [ 200/3456 (  6%)]  Loss:  0.585471 (0.5799)  Time: 0.526s,   38.03/s  (0.533s,   37.54/s)  LR: 3.947e-02  Data: 0.011 (0.014)\n",
      "Train: 22 [ 250/3456 (  7%)]  Loss:  0.458392 (0.5643)  Time: 0.530s,   37.72/s  (0.532s,   37.62/s)  LR: 3.947e-02  Data: 0.011 (0.014)\n",
      "Train: 22 [ 300/3456 (  9%)]  Loss:  0.554144 (0.5549)  Time: 0.527s,   37.93/s  (0.531s,   37.67/s)  LR: 3.947e-02  Data: 0.011 (0.013)\n",
      "Train: 22 [ 350/3456 ( 10%)]  Loss:  0.543090 (0.5478)  Time: 0.525s,   38.09/s  (0.530s,   37.71/s)  LR: 3.947e-02  Data: 0.011 (0.013)\n",
      "Train: 22 [ 400/3456 ( 12%)]  Loss:  0.501495 (0.5417)  Time: 0.526s,   38.05/s  (0.531s,   37.69/s)  LR: 3.947e-02  Data: 0.011 (0.013)\n",
      "Train: 22 [ 450/3456 ( 13%)]  Loss:  0.524750 (0.5378)  Time: 0.529s,   37.82/s  (0.530s,   37.71/s)  LR: 3.947e-02  Data: 0.010 (0.012)\n",
      "Train: 22 [ 500/3456 ( 14%)]  Loss:  0.468090 (0.5343)  Time: 0.530s,   37.77/s  (0.530s,   37.73/s)  LR: 3.947e-02  Data: 0.010 (0.012)\n",
      "Train: 22 [ 550/3456 ( 16%)]  Loss:  0.463084 (0.5295)  Time: 0.529s,   37.84/s  (0.530s,   37.75/s)  LR: 3.947e-02  Data: 0.010 (0.012)\n",
      "Train: 22 [ 600/3456 ( 17%)]  Loss:  0.534394 (0.5266)  Time: 0.541s,   36.93/s  (0.530s,   37.70/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 650/3456 ( 19%)]  Loss:  0.536368 (0.5245)  Time: 0.545s,   36.73/s  (0.532s,   37.63/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 700/3456 ( 20%)]  Loss:  0.402103 (0.5218)  Time: 0.547s,   36.57/s  (0.532s,   37.56/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 750/3456 ( 22%)]  Loss:  0.511430 (0.5204)  Time: 0.527s,   37.98/s  (0.533s,   37.55/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 800/3456 ( 23%)]  Loss:  0.569774 (0.5178)  Time: 0.524s,   38.14/s  (0.532s,   37.57/s)  LR: 3.947e-02  Data: 0.010 (0.012)\n",
      "Train: 22 [ 850/3456 ( 25%)]  Loss:  0.407948 (0.5163)  Time: 0.544s,   36.78/s  (0.533s,   37.53/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 900/3456 ( 26%)]  Loss:  0.485189 (0.5153)  Time: 0.547s,   36.54/s  (0.534s,   37.48/s)  LR: 3.947e-02  Data: 0.011 (0.012)\n",
      "Train: 22 [ 950/3456 ( 27%)]  Loss:  0.498205 (0.5138)  Time: 0.543s,   36.85/s  (0.534s,   37.44/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1000/3456 ( 29%)]  Loss:  0.546600 (0.5131)  Time: 0.525s,   38.09/s  (0.534s,   37.46/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1050/3456 ( 30%)]  Loss:  0.489450 (0.5115)  Time: 0.526s,   38.02/s  (0.534s,   37.48/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1100/3456 ( 32%)]  Loss:  0.514783 (0.5108)  Time: 0.526s,   38.02/s  (0.533s,   37.50/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1150/3456 ( 33%)]  Loss:  0.527615 (0.5099)  Time: 0.524s,   38.18/s  (0.533s,   37.52/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1200/3456 ( 35%)]  Loss:  0.533113 (0.5087)  Time: 0.526s,   38.03/s  (0.533s,   37.54/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1250/3456 ( 36%)]  Loss:  0.632724 (0.5077)  Time: 0.527s,   37.98/s  (0.532s,   37.56/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1300/3456 ( 38%)]  Loss:  0.464468 (0.5065)  Time: 0.525s,   38.08/s  (0.532s,   37.58/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1350/3456 ( 39%)]  Loss:  0.566798 (0.5058)  Time: 0.549s,   36.45/s  (0.533s,   37.55/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n",
      "Train: 22 [1400/3456 ( 41%)]  Loss:  0.472104 (0.5052)  Time: 0.525s,   38.09/s  (0.533s,   37.55/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1450/3456 ( 42%)]  Loss:  0.589004 (0.5048)  Time: 0.547s,   36.57/s  (0.533s,   37.55/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1500/3456 ( 43%)]  Loss:  0.510100 (0.5049)  Time: 0.545s,   36.70/s  (0.533s,   37.52/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n",
      "Train: 22 [1550/3456 ( 45%)]  Loss:  0.403887 (0.5043)  Time: 0.542s,   36.87/s  (0.533s,   37.49/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n",
      "Train: 22 [1600/3456 ( 46%)]  Loss:  0.493170 (0.5035)  Time: 0.548s,   36.51/s  (0.534s,   37.46/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1650/3456 ( 48%)]  Loss:  0.407102 (0.5031)  Time: 0.543s,   36.85/s  (0.534s,   37.44/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1700/3456 ( 49%)]  Loss:  0.445979 (0.5029)  Time: 0.544s,   36.79/s  (0.534s,   37.42/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1750/3456 ( 51%)]  Loss:  0.496876 (0.5027)  Time: 0.550s,   36.37/s  (0.535s,   37.40/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1800/3456 ( 52%)]  Loss:  0.611776 (0.5025)  Time: 0.543s,   36.85/s  (0.535s,   37.38/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1850/3456 ( 54%)]  Loss:  0.508544 (0.5015)  Time: 0.544s,   36.75/s  (0.535s,   37.35/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1900/3456 ( 55%)]  Loss:  0.608728 (0.5011)  Time: 0.543s,   36.81/s  (0.536s,   37.34/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [1950/3456 ( 56%)]  Loss:  0.586255 (0.5009)  Time: 0.542s,   36.92/s  (0.536s,   37.32/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2000/3456 ( 58%)]  Loss:  0.457830 (0.5009)  Time: 0.543s,   36.85/s  (0.536s,   37.31/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2050/3456 ( 59%)]  Loss:  0.515703 (0.5003)  Time: 0.523s,   38.24/s  (0.536s,   37.33/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2100/3456 ( 61%)]  Loss:  0.523170 (0.5003)  Time: 0.526s,   37.99/s  (0.536s,   37.34/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2150/3456 ( 62%)]  Loss:  0.514461 (0.5000)  Time: 0.523s,   38.26/s  (0.535s,   37.36/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2200/3456 ( 64%)]  Loss:  0.545350 (0.4995)  Time: 0.522s,   38.29/s  (0.535s,   37.37/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2250/3456 ( 65%)]  Loss:  0.507579 (0.4990)  Time: 0.530s,   37.70/s  (0.535s,   37.38/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2300/3456 ( 67%)]  Loss:  0.502575 (0.4986)  Time: 0.532s,   37.62/s  (0.535s,   37.39/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2350/3456 ( 68%)]  Loss:  0.443967 (0.4981)  Time: 0.522s,   38.28/s  (0.535s,   37.41/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2400/3456 ( 69%)]  Loss:  0.492826 (0.4982)  Time: 0.529s,   37.84/s  (0.534s,   37.42/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2450/3456 ( 71%)]  Loss:  0.527179 (0.4981)  Time: 0.541s,   36.95/s  (0.535s,   37.41/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2500/3456 ( 72%)]  Loss:  0.475737 (0.4979)  Time: 0.542s,   36.92/s  (0.535s,   37.39/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2550/3456 ( 74%)]  Loss:  0.493296 (0.4974)  Time: 0.548s,   36.52/s  (0.535s,   37.38/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2600/3456 ( 75%)]  Loss:  0.407581 (0.4973)  Time: 0.551s,   36.31/s  (0.535s,   37.36/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2650/3456 ( 77%)]  Loss:  0.524613 (0.4972)  Time: 0.541s,   36.95/s  (0.535s,   37.35/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2700/3456 ( 78%)]  Loss:  0.460394 (0.4969)  Time: 0.543s,   36.84/s  (0.536s,   37.34/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2750/3456 ( 80%)]  Loss:  0.467242 (0.4967)  Time: 0.546s,   36.65/s  (0.536s,   37.33/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2800/3456 ( 81%)]  Loss:  0.411606 (0.4964)  Time: 0.524s,   38.18/s  (0.536s,   37.33/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2850/3456 ( 82%)]  Loss:  0.592360 (0.4962)  Time: 0.545s,   36.69/s  (0.536s,   37.32/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2900/3456 ( 84%)]  Loss:  0.563833 (0.4963)  Time: 0.545s,   36.73/s  (0.536s,   37.31/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [2950/3456 ( 85%)]  Loss:  0.552365 (0.4961)  Time: 0.544s,   36.76/s  (0.536s,   37.30/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [3000/3456 ( 87%)]  Loss:  0.402471 (0.4961)  Time: 0.542s,   36.93/s  (0.536s,   37.29/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [3050/3456 ( 88%)]  Loss:  0.412847 (0.4960)  Time: 0.532s,   37.62/s  (0.536s,   37.28/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n",
      "Train: 22 [3100/3456 ( 90%)]  Loss:  0.467049 (0.4956)  Time: 0.527s,   37.98/s  (0.536s,   37.29/s)  LR: 3.947e-02  Data: 0.011 (0.011)\n",
      "Train: 22 [3150/3456 ( 91%)]  Loss:  0.387545 (0.4953)  Time: 0.526s,   38.05/s  (0.536s,   37.30/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n",
      "Train: 22 [3200/3456 ( 93%)]  Loss:  0.421846 (0.4951)  Time: 0.528s,   37.90/s  (0.536s,   37.31/s)  LR: 3.947e-02  Data: 0.012 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 22 [3250/3456 ( 94%)]  Loss:  1.682870 (0.5032)  Time: 0.950s,   21.06/s  (0.540s,   37.02/s)  LR: 3.947e-02  Data: 0.413 (0.015)\n",
      "Train: 22 [3300/3456 ( 96%)]  Loss:  0.965036 (0.5094)  Time: 0.850s,   23.54/s  (0.546s,   36.65/s)  LR: 3.947e-02  Data: 0.317 (0.021)\n",
      "Train: 22 [3350/3456 ( 97%)]  Loss:  0.472618 (0.5126)  Time: 0.876s,   22.84/s  (0.551s,   36.30/s)  LR: 3.947e-02  Data: 0.362 (0.026)\n",
      "Train: 22 [3400/3456 ( 98%)]  Loss:  0.762732 (0.5160)  Time: 0.891s,   22.45/s  (0.556s,   35.98/s)  LR: 3.947e-02  Data: 0.374 (0.031)\n",
      "Train: 22 [3450/3456 (100%)]  Loss:  0.606715 (0.5192)  Time: 0.873s,   22.91/s  (0.561s,   35.67/s)  LR: 3.947e-02  Data: 0.357 (0.036)\n",
      "Train: 22 [3455/3456 (100%)]  Loss:  1.151124 (0.5196)  Time: 0.444s,   24.77/s  (0.561s,   19.61/s)  LR: 3.947e-02  Data: 0.040 (0.036)\n",
      "Test: [   0/147]  Time: 0.863 (0.863)  Loss:  2.1898 (2.1898)  \n",
      "Test: [  50/147]  Time: 0.269 (0.280)  Loss:  2.8257 (2.5613)  \n",
      "Test: [ 100/147]  Time: 0.270 (0.275)  Loss:  2.5494 (2.5581)  \n",
      "Test: [ 147/147]  Time: 0.289 (0.322)  Loss:  1.0311 (2.4183)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-19.pth.tar', 2.476996954429794)\n",
      "\n",
      "Train: 23 [   0/3456 (  0%)]  Loss:  2.483087 (2.4831)  Time: 1.448s,   13.81/s  (1.448s,   13.81/s)  LR: 3.942e-02  Data: 0.824 (0.824)\n",
      "Train: 23 [  50/3456 (  1%)]  Loss:  0.423813 (0.7475)  Time: 0.526s,   38.01/s  (0.545s,   36.71/s)  LR: 3.942e-02  Data: 0.011 (0.027)\n",
      "Train: 23 [ 100/3456 (  3%)]  Loss:  0.597656 (0.6414)  Time: 0.527s,   37.98/s  (0.536s,   37.31/s)  LR: 3.942e-02  Data: 0.011 (0.019)\n",
      "Train: 23 [ 150/3456 (  4%)]  Loss:  0.466244 (0.5938)  Time: 0.528s,   37.91/s  (0.534s,   37.48/s)  LR: 3.942e-02  Data: 0.011 (0.016)\n",
      "Train: 23 [ 200/3456 (  6%)]  Loss:  0.544504 (0.5710)  Time: 0.526s,   38.01/s  (0.532s,   37.57/s)  LR: 3.942e-02  Data: 0.010 (0.015)\n",
      "Train: 23 [ 250/3456 (  7%)]  Loss:  0.501762 (0.5596)  Time: 0.525s,   38.08/s  (0.532s,   37.60/s)  LR: 3.942e-02  Data: 0.011 (0.014)\n",
      "Train: 23 [ 300/3456 (  9%)]  Loss:  0.559751 (0.5500)  Time: 0.525s,   38.11/s  (0.532s,   37.63/s)  LR: 3.942e-02  Data: 0.011 (0.013)\n",
      "Train: 23 [ 350/3456 ( 10%)]  Loss:  0.537989 (0.5437)  Time: 0.528s,   37.85/s  (0.531s,   37.68/s)  LR: 3.942e-02  Data: 0.011 (0.013)\n",
      "Train: 23 [ 400/3456 ( 12%)]  Loss:  0.467253 (0.5370)  Time: 0.524s,   38.15/s  (0.530s,   37.70/s)  LR: 3.942e-02  Data: 0.011 (0.013)\n",
      "Train: 23 [ 450/3456 ( 13%)]  Loss:  0.512493 (0.5344)  Time: 0.525s,   38.06/s  (0.530s,   37.74/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 500/3456 ( 14%)]  Loss:  0.423319 (0.5309)  Time: 0.530s,   37.73/s  (0.530s,   37.76/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 550/3456 ( 16%)]  Loss:  0.481217 (0.5267)  Time: 0.542s,   36.87/s  (0.531s,   37.66/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 600/3456 ( 17%)]  Loss:  0.533100 (0.5236)  Time: 0.546s,   36.60/s  (0.532s,   37.58/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 650/3456 ( 19%)]  Loss:  0.535864 (0.5216)  Time: 0.526s,   38.03/s  (0.532s,   37.59/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 700/3456 ( 20%)]  Loss:  0.434652 (0.5193)  Time: 0.526s,   38.04/s  (0.532s,   37.61/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 750/3456 ( 22%)]  Loss:  0.662941 (0.5178)  Time: 0.543s,   36.83/s  (0.533s,   37.54/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 800/3456 ( 23%)]  Loss:  0.555878 (0.5170)  Time: 0.545s,   36.73/s  (0.533s,   37.49/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 850/3456 ( 25%)]  Loss:  0.491367 (0.5156)  Time: 0.549s,   36.46/s  (0.534s,   37.44/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 900/3456 ( 26%)]  Loss:  0.533476 (0.5152)  Time: 0.546s,   36.60/s  (0.535s,   37.39/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [ 950/3456 ( 27%)]  Loss:  0.498998 (0.5135)  Time: 0.549s,   36.46/s  (0.535s,   37.35/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [1000/3456 ( 29%)]  Loss:  0.407441 (0.5115)  Time: 0.544s,   36.74/s  (0.536s,   37.31/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [1050/3456 ( 30%)]  Loss:  0.421921 (0.5100)  Time: 0.542s,   36.93/s  (0.537s,   37.27/s)  LR: 3.942e-02  Data: 0.012 (0.012)\n",
      "Train: 23 [1100/3456 ( 32%)]  Loss:  0.549397 (0.5098)  Time: 0.525s,   38.12/s  (0.536s,   37.29/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [1150/3456 ( 33%)]  Loss:  0.528514 (0.5088)  Time: 0.524s,   38.16/s  (0.536s,   37.32/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [1200/3456 ( 35%)]  Loss:  0.523418 (0.5081)  Time: 0.532s,   37.63/s  (0.536s,   37.34/s)  LR: 3.942e-02  Data: 0.011 (0.012)\n",
      "Train: 23 [1250/3456 ( 36%)]  Loss:  0.441692 (0.5076)  Time: 0.525s,   38.10/s  (0.535s,   37.36/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1300/3456 ( 38%)]  Loss:  0.505583 (0.5072)  Time: 0.531s,   37.65/s  (0.535s,   37.38/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1350/3456 ( 39%)]  Loss:  0.565745 (0.5063)  Time: 0.527s,   37.98/s  (0.535s,   37.40/s)  LR: 3.942e-02  Data: 0.010 (0.011)\n",
      "Train: 23 [1400/3456 ( 41%)]  Loss:  0.486251 (0.5055)  Time: 0.528s,   37.91/s  (0.535s,   37.42/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1450/3456 ( 42%)]  Loss:  0.537174 (0.5048)  Time: 0.530s,   37.71/s  (0.534s,   37.44/s)  LR: 3.942e-02  Data: 0.010 (0.011)\n",
      "Train: 23 [1500/3456 ( 43%)]  Loss:  0.559487 (0.5048)  Time: 0.523s,   38.24/s  (0.534s,   37.43/s)  LR: 3.942e-02  Data: 0.010 (0.011)\n",
      "Train: 23 [1550/3456 ( 45%)]  Loss:  0.399082 (0.5041)  Time: 0.542s,   36.91/s  (0.534s,   37.44/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1600/3456 ( 46%)]  Loss:  0.477859 (0.5036)  Time: 0.530s,   37.74/s  (0.534s,   37.44/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1650/3456 ( 48%)]  Loss:  0.380175 (0.5032)  Time: 0.541s,   36.96/s  (0.534s,   37.42/s)  LR: 3.942e-02  Data: 0.012 (0.011)\n",
      "Train: 23 [1700/3456 ( 49%)]  Loss:  0.400444 (0.5028)  Time: 0.535s,   37.36/s  (0.535s,   37.42/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1750/3456 ( 51%)]  Loss:  0.534083 (0.5027)  Time: 0.524s,   38.17/s  (0.534s,   37.43/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1800/3456 ( 52%)]  Loss:  0.545279 (0.5026)  Time: 0.544s,   36.75/s  (0.534s,   37.42/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1850/3456 ( 54%)]  Loss:  0.573509 (0.5018)  Time: 0.546s,   36.66/s  (0.535s,   37.40/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1900/3456 ( 55%)]  Loss:  0.569824 (0.5019)  Time: 0.544s,   36.76/s  (0.535s,   37.38/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [1950/3456 ( 56%)]  Loss:  0.533847 (0.5018)  Time: 0.549s,   36.44/s  (0.535s,   37.37/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2000/3456 ( 58%)]  Loss:  0.502306 (0.5018)  Time: 0.550s,   36.37/s  (0.535s,   37.35/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2050/3456 ( 59%)]  Loss:  0.467032 (0.5016)  Time: 0.545s,   36.66/s  (0.536s,   37.33/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2100/3456 ( 61%)]  Loss:  0.447380 (0.5014)  Time: 0.542s,   36.91/s  (0.536s,   37.32/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2150/3456 ( 62%)]  Loss:  0.547380 (0.5013)  Time: 0.542s,   36.87/s  (0.536s,   37.30/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2200/3456 ( 64%)]  Loss:  0.432911 (0.5007)  Time: 0.544s,   36.73/s  (0.536s,   37.29/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2250/3456 ( 65%)]  Loss:  0.495528 (0.5003)  Time: 0.528s,   37.85/s  (0.536s,   37.30/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2300/3456 ( 67%)]  Loss:  0.537855 (0.4999)  Time: 0.531s,   37.66/s  (0.536s,   37.32/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2350/3456 ( 68%)]  Loss:  0.511961 (0.4995)  Time: 0.545s,   36.71/s  (0.536s,   37.33/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2400/3456 ( 69%)]  Loss:  0.488173 (0.4996)  Time: 0.548s,   36.48/s  (0.536s,   37.32/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2450/3456 ( 71%)]  Loss:  0.459263 (0.4994)  Time: 0.540s,   37.06/s  (0.536s,   37.31/s)  LR: 3.942e-02  Data: 0.012 (0.011)\n",
      "Train: 23 [2500/3456 ( 72%)]  Loss:  0.380476 (0.4993)  Time: 0.525s,   38.08/s  (0.536s,   37.31/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2550/3456 ( 74%)]  Loss:  0.464274 (0.4989)  Time: 0.546s,   36.60/s  (0.536s,   37.30/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2600/3456 ( 75%)]  Loss:  0.457904 (0.4987)  Time: 0.545s,   36.71/s  (0.536s,   37.29/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2650/3456 ( 77%)]  Loss:  0.635789 (0.4988)  Time: 0.526s,   38.06/s  (0.536s,   37.30/s)  LR: 3.942e-02  Data: 0.010 (0.011)\n",
      "Train: 23 [2700/3456 ( 78%)]  Loss:  0.572954 (0.4986)  Time: 0.525s,   38.07/s  (0.536s,   37.31/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2750/3456 ( 80%)]  Loss:  0.494454 (0.4987)  Time: 0.525s,   38.07/s  (0.536s,   37.32/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2800/3456 ( 81%)]  Loss:  0.393451 (0.4982)  Time: 0.524s,   38.19/s  (0.536s,   37.33/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2850/3456 ( 82%)]  Loss:  0.549861 (0.4981)  Time: 0.529s,   37.82/s  (0.536s,   37.35/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2900/3456 ( 84%)]  Loss:  0.520833 (0.4981)  Time: 0.526s,   38.00/s  (0.535s,   37.36/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [2950/3456 ( 85%)]  Loss:  0.556852 (0.4979)  Time: 0.524s,   38.19/s  (0.535s,   37.37/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3000/3456 ( 87%)]  Loss:  0.425106 (0.4980)  Time: 0.526s,   37.99/s  (0.535s,   37.38/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3050/3456 ( 88%)]  Loss:  0.373471 (0.4977)  Time: 0.524s,   38.19/s  (0.535s,   37.38/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3100/3456 ( 90%)]  Loss:  0.472250 (0.4975)  Time: 0.545s,   36.67/s  (0.535s,   37.39/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3150/3456 ( 91%)]  Loss:  0.440288 (0.4972)  Time: 0.546s,   36.60/s  (0.535s,   37.37/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3200/3456 ( 93%)]  Loss:  0.453440 (0.4971)  Time: 0.542s,   36.92/s  (0.535s,   37.37/s)  LR: 3.942e-02  Data: 0.011 (0.011)\n",
      "Train: 23 [3250/3456 ( 94%)]  Loss:  1.094125 (0.5016)  Time: 0.914s,   21.88/s  (0.539s,   37.07/s)  LR: 3.942e-02  Data: 0.382 (0.015)\n",
      "Train: 23 [3300/3456 ( 96%)]  Loss:  1.007982 (0.5066)  Time: 0.928s,   21.55/s  (0.545s,   36.69/s)  LR: 3.942e-02  Data: 0.397 (0.021)\n",
      "Train: 23 [3350/3456 ( 97%)]  Loss:  0.983002 (0.5101)  Time: 0.906s,   22.07/s  (0.550s,   36.34/s)  LR: 3.942e-02  Data: 0.372 (0.026)\n",
      "Train: 23 [3400/3456 ( 98%)]  Loss:  0.993790 (0.5139)  Time: 0.866s,   23.10/s  (0.555s,   36.01/s)  LR: 3.942e-02  Data: 0.335 (0.031)\n",
      "Train: 23 [3450/3456 (100%)]  Loss:  0.564392 (0.5166)  Time: 0.875s,   22.86/s  (0.560s,   35.69/s)  LR: 3.942e-02  Data: 0.341 (0.036)\n",
      "Train: 23 [3455/3456 (100%)]  Loss:  1.016442 (0.5168)  Time: 0.462s,   23.83/s  (0.561s,   19.62/s)  LR: 3.942e-02  Data: 0.038 (0.036)\n",
      "Test: [   0/147]  Time: 0.878 (0.878)  Loss:  2.4137 (2.4137)  \n",
      "Test: [  50/147]  Time: 0.279 (0.292)  Loss:  2.8531 (2.6560)  \n",
      "Test: [ 100/147]  Time: 0.278 (0.287)  Loss:  2.6121 (2.6478)  \n",
      "Test: [ 147/147]  Time: 0.299 (0.333)  Loss:  0.6896 (2.4690)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-23.pth.tar', 2.469002184634273)\n",
      "\n",
      "Train: 24 [   0/3456 (  0%)]  Loss:  2.903614 (2.9036)  Time: 1.458s,   13.72/s  (1.458s,   13.72/s)  LR: 3.937e-02  Data: 0.826 (0.826)\n",
      "Train: 24 [  50/3456 (  1%)]  Loss:  0.381488 (0.8637)  Time: 0.542s,   36.91/s  (0.561s,   35.63/s)  LR: 3.937e-02  Data: 0.011 (0.028)\n",
      "Train: 24 [ 100/3456 (  3%)]  Loss:  0.618884 (0.7110)  Time: 0.542s,   36.92/s  (0.554s,   36.08/s)  LR: 3.937e-02  Data: 0.011 (0.019)\n",
      "Train: 24 [ 150/3456 (  4%)]  Loss:  0.499934 (0.6422)  Time: 0.543s,   36.84/s  (0.551s,   36.33/s)  LR: 3.937e-02  Data: 0.011 (0.017)\n",
      "Train: 24 [ 200/3456 (  6%)]  Loss:  0.514942 (0.6088)  Time: 0.544s,   36.78/s  (0.548s,   36.46/s)  LR: 3.937e-02  Data: 0.011 (0.015)\n",
      "Train: 24 [ 250/3456 (  7%)]  Loss:  0.509767 (0.5882)  Time: 0.540s,   37.01/s  (0.547s,   36.53/s)  LR: 3.937e-02  Data: 0.011 (0.014)\n",
      "Train: 24 [ 300/3456 (  9%)]  Loss:  0.588046 (0.5771)  Time: 0.546s,   36.62/s  (0.546s,   36.65/s)  LR: 3.937e-02  Data: 0.011 (0.014)\n",
      "Train: 24 [ 350/3456 ( 10%)]  Loss:  0.537913 (0.5668)  Time: 0.526s,   38.06/s  (0.545s,   36.71/s)  LR: 3.937e-02  Data: 0.011 (0.013)\n",
      "Train: 24 [ 400/3456 ( 12%)]  Loss:  0.504121 (0.5577)  Time: 0.525s,   38.07/s  (0.542s,   36.87/s)  LR: 3.937e-02  Data: 0.011 (0.013)\n",
      "Train: 24 [ 450/3456 ( 13%)]  Loss:  0.518758 (0.5529)  Time: 0.525s,   38.06/s  (0.541s,   37.00/s)  LR: 3.937e-02  Data: 0.011 (0.013)\n",
      "Train: 24 [ 500/3456 ( 14%)]  Loss:  0.441312 (0.5478)  Time: 0.526s,   38.04/s  (0.539s,   37.10/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 550/3456 ( 16%)]  Loss:  0.446936 (0.5418)  Time: 0.526s,   38.01/s  (0.538s,   37.16/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 600/3456 ( 17%)]  Loss:  0.465866 (0.5380)  Time: 0.526s,   37.99/s  (0.537s,   37.23/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 650/3456 ( 19%)]  Loss:  0.519241 (0.5352)  Time: 0.524s,   38.17/s  (0.536s,   37.29/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 700/3456 ( 20%)]  Loss:  0.456679 (0.5324)  Time: 0.544s,   36.78/s  (0.536s,   37.31/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 750/3456 ( 22%)]  Loss:  0.500633 (0.5299)  Time: 0.541s,   36.99/s  (0.537s,   37.27/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 800/3456 ( 23%)]  Loss:  0.550313 (0.5275)  Time: 0.541s,   36.94/s  (0.537s,   37.24/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 850/3456 ( 25%)]  Loss:  0.466475 (0.5259)  Time: 0.541s,   36.94/s  (0.537s,   37.21/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 900/3456 ( 26%)]  Loss:  0.483505 (0.5247)  Time: 0.547s,   36.56/s  (0.538s,   37.19/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [ 950/3456 ( 27%)]  Loss:  0.514120 (0.5232)  Time: 0.528s,   37.90/s  (0.538s,   37.19/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [1000/3456 ( 29%)]  Loss:  0.525314 (0.5215)  Time: 0.529s,   37.79/s  (0.537s,   37.23/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [1050/3456 ( 30%)]  Loss:  0.472080 (0.5202)  Time: 0.531s,   37.67/s  (0.537s,   37.27/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [1100/3456 ( 32%)]  Loss:  0.515573 (0.5193)  Time: 0.531s,   37.64/s  (0.536s,   37.29/s)  LR: 3.937e-02  Data: 0.011 (0.012)\n",
      "Train: 24 [1150/3456 ( 33%)]  Loss:  0.489912 (0.5178)  Time: 0.530s,   37.73/s  (0.536s,   37.32/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1200/3456 ( 35%)]  Loss:  0.521405 (0.5168)  Time: 0.533s,   37.52/s  (0.536s,   37.34/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1250/3456 ( 36%)]  Loss:  0.548213 (0.5159)  Time: 0.529s,   37.83/s  (0.535s,   37.37/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1300/3456 ( 38%)]  Loss:  0.496463 (0.5147)  Time: 0.525s,   38.11/s  (0.535s,   37.39/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1350/3456 ( 39%)]  Loss:  0.501776 (0.5134)  Time: 0.524s,   38.15/s  (0.535s,   37.40/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1400/3456 ( 41%)]  Loss:  0.464663 (0.5123)  Time: 0.529s,   37.83/s  (0.534s,   37.43/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1450/3456 ( 42%)]  Loss:  0.617498 (0.5115)  Time: 0.526s,   38.04/s  (0.534s,   37.45/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1500/3456 ( 43%)]  Loss:  0.499775 (0.5111)  Time: 0.523s,   38.25/s  (0.534s,   37.47/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1550/3456 ( 45%)]  Loss:  0.449638 (0.5107)  Time: 0.526s,   38.05/s  (0.533s,   37.49/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1600/3456 ( 46%)]  Loss:  0.560531 (0.5098)  Time: 0.525s,   38.08/s  (0.533s,   37.50/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1650/3456 ( 48%)]  Loss:  0.430913 (0.5095)  Time: 0.532s,   37.60/s  (0.533s,   37.52/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1700/3456 ( 49%)]  Loss:  0.430438 (0.5092)  Time: 0.532s,   37.59/s  (0.533s,   37.53/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1750/3456 ( 51%)]  Loss:  0.482497 (0.5090)  Time: 0.524s,   38.15/s  (0.533s,   37.54/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1800/3456 ( 52%)]  Loss:  0.559207 (0.5089)  Time: 0.524s,   38.16/s  (0.533s,   37.56/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1850/3456 ( 54%)]  Loss:  0.471481 (0.5084)  Time: 0.528s,   37.91/s  (0.532s,   37.57/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1900/3456 ( 55%)]  Loss:  0.500688 (0.5079)  Time: 0.526s,   38.02/s  (0.532s,   37.58/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [1950/3456 ( 56%)]  Loss:  0.507186 (0.5074)  Time: 0.525s,   38.08/s  (0.532s,   37.59/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2000/3456 ( 58%)]  Loss:  0.525369 (0.5073)  Time: 0.526s,   38.05/s  (0.532s,   37.60/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2050/3456 ( 59%)]  Loss:  0.459640 (0.5069)  Time: 0.525s,   38.08/s  (0.532s,   37.61/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2100/3456 ( 61%)]  Loss:  0.450177 (0.5067)  Time: 0.524s,   38.19/s  (0.532s,   37.62/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2150/3456 ( 62%)]  Loss:  0.493502 (0.5062)  Time: 0.527s,   37.94/s  (0.532s,   37.63/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2200/3456 ( 64%)]  Loss:  0.395605 (0.5053)  Time: 0.526s,   38.05/s  (0.531s,   37.63/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2250/3456 ( 65%)]  Loss:  0.486940 (0.5048)  Time: 0.525s,   38.12/s  (0.531s,   37.64/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2300/3456 ( 67%)]  Loss:  0.428585 (0.5041)  Time: 0.525s,   38.11/s  (0.531s,   37.65/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2350/3456 ( 68%)]  Loss:  0.504903 (0.5038)  Time: 0.524s,   38.14/s  (0.531s,   37.65/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2400/3456 ( 69%)]  Loss:  0.540729 (0.5038)  Time: 0.526s,   38.02/s  (0.531s,   37.65/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2450/3456 ( 71%)]  Loss:  0.432945 (0.5036)  Time: 0.523s,   38.21/s  (0.531s,   37.66/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2500/3456 ( 72%)]  Loss:  0.395048 (0.5036)  Time: 0.530s,   37.73/s  (0.531s,   37.66/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2550/3456 ( 74%)]  Loss:  0.447058 (0.5032)  Time: 0.530s,   37.76/s  (0.531s,   37.67/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2600/3456 ( 75%)]  Loss:  0.445621 (0.5030)  Time: 0.523s,   38.21/s  (0.531s,   37.67/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2650/3456 ( 77%)]  Loss:  0.550474 (0.5030)  Time: 0.532s,   37.60/s  (0.531s,   37.67/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2700/3456 ( 78%)]  Loss:  0.471258 (0.5026)  Time: 0.524s,   38.15/s  (0.531s,   37.68/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2750/3456 ( 80%)]  Loss:  0.517619 (0.5025)  Time: 0.546s,   36.60/s  (0.531s,   37.67/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2800/3456 ( 81%)]  Loss:  0.469026 (0.5024)  Time: 0.544s,   36.75/s  (0.531s,   37.65/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2850/3456 ( 82%)]  Loss:  0.636701 (0.5021)  Time: 0.548s,   36.47/s  (0.531s,   37.63/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2900/3456 ( 84%)]  Loss:  0.585473 (0.5021)  Time: 0.530s,   37.77/s  (0.532s,   37.62/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [2950/3456 ( 85%)]  Loss:  0.567790 (0.5019)  Time: 0.549s,   36.45/s  (0.532s,   37.60/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [3000/3456 ( 87%)]  Loss:  0.437180 (0.5018)  Time: 0.549s,   36.46/s  (0.532s,   37.59/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [3050/3456 ( 88%)]  Loss:  0.409352 (0.5017)  Time: 0.547s,   36.57/s  (0.532s,   37.57/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [3100/3456 ( 90%)]  Loss:  0.492449 (0.5013)  Time: 0.542s,   36.92/s  (0.533s,   37.55/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [3150/3456 ( 91%)]  Loss:  0.490804 (0.5010)  Time: 0.525s,   38.09/s  (0.533s,   37.54/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n",
      "Train: 24 [3200/3456 ( 93%)]  Loss:  0.484928 (0.5008)  Time: 0.526s,   38.03/s  (0.533s,   37.55/s)  LR: 3.937e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 24 [3250/3456 ( 94%)]  Loss:  1.242921 (0.5074)  Time: 0.971s,   20.59/s  (0.537s,   37.26/s)  LR: 3.937e-02  Data: 0.455 (0.015)\n",
      "Train: 24 [3300/3456 ( 96%)]  Loss:  1.386028 (0.5114)  Time: 0.908s,   22.02/s  (0.543s,   36.86/s)  LR: 3.937e-02  Data: 0.376 (0.021)\n",
      "Train: 24 [3350/3456 ( 97%)]  Loss:  0.721920 (0.5159)  Time: 0.899s,   22.25/s  (0.548s,   36.51/s)  LR: 3.937e-02  Data: 0.364 (0.026)\n",
      "Train: 24 [3400/3456 ( 98%)]  Loss:  1.057033 (0.5202)  Time: 0.912s,   21.92/s  (0.553s,   36.17/s)  LR: 3.937e-02  Data: 0.381 (0.031)\n",
      "Train: 24 [3450/3456 (100%)]  Loss:  0.669990 (0.5233)  Time: 0.882s,   22.69/s  (0.558s,   35.86/s)  LR: 3.937e-02  Data: 0.361 (0.036)\n",
      "Train: 24 [3455/3456 (100%)]  Loss:  1.052240 (0.5236)  Time: 0.449s,   24.50/s  (0.558s,   19.72/s)  LR: 3.937e-02  Data: 0.039 (0.036)\n",
      "Test: [   0/147]  Time: 0.861 (0.861)  Loss:  2.2081 (2.2081)  \n",
      "Test: [  50/147]  Time: 0.264 (0.279)  Loss:  2.6300 (2.5101)  \n",
      "Test: [ 100/147]  Time: 0.264 (0.272)  Loss:  2.4630 (2.5052)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.321)  Loss:  0.7515 (2.3681)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-24.pth.tar', 2.3681411847874925)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-12.pth.tar', 2.464974844777906)\n",
      "\n",
      "Train: 25 [   0/3456 (  0%)]  Loss:  2.525288 (2.5253)  Time: 1.429s,   14.00/s  (1.429s,   14.00/s)  LR: 3.932e-02  Data: 0.813 (0.813)\n",
      "Train: 25 [  50/3456 (  1%)]  Loss:  0.388419 (0.7431)  Time: 0.548s,   36.52/s  (0.562s,   35.57/s)  LR: 3.932e-02  Data: 0.011 (0.027)\n",
      "Train: 25 [ 100/3456 (  3%)]  Loss:  0.590346 (0.6399)  Time: 0.545s,   36.73/s  (0.553s,   36.14/s)  LR: 3.932e-02  Data: 0.011 (0.019)\n",
      "Train: 25 [ 150/3456 (  4%)]  Loss:  0.550858 (0.5958)  Time: 0.523s,   38.28/s  (0.546s,   36.62/s)  LR: 3.932e-02  Data: 0.011 (0.016)\n",
      "Train: 25 [ 200/3456 (  6%)]  Loss:  0.519139 (0.5726)  Time: 0.541s,   36.93/s  (0.544s,   36.80/s)  LR: 3.932e-02  Data: 0.011 (0.015)\n",
      "Train: 25 [ 250/3456 (  7%)]  Loss:  0.472955 (0.5604)  Time: 0.542s,   36.88/s  (0.543s,   36.81/s)  LR: 3.932e-02  Data: 0.011 (0.014)\n",
      "Train: 25 [ 300/3456 (  9%)]  Loss:  0.546798 (0.5504)  Time: 0.544s,   36.75/s  (0.544s,   36.76/s)  LR: 3.932e-02  Data: 0.011 (0.014)\n",
      "Train: 25 [ 350/3456 ( 10%)]  Loss:  0.576932 (0.5438)  Time: 0.543s,   36.86/s  (0.544s,   36.77/s)  LR: 3.932e-02  Data: 0.011 (0.013)\n",
      "Train: 25 [ 400/3456 ( 12%)]  Loss:  0.496723 (0.5389)  Time: 0.545s,   36.73/s  (0.544s,   36.78/s)  LR: 3.932e-02  Data: 0.011 (0.013)\n",
      "Train: 25 [ 450/3456 ( 13%)]  Loss:  0.573385 (0.5368)  Time: 0.544s,   36.77/s  (0.544s,   36.77/s)  LR: 3.932e-02  Data: 0.011 (0.013)\n",
      "Train: 25 [ 500/3456 ( 14%)]  Loss:  0.488151 (0.5327)  Time: 0.543s,   36.85/s  (0.544s,   36.78/s)  LR: 3.932e-02  Data: 0.011 (0.013)\n",
      "Train: 25 [ 550/3456 ( 16%)]  Loss:  0.512090 (0.5281)  Time: 0.528s,   37.91/s  (0.542s,   36.88/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 600/3456 ( 17%)]  Loss:  0.534908 (0.5250)  Time: 0.529s,   37.79/s  (0.541s,   36.94/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 650/3456 ( 19%)]  Loss:  0.637179 (0.5230)  Time: 0.530s,   37.73/s  (0.540s,   37.03/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 700/3456 ( 20%)]  Loss:  0.420734 (0.5210)  Time: 0.522s,   38.32/s  (0.539s,   37.08/s)  LR: 3.932e-02  Data: 0.010 (0.012)\n",
      "Train: 25 [ 750/3456 ( 22%)]  Loss:  0.633320 (0.5196)  Time: 0.540s,   37.01/s  (0.539s,   37.11/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 800/3456 ( 23%)]  Loss:  0.592068 (0.5183)  Time: 0.541s,   36.94/s  (0.539s,   37.09/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 850/3456 ( 25%)]  Loss:  0.456366 (0.5174)  Time: 0.542s,   36.92/s  (0.540s,   37.07/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [ 900/3456 ( 26%)]  Loss:  0.511892 (0.5168)  Time: 0.526s,   38.00/s  (0.539s,   37.07/s)  LR: 3.932e-02  Data: 0.010 (0.012)\n",
      "Train: 25 [ 950/3456 ( 27%)]  Loss:  0.544215 (0.5152)  Time: 0.530s,   37.75/s  (0.539s,   37.12/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [1000/3456 ( 29%)]  Loss:  0.492946 (0.5144)  Time: 0.525s,   38.11/s  (0.538s,   37.16/s)  LR: 3.932e-02  Data: 0.010 (0.012)\n",
      "Train: 25 [1050/3456 ( 30%)]  Loss:  0.524822 (0.5139)  Time: 0.527s,   37.95/s  (0.538s,   37.17/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [1100/3456 ( 32%)]  Loss:  0.542456 (0.5134)  Time: 0.523s,   38.24/s  (0.538s,   37.20/s)  LR: 3.932e-02  Data: 0.011 (0.012)\n",
      "Train: 25 [1150/3456 ( 33%)]  Loss:  0.431226 (0.5124)  Time: 0.523s,   38.24/s  (0.537s,   37.23/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1200/3456 ( 35%)]  Loss:  0.529019 (0.5115)  Time: 0.529s,   37.84/s  (0.537s,   37.26/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1250/3456 ( 36%)]  Loss:  0.499524 (0.5105)  Time: 0.543s,   36.80/s  (0.537s,   37.27/s)  LR: 3.932e-02  Data: 0.012 (0.011)\n",
      "Train: 25 [1300/3456 ( 38%)]  Loss:  0.464187 (0.5091)  Time: 0.544s,   36.76/s  (0.537s,   37.25/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1350/3456 ( 39%)]  Loss:  0.469607 (0.5080)  Time: 0.549s,   36.43/s  (0.537s,   37.22/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1400/3456 ( 41%)]  Loss:  0.439318 (0.5072)  Time: 0.544s,   36.79/s  (0.538s,   37.20/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1450/3456 ( 42%)]  Loss:  0.576055 (0.5065)  Time: 0.541s,   36.95/s  (0.538s,   37.19/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1500/3456 ( 43%)]  Loss:  0.517517 (0.5064)  Time: 0.547s,   36.54/s  (0.538s,   37.16/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1550/3456 ( 45%)]  Loss:  0.392068 (0.5056)  Time: 0.541s,   36.95/s  (0.538s,   37.17/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1600/3456 ( 46%)]  Loss:  0.525621 (0.5048)  Time: 0.544s,   36.74/s  (0.538s,   37.15/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1650/3456 ( 48%)]  Loss:  0.418579 (0.5042)  Time: 0.543s,   36.82/s  (0.539s,   37.14/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1700/3456 ( 49%)]  Loss:  0.402939 (0.5038)  Time: 0.548s,   36.47/s  (0.539s,   37.12/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1750/3456 ( 51%)]  Loss:  0.488807 (0.5036)  Time: 0.545s,   36.72/s  (0.539s,   37.10/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1800/3456 ( 52%)]  Loss:  0.532514 (0.5036)  Time: 0.548s,   36.50/s  (0.539s,   37.09/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1850/3456 ( 54%)]  Loss:  0.471098 (0.5029)  Time: 0.548s,   36.52/s  (0.539s,   37.08/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1900/3456 ( 55%)]  Loss:  0.552723 (0.5024)  Time: 0.541s,   36.94/s  (0.540s,   37.07/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [1950/3456 ( 56%)]  Loss:  0.479949 (0.5020)  Time: 0.549s,   36.46/s  (0.540s,   37.06/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2000/3456 ( 58%)]  Loss:  0.477708 (0.5022)  Time: 0.526s,   37.99/s  (0.540s,   37.06/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2050/3456 ( 59%)]  Loss:  0.494930 (0.5019)  Time: 0.525s,   38.07/s  (0.539s,   37.08/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2100/3456 ( 61%)]  Loss:  0.498983 (0.5018)  Time: 0.527s,   37.93/s  (0.539s,   37.11/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2150/3456 ( 62%)]  Loss:  0.460935 (0.5012)  Time: 0.523s,   38.20/s  (0.539s,   37.12/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2200/3456 ( 64%)]  Loss:  0.436474 (0.5005)  Time: 0.547s,   36.57/s  (0.539s,   37.12/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2250/3456 ( 65%)]  Loss:  0.524486 (0.5000)  Time: 0.527s,   37.95/s  (0.539s,   37.14/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2300/3456 ( 67%)]  Loss:  0.551661 (0.4996)  Time: 0.525s,   38.07/s  (0.538s,   37.16/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2350/3456 ( 68%)]  Loss:  0.461057 (0.4994)  Time: 0.523s,   38.21/s  (0.538s,   37.17/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2400/3456 ( 69%)]  Loss:  0.477689 (0.4995)  Time: 0.525s,   38.12/s  (0.538s,   37.19/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2450/3456 ( 71%)]  Loss:  0.564767 (0.4997)  Time: 0.542s,   36.88/s  (0.538s,   37.19/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2500/3456 ( 72%)]  Loss:  0.416134 (0.4995)  Time: 0.541s,   36.97/s  (0.538s,   37.18/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2550/3456 ( 74%)]  Loss:  0.485040 (0.4988)  Time: 0.528s,   37.88/s  (0.538s,   37.19/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2600/3456 ( 75%)]  Loss:  0.522758 (0.4986)  Time: 0.526s,   37.99/s  (0.538s,   37.20/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2650/3456 ( 77%)]  Loss:  0.553062 (0.4985)  Time: 0.525s,   38.06/s  (0.537s,   37.22/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2700/3456 ( 78%)]  Loss:  0.554097 (0.4984)  Time: 0.526s,   38.02/s  (0.537s,   37.23/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2750/3456 ( 80%)]  Loss:  0.435919 (0.4983)  Time: 0.525s,   38.06/s  (0.537s,   37.25/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2800/3456 ( 81%)]  Loss:  0.469679 (0.4979)  Time: 0.526s,   38.02/s  (0.537s,   37.26/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2850/3456 ( 82%)]  Loss:  0.534849 (0.4978)  Time: 0.527s,   37.98/s  (0.537s,   37.27/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2900/3456 ( 84%)]  Loss:  0.499403 (0.4977)  Time: 0.525s,   38.07/s  (0.536s,   37.28/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [2950/3456 ( 85%)]  Loss:  0.466603 (0.4975)  Time: 0.528s,   37.90/s  (0.536s,   37.29/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [3000/3456 ( 87%)]  Loss:  0.459222 (0.4975)  Time: 0.527s,   37.96/s  (0.536s,   37.30/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [3050/3456 ( 88%)]  Loss:  0.384384 (0.4972)  Time: 0.530s,   37.76/s  (0.536s,   37.31/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [3100/3456 ( 90%)]  Loss:  0.459643 (0.4971)  Time: 0.529s,   37.81/s  (0.536s,   37.32/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n",
      "Train: 25 [3150/3456 ( 91%)]  Loss:  0.429693 (0.4969)  Time: 0.547s,   36.55/s  (0.536s,   37.32/s)  LR: 3.932e-02  Data: 0.012 (0.011)\n",
      "Train: 25 [3200/3456 ( 93%)]  Loss:  0.442078 (0.4967)  Time: 0.549s,   36.42/s  (0.536s,   37.31/s)  LR: 3.932e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 25 [3250/3456 ( 94%)]  Loss:  1.461923 (0.5019)  Time: 0.949s,   21.08/s  (0.540s,   37.03/s)  LR: 3.932e-02  Data: 0.416 (0.015)\n",
      "Train: 25 [3300/3456 ( 96%)]  Loss:  1.129283 (0.5064)  Time: 0.893s,   22.40/s  (0.546s,   36.64/s)  LR: 3.932e-02  Data: 0.357 (0.021)\n",
      "Train: 25 [3450/3456 (100%)]  Loss:  0.465153 (0.5161)  Time: 0.863s,   23.18/s  (0.561s,   35.65/s)  LR: 3.932e-02  Data: 0.332 (0.036)\n",
      "Train: 25 [3455/3456 (100%)]  Loss:  1.086889 (0.5165)  Time: 0.459s,   23.97/s  (0.561s,   19.60/s)  LR: 3.932e-02  Data: 0.039 (0.036)\n",
      "Test: [   0/147]  Time: 0.875 (0.875)  Loss:  2.2564 (2.2564)  \n",
      "Test: [  50/147]  Time: 0.282 (0.293)  Loss:  2.7964 (2.5586)  \n",
      "Test: [ 100/147]  Time: 0.280 (0.288)  Loss:  2.6203 (2.5515)  \n",
      "Test: [ 147/147]  Time: 0.303 (0.333)  Loss:  1.2456 (2.4258)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-24.pth.tar', 2.3681411847874925)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-25.pth.tar', 2.425751063872028)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-18.pth.tar', 2.464192874125532)\n",
      "\n",
      "Train: 26 [   0/3456 (  0%)]  Loss:  2.274111 (2.2741)  Time: 1.432s,   13.96/s  (1.432s,   13.96/s)  LR: 3.926e-02  Data: 0.797 (0.797)\n",
      "Train: 26 [  50/3456 (  1%)]  Loss:  0.476902 (0.7106)  Time: 0.541s,   36.96/s  (0.561s,   35.64/s)  LR: 3.926e-02  Data: 0.011 (0.027)\n",
      "Train: 26 [ 100/3456 (  3%)]  Loss:  0.537168 (0.6248)  Time: 0.525s,   38.11/s  (0.552s,   36.21/s)  LR: 3.926e-02  Data: 0.011 (0.019)\n",
      "Train: 26 [ 150/3456 (  4%)]  Loss:  0.507878 (0.5848)  Time: 0.522s,   38.31/s  (0.545s,   36.69/s)  LR: 3.926e-02  Data: 0.011 (0.016)\n",
      "Train: 26 [ 200/3456 (  6%)]  Loss:  0.539194 (0.5622)  Time: 0.524s,   38.17/s  (0.540s,   37.03/s)  LR: 3.926e-02  Data: 0.011 (0.015)\n",
      "Train: 26 [ 250/3456 (  7%)]  Loss:  0.552583 (0.5543)  Time: 0.523s,   38.26/s  (0.537s,   37.24/s)  LR: 3.926e-02  Data: 0.011 (0.014)\n",
      "Train: 26 [ 300/3456 (  9%)]  Loss:  0.592069 (0.5463)  Time: 0.525s,   38.10/s  (0.535s,   37.38/s)  LR: 3.926e-02  Data: 0.011 (0.013)\n",
      "Train: 26 [ 350/3456 ( 10%)]  Loss:  0.565954 (0.5395)  Time: 0.526s,   38.05/s  (0.534s,   37.45/s)  LR: 3.926e-02  Data: 0.011 (0.013)\n",
      "Train: 26 [ 400/3456 ( 12%)]  Loss:  0.493886 (0.5334)  Time: 0.694s,   28.83/s  (0.534s,   37.48/s)  LR: 3.926e-02  Data: 0.011 (0.013)\n",
      "Train: 26 [ 450/3456 ( 13%)]  Loss:  0.425040 (0.5310)  Time: 0.526s,   38.05/s  (0.533s,   37.53/s)  LR: 3.926e-02  Data: 0.011 (0.013)\n",
      "Train: 26 [ 500/3456 ( 14%)]  Loss:  0.488567 (0.5283)  Time: 0.526s,   38.05/s  (0.532s,   37.58/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 550/3456 ( 16%)]  Loss:  0.515150 (0.5250)  Time: 0.526s,   38.00/s  (0.532s,   37.62/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 600/3456 ( 17%)]  Loss:  0.461226 (0.5226)  Time: 0.526s,   38.03/s  (0.532s,   37.63/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 650/3456 ( 19%)]  Loss:  0.559804 (0.5203)  Time: 0.525s,   38.11/s  (0.531s,   37.65/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 700/3456 ( 20%)]  Loss:  0.469089 (0.5188)  Time: 0.530s,   37.72/s  (0.531s,   37.67/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 750/3456 ( 22%)]  Loss:  0.537620 (0.5179)  Time: 0.545s,   36.68/s  (0.531s,   37.67/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 800/3456 ( 23%)]  Loss:  0.539950 (0.5170)  Time: 0.543s,   36.84/s  (0.532s,   37.62/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 850/3456 ( 25%)]  Loss:  0.408437 (0.5155)  Time: 0.544s,   36.76/s  (0.533s,   37.55/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 900/3456 ( 26%)]  Loss:  0.509252 (0.5152)  Time: 0.524s,   38.17/s  (0.533s,   37.53/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [ 950/3456 ( 27%)]  Loss:  0.512463 (0.5140)  Time: 0.525s,   38.12/s  (0.533s,   37.56/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [1000/3456 ( 29%)]  Loss:  0.548718 (0.5129)  Time: 0.525s,   38.08/s  (0.532s,   37.58/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [1050/3456 ( 30%)]  Loss:  0.564230 (0.5124)  Time: 0.525s,   38.10/s  (0.532s,   37.61/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [1100/3456 ( 32%)]  Loss:  0.446876 (0.5115)  Time: 0.525s,   38.12/s  (0.532s,   37.63/s)  LR: 3.926e-02  Data: 0.011 (0.012)\n",
      "Train: 26 [1150/3456 ( 33%)]  Loss:  0.421800 (0.5107)  Time: 0.525s,   38.13/s  (0.531s,   37.65/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1200/3456 ( 35%)]  Loss:  0.485202 (0.5101)  Time: 0.525s,   38.09/s  (0.531s,   37.67/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1250/3456 ( 36%)]  Loss:  0.482248 (0.5091)  Time: 0.524s,   38.16/s  (0.531s,   37.67/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1300/3456 ( 38%)]  Loss:  0.514755 (0.5081)  Time: 0.524s,   38.13/s  (0.531s,   37.69/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1350/3456 ( 39%)]  Loss:  0.468107 (0.5072)  Time: 0.525s,   38.07/s  (0.530s,   37.71/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1400/3456 ( 41%)]  Loss:  0.542559 (0.5062)  Time: 0.542s,   36.89/s  (0.531s,   37.68/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1450/3456 ( 42%)]  Loss:  0.608651 (0.5057)  Time: 0.546s,   36.63/s  (0.531s,   37.65/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1500/3456 ( 43%)]  Loss:  0.447806 (0.5059)  Time: 0.541s,   36.96/s  (0.532s,   37.62/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1550/3456 ( 45%)]  Loss:  0.396546 (0.5054)  Time: 0.542s,   36.88/s  (0.532s,   37.59/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1600/3456 ( 46%)]  Loss:  0.445911 (0.5046)  Time: 0.548s,   36.50/s  (0.532s,   37.56/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1650/3456 ( 48%)]  Loss:  0.371347 (0.5040)  Time: 0.541s,   36.94/s  (0.533s,   37.53/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1700/3456 ( 49%)]  Loss:  0.451373 (0.5038)  Time: 0.542s,   36.88/s  (0.533s,   37.51/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1750/3456 ( 51%)]  Loss:  0.519359 (0.5035)  Time: 0.542s,   36.91/s  (0.533s,   37.49/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1800/3456 ( 52%)]  Loss:  0.513569 (0.5033)  Time: 0.546s,   36.64/s  (0.534s,   37.47/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1850/3456 ( 54%)]  Loss:  0.476006 (0.5026)  Time: 0.548s,   36.50/s  (0.534s,   37.45/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1900/3456 ( 55%)]  Loss:  0.544757 (0.5025)  Time: 0.545s,   36.72/s  (0.534s,   37.43/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [1950/3456 ( 56%)]  Loss:  0.547312 (0.5023)  Time: 0.526s,   38.02/s  (0.534s,   37.43/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2000/3456 ( 58%)]  Loss:  0.503427 (0.5021)  Time: 0.525s,   38.08/s  (0.534s,   37.45/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2050/3456 ( 59%)]  Loss:  0.493434 (0.5018)  Time: 0.546s,   36.64/s  (0.534s,   37.44/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2100/3456 ( 61%)]  Loss:  0.501930 (0.5018)  Time: 0.544s,   36.79/s  (0.534s,   37.42/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2150/3456 ( 62%)]  Loss:  0.484679 (0.5012)  Time: 0.543s,   36.83/s  (0.535s,   37.40/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2200/3456 ( 64%)]  Loss:  0.388265 (0.5004)  Time: 0.527s,   37.95/s  (0.535s,   37.39/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2250/3456 ( 65%)]  Loss:  0.493880 (0.4999)  Time: 0.550s,   36.39/s  (0.535s,   37.37/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2500/3456 ( 72%)]  Loss:  0.393217 (0.4994)  Time: 0.550s,   36.38/s  (0.536s,   37.29/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2550/3456 ( 74%)]  Loss:  0.426454 (0.4989)  Time: 0.545s,   36.70/s  (0.537s,   37.28/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2600/3456 ( 75%)]  Loss:  0.464034 (0.4987)  Time: 0.544s,   36.74/s  (0.537s,   37.27/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2650/3456 ( 77%)]  Loss:  0.533008 (0.4987)  Time: 0.549s,   36.44/s  (0.537s,   37.25/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2700/3456 ( 78%)]  Loss:  0.467644 (0.4984)  Time: 0.545s,   36.71/s  (0.537s,   37.24/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2750/3456 ( 80%)]  Loss:  0.469318 (0.4982)  Time: 0.542s,   36.91/s  (0.537s,   37.23/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2800/3456 ( 81%)]  Loss:  0.434916 (0.4981)  Time: 0.546s,   36.64/s  (0.537s,   37.22/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2850/3456 ( 82%)]  Loss:  0.528145 (0.4979)  Time: 0.542s,   36.93/s  (0.537s,   37.21/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2900/3456 ( 84%)]  Loss:  0.539765 (0.4978)  Time: 0.544s,   36.77/s  (0.538s,   37.21/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [2950/3456 ( 85%)]  Loss:  0.600639 (0.4977)  Time: 0.541s,   36.99/s  (0.538s,   37.20/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [3000/3456 ( 87%)]  Loss:  0.454898 (0.4976)  Time: 0.546s,   36.66/s  (0.538s,   37.19/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [3050/3456 ( 88%)]  Loss:  0.405559 (0.4974)  Time: 0.545s,   36.72/s  (0.538s,   37.18/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [3100/3456 ( 90%)]  Loss:  0.594100 (0.4974)  Time: 0.546s,   36.60/s  (0.538s,   37.18/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [3150/3456 ( 91%)]  Loss:  0.458576 (0.4971)  Time: 0.547s,   36.55/s  (0.538s,   37.17/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n",
      "Train: 26 [3200/3456 ( 93%)]  Loss:  0.467544 (0.4971)  Time: 0.524s,   38.15/s  (0.538s,   37.18/s)  LR: 3.926e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 26 [3250/3456 ( 94%)]  Loss:  0.753591 (0.5103)  Time: 0.956s,   20.92/s  (0.542s,   36.89/s)  LR: 3.926e-02  Data: 0.420 (0.015)\n",
      "Train: 26 [3300/3456 ( 96%)]  Loss:  1.190778 (0.5172)  Time: 0.864s,   23.15/s  (0.548s,   36.51/s)  LR: 3.926e-02  Data: 0.329 (0.021)\n",
      "Train: 26 [3350/3456 ( 97%)]  Loss:  0.799374 (0.5213)  Time: 0.882s,   22.68/s  (0.553s,   36.17/s)  LR: 3.926e-02  Data: 0.348 (0.026)\n",
      "Train: 26 [3400/3456 ( 98%)]  Loss:  1.112008 (0.5257)  Time: 0.877s,   22.80/s  (0.558s,   35.84/s)  LR: 3.926e-02  Data: 0.345 (0.031)\n",
      "Train: 26 [3450/3456 (100%)]  Loss:  0.464418 (0.5283)  Time: 0.898s,   22.28/s  (0.563s,   35.53/s)  LR: 3.926e-02  Data: 0.362 (0.036)\n",
      "Train: 26 [3455/3456 (100%)]  Loss:  1.182837 (0.5286)  Time: 0.458s,   24.02/s  (0.563s,   19.53/s)  LR: 3.926e-02  Data: 0.039 (0.036)\n",
      "Test: [   0/147]  Time: 0.874 (0.874)  Loss:  2.3736 (2.3736)  \n",
      "Test: [  50/147]  Time: 0.265 (0.287)  Loss:  2.8396 (2.6564)  \n",
      "Test: [ 100/147]  Time: 0.271 (0.278)  Loss:  2.6718 (2.6524)  \n",
      "Test: [ 147/147]  Time: 0.299 (0.325)  Loss:  1.0831 (2.5067)  \n",
      "Train: 27 [   0/3456 (  0%)]  Loss:  2.566456 (2.5665)  Time: 1.458s,   13.72/s  (1.458s,   13.72/s)  LR: 3.921e-02  Data: 0.828 (0.828)\n",
      "Train: 27 [  50/3456 (  1%)]  Loss:  0.483285 (0.7928)  Time: 0.523s,   38.21/s  (0.558s,   35.82/s)  LR: 3.921e-02  Data: 0.011 (0.027)\n",
      "Train: 27 [ 100/3456 (  3%)]  Loss:  0.562574 (0.6725)  Time: 0.530s,   37.76/s  (0.542s,   36.91/s)  LR: 3.921e-02  Data: 0.011 (0.019)\n",
      "Train: 27 [ 150/3456 (  4%)]  Loss:  0.532239 (0.6235)  Time: 0.529s,   37.79/s  (0.536s,   37.30/s)  LR: 3.921e-02  Data: 0.011 (0.016)\n",
      "Train: 27 [ 200/3456 (  6%)]  Loss:  0.511879 (0.5983)  Time: 0.527s,   37.98/s  (0.534s,   37.48/s)  LR: 3.921e-02  Data: 0.010 (0.015)\n",
      "Train: 27 [ 250/3456 (  7%)]  Loss:  0.502117 (0.5822)  Time: 0.525s,   38.12/s  (0.532s,   37.60/s)  LR: 3.921e-02  Data: 0.011 (0.014)\n",
      "Train: 27 [ 300/3456 (  9%)]  Loss:  0.550951 (0.5713)  Time: 0.545s,   36.70/s  (0.531s,   37.65/s)  LR: 3.921e-02  Data: 0.011 (0.013)\n",
      "Train: 27 [ 350/3456 ( 10%)]  Loss:  0.481274 (0.5610)  Time: 0.522s,   38.34/s  (0.533s,   37.56/s)  LR: 3.921e-02  Data: 0.010 (0.013)\n",
      "Train: 27 [ 400/3456 ( 12%)]  Loss:  0.470681 (0.5547)  Time: 0.526s,   38.01/s  (0.532s,   37.61/s)  LR: 3.921e-02  Data: 0.011 (0.013)\n",
      "Train: 27 [ 450/3456 ( 13%)]  Loss:  0.449855 (0.5501)  Time: 0.541s,   36.97/s  (0.533s,   37.50/s)  LR: 3.921e-02  Data: 0.011 (0.013)\n",
      "Train: 27 [ 500/3456 ( 14%)]  Loss:  0.544140 (0.5443)  Time: 0.524s,   38.19/s  (0.533s,   37.51/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 550/3456 ( 16%)]  Loss:  0.513459 (0.5393)  Time: 0.524s,   38.20/s  (0.533s,   37.55/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 600/3456 ( 17%)]  Loss:  0.527391 (0.5361)  Time: 0.531s,   37.67/s  (0.532s,   37.60/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 650/3456 ( 19%)]  Loss:  0.694486 (0.5352)  Time: 0.524s,   38.17/s  (0.531s,   37.63/s)  LR: 3.921e-02  Data: 0.010 (0.012)\n",
      "Train: 27 [ 700/3456 ( 20%)]  Loss:  0.412129 (0.5321)  Time: 0.522s,   38.31/s  (0.531s,   37.64/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 750/3456 ( 22%)]  Loss:  0.534743 (0.5306)  Time: 0.523s,   38.26/s  (0.531s,   37.68/s)  LR: 3.921e-02  Data: 0.010 (0.012)\n",
      "Train: 27 [ 800/3456 ( 23%)]  Loss:  0.538998 (0.5288)  Time: 0.527s,   37.97/s  (0.531s,   37.70/s)  LR: 3.921e-02  Data: 0.010 (0.012)\n",
      "Train: 27 [ 850/3456 ( 25%)]  Loss:  0.549738 (0.5265)  Time: 0.528s,   37.90/s  (0.530s,   37.72/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 900/3456 ( 26%)]  Loss:  0.474834 (0.5258)  Time: 0.526s,   38.02/s  (0.530s,   37.74/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [ 950/3456 ( 27%)]  Loss:  0.498013 (0.5239)  Time: 0.542s,   36.87/s  (0.530s,   37.71/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [1000/3456 ( 29%)]  Loss:  0.453389 (0.5224)  Time: 0.526s,   38.06/s  (0.530s,   37.70/s)  LR: 3.921e-02  Data: 0.011 (0.012)\n",
      "Train: 27 [1050/3456 ( 30%)]  Loss:  0.502223 (0.5213)  Time: 0.534s,   37.46/s  (0.530s,   37.72/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1100/3456 ( 32%)]  Loss:  0.515259 (0.5204)  Time: 0.550s,   36.33/s  (0.530s,   37.72/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1150/3456 ( 33%)]  Loss:  0.425681 (0.5187)  Time: 0.526s,   38.00/s  (0.530s,   37.73/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1200/3456 ( 35%)]  Loss:  0.580586 (0.5180)  Time: 0.525s,   38.12/s  (0.530s,   37.75/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1250/3456 ( 36%)]  Loss:  0.574948 (0.5173)  Time: 0.526s,   38.03/s  (0.530s,   37.76/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1300/3456 ( 38%)]  Loss:  0.621967 (0.5162)  Time: 0.527s,   37.99/s  (0.530s,   37.76/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1550/3456 ( 45%)]  Loss:  0.382972 (0.5132)  Time: 0.527s,   37.96/s  (0.530s,   37.73/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1600/3456 ( 46%)]  Loss:  0.516648 (0.5123)  Time: 0.526s,   38.00/s  (0.530s,   37.73/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1650/3456 ( 48%)]  Loss:  0.370075 (0.5117)  Time: 0.547s,   36.54/s  (0.530s,   37.72/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1700/3456 ( 49%)]  Loss:  0.416167 (0.5111)  Time: 0.544s,   36.77/s  (0.531s,   37.69/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1750/3456 ( 51%)]  Loss:  0.474502 (0.5108)  Time: 0.545s,   36.71/s  (0.531s,   37.66/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1800/3456 ( 52%)]  Loss:  0.509873 (0.5106)  Time: 0.540s,   37.06/s  (0.531s,   37.63/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1850/3456 ( 54%)]  Loss:  0.475211 (0.5095)  Time: 0.543s,   36.85/s  (0.532s,   37.61/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1900/3456 ( 55%)]  Loss:  0.528461 (0.5093)  Time: 0.545s,   36.69/s  (0.532s,   37.59/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [1950/3456 ( 56%)]  Loss:  0.532336 (0.5089)  Time: 0.543s,   36.82/s  (0.532s,   37.57/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2000/3456 ( 58%)]  Loss:  0.536795 (0.5088)  Time: 0.551s,   36.30/s  (0.533s,   37.54/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2050/3456 ( 59%)]  Loss:  0.499086 (0.5084)  Time: 0.543s,   36.81/s  (0.533s,   37.52/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2100/3456 ( 61%)]  Loss:  0.512324 (0.5083)  Time: 0.542s,   36.91/s  (0.533s,   37.50/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2150/3456 ( 62%)]  Loss:  0.493580 (0.5080)  Time: 0.714s,   28.00/s  (0.534s,   37.47/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2200/3456 ( 64%)]  Loss:  0.430042 (0.5070)  Time: 0.542s,   36.90/s  (0.534s,   37.45/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2250/3456 ( 65%)]  Loss:  0.520328 (0.5065)  Time: 0.543s,   36.82/s  (0.534s,   37.43/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2300/3456 ( 67%)]  Loss:  0.481919 (0.5058)  Time: 0.548s,   36.52/s  (0.535s,   37.42/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2350/3456 ( 68%)]  Loss:  0.472614 (0.5056)  Time: 0.549s,   36.40/s  (0.535s,   37.40/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2400/3456 ( 69%)]  Loss:  0.583579 (0.5056)  Time: 0.550s,   36.39/s  (0.535s,   37.38/s)  LR: 3.921e-02  Data: 0.012 (0.011)\n",
      "Train: 27 [2450/3456 ( 71%)]  Loss:  0.414287 (0.5052)  Time: 0.544s,   36.74/s  (0.535s,   37.36/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2500/3456 ( 72%)]  Loss:  0.509935 (0.5052)  Time: 0.543s,   36.84/s  (0.536s,   37.35/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2550/3456 ( 74%)]  Loss:  0.474905 (0.5047)  Time: 0.550s,   36.38/s  (0.536s,   37.34/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2600/3456 ( 75%)]  Loss:  0.513634 (0.5045)  Time: 0.549s,   36.44/s  (0.536s,   37.32/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2650/3456 ( 77%)]  Loss:  0.523748 (0.5043)  Time: 0.546s,   36.64/s  (0.536s,   37.31/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2700/3456 ( 78%)]  Loss:  0.477388 (0.5041)  Time: 0.548s,   36.49/s  (0.536s,   37.30/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2750/3456 ( 80%)]  Loss:  0.521345 (0.5039)  Time: 0.544s,   36.77/s  (0.536s,   37.29/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2800/3456 ( 81%)]  Loss:  0.356741 (0.5037)  Time: 0.542s,   36.87/s  (0.537s,   37.28/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2850/3456 ( 82%)]  Loss:  0.596534 (0.5037)  Time: 0.547s,   36.55/s  (0.537s,   37.26/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2900/3456 ( 84%)]  Loss:  0.542474 (0.5035)  Time: 0.527s,   37.98/s  (0.537s,   37.26/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [2950/3456 ( 85%)]  Loss:  0.535205 (0.5033)  Time: 0.528s,   37.91/s  (0.537s,   37.27/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3000/3456 ( 87%)]  Loss:  0.461057 (0.5029)  Time: 0.528s,   37.86/s  (0.536s,   37.28/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3050/3456 ( 88%)]  Loss:  0.470304 (0.5026)  Time: 0.526s,   38.05/s  (0.536s,   37.29/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3100/3456 ( 90%)]  Loss:  0.467942 (0.5024)  Time: 0.525s,   38.13/s  (0.536s,   37.31/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3150/3456 ( 91%)]  Loss:  0.500919 (0.5022)  Time: 0.531s,   37.70/s  (0.536s,   37.32/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3200/3456 ( 93%)]  Loss:  0.409696 (0.5021)  Time: 0.528s,   37.90/s  (0.536s,   37.33/s)  LR: 3.921e-02  Data: 0.011 (0.011)\n",
      "Train: 27 [3250/3456 ( 94%)]  Loss:  1.243126 (0.5067)  Time: 0.945s,   21.16/s  (0.540s,   37.04/s)  LR: 3.921e-02  Data: 0.430 (0.015)\n",
      "Train: 27 [3300/3456 ( 96%)]  Loss:  1.021564 (0.5116)  Time: 0.906s,   22.08/s  (0.546s,   36.66/s)  LR: 3.921e-02  Data: 0.388 (0.021)\n",
      "Train: 27 [3350/3456 ( 97%)]  Loss:  0.836347 (0.5153)  Time: 0.880s,   22.74/s  (0.551s,   36.31/s)  LR: 3.921e-02  Data: 0.367 (0.027)\n",
      "Train: 27 [3400/3456 ( 98%)]  Loss:  0.794224 (0.5183)  Time: 0.903s,   22.16/s  (0.556s,   35.99/s)  LR: 3.921e-02  Data: 0.382 (0.032)\n",
      "Train: 27 [3450/3456 (100%)]  Loss:  0.548892 (0.5207)  Time: 0.904s,   22.11/s  (0.561s,   35.67/s)  LR: 3.921e-02  Data: 0.367 (0.037)\n",
      "Train: 27 [3455/3456 (100%)]  Loss:  1.190112 (0.5212)  Time: 0.459s,   23.97/s  (0.561s,   19.61/s)  LR: 3.921e-02  Data: 0.038 (0.037)\n",
      "Test: [   0/147]  Time: 0.879 (0.879)  Loss:  2.4932 (2.4932)  \n",
      "Test: [  50/147]  Time: 0.264 (0.279)  Loss:  2.9229 (2.7181)  \n",
      "Test: [ 100/147]  Time: 0.284 (0.277)  Loss:  2.7114 (2.7099)  \n",
      "Test: [ 147/147]  Time: 0.303 (0.326)  Loss:  2.0954 (2.6400)  \n",
      "Train: 28 [   0/3456 (  0%)]  Loss:  2.482634 (2.4826)  Time: 1.479s,   13.52/s  (1.479s,   13.52/s)  LR: 3.915e-02  Data: 0.848 (0.848)\n",
      "Train: 28 [  50/3456 (  1%)]  Loss:  0.453707 (0.8084)  Time: 0.526s,   38.03/s  (0.548s,   36.49/s)  LR: 3.915e-02  Data: 0.011 (0.028)\n",
      "Train: 28 [ 100/3456 (  3%)]  Loss:  0.555051 (0.6797)  Time: 0.524s,   38.13/s  (0.537s,   37.23/s)  LR: 3.915e-02  Data: 0.011 (0.019)\n",
      "Train: 28 [ 150/3456 (  4%)]  Loss:  0.441926 (0.6228)  Time: 0.524s,   38.18/s  (0.535s,   37.38/s)  LR: 3.915e-02  Data: 0.011 (0.016)\n",
      "Train: 28 [ 200/3456 (  6%)]  Loss:  0.635597 (0.5950)  Time: 0.525s,   38.12/s  (0.534s,   37.48/s)  LR: 3.915e-02  Data: 0.011 (0.015)\n",
      "Train: 28 [ 250/3456 (  7%)]  Loss:  0.512864 (0.5798)  Time: 0.548s,   36.47/s  (0.535s,   37.40/s)  LR: 3.915e-02  Data: 0.011 (0.014)\n",
      "Train: 28 [ 300/3456 (  9%)]  Loss:  0.540564 (0.5666)  Time: 0.543s,   36.81/s  (0.537s,   37.28/s)  LR: 3.915e-02  Data: 0.011 (0.014)\n",
      "Train: 28 [ 350/3456 ( 10%)]  Loss:  0.563993 (0.5572)  Time: 0.548s,   36.48/s  (0.538s,   37.20/s)  LR: 3.915e-02  Data: 0.011 (0.013)\n",
      "Train: 28 [ 400/3456 ( 12%)]  Loss:  0.489261 (0.5505)  Time: 0.547s,   36.57/s  (0.537s,   37.22/s)  LR: 3.915e-02  Data: 0.011 (0.013)\n",
      "Train: 28 [ 450/3456 ( 13%)]  Loss:  0.573489 (0.5462)  Time: 0.545s,   36.69/s  (0.538s,   37.17/s)  LR: 3.915e-02  Data: 0.011 (0.013)\n",
      "Train: 28 [ 500/3456 ( 14%)]  Loss:  0.508501 (0.5412)  Time: 0.527s,   37.98/s  (0.537s,   37.24/s)  LR: 3.915e-02  Data: 0.011 (0.013)\n",
      "Train: 28 [ 550/3456 ( 16%)]  Loss:  0.519335 (0.5376)  Time: 0.526s,   38.04/s  (0.536s,   37.31/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [ 600/3456 ( 17%)]  Loss:  0.519800 (0.5347)  Time: 0.524s,   38.18/s  (0.536s,   37.34/s)  LR: 3.915e-02  Data: 0.010 (0.012)\n",
      "Train: 28 [ 650/3456 ( 19%)]  Loss:  0.531089 (0.5323)  Time: 0.523s,   38.25/s  (0.535s,   37.40/s)  LR: 3.915e-02  Data: 0.010 (0.012)\n",
      "Train: 28 [ 700/3456 ( 20%)]  Loss:  0.528245 (0.5283)  Time: 0.524s,   38.19/s  (0.534s,   37.45/s)  LR: 3.915e-02  Data: 0.010 (0.012)\n",
      "Train: 28 [ 750/3456 ( 22%)]  Loss:  0.585207 (0.5263)  Time: 0.522s,   38.30/s  (0.533s,   37.50/s)  LR: 3.915e-02  Data: 0.010 (0.012)\n",
      "Train: 28 [ 800/3456 ( 23%)]  Loss:  0.541702 (0.5243)  Time: 0.529s,   37.83/s  (0.533s,   37.53/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [ 850/3456 ( 25%)]  Loss:  0.560798 (0.5230)  Time: 0.523s,   38.21/s  (0.533s,   37.56/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [ 900/3456 ( 26%)]  Loss:  0.492049 (0.5225)  Time: 0.526s,   38.02/s  (0.532s,   37.59/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [ 950/3456 ( 27%)]  Loss:  0.492569 (0.5204)  Time: 0.524s,   38.14/s  (0.532s,   37.60/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [1000/3456 ( 29%)]  Loss:  0.536691 (0.5189)  Time: 0.528s,   37.85/s  (0.532s,   37.62/s)  LR: 3.915e-02  Data: 0.010 (0.012)\n",
      "Train: 28 [1050/3456 ( 30%)]  Loss:  0.550115 (0.5184)  Time: 0.524s,   38.19/s  (0.531s,   37.64/s)  LR: 3.915e-02  Data: 0.011 (0.012)\n",
      "Train: 28 [1100/3456 ( 32%)]  Loss:  0.507875 (0.5180)  Time: 0.546s,   36.65/s  (0.532s,   37.60/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1150/3456 ( 33%)]  Loss:  0.492652 (0.5167)  Time: 0.542s,   36.92/s  (0.532s,   37.56/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1200/3456 ( 35%)]  Loss:  0.488425 (0.5155)  Time: 0.541s,   36.97/s  (0.533s,   37.53/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1250/3456 ( 36%)]  Loss:  0.539833 (0.5145)  Time: 0.541s,   37.00/s  (0.533s,   37.50/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1300/3456 ( 38%)]  Loss:  0.478403 (0.5131)  Time: 0.542s,   36.92/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1350/3456 ( 39%)]  Loss:  0.540781 (0.5123)  Time: 0.530s,   37.76/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1400/3456 ( 41%)]  Loss:  0.512447 (0.5115)  Time: 0.543s,   36.86/s  (0.534s,   37.45/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1450/3456 ( 42%)]  Loss:  0.607171 (0.5111)  Time: 0.526s,   37.99/s  (0.534s,   37.43/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1500/3456 ( 43%)]  Loss:  0.550349 (0.5107)  Time: 0.540s,   37.04/s  (0.534s,   37.44/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1550/3456 ( 45%)]  Loss:  0.436912 (0.5103)  Time: 0.548s,   36.52/s  (0.535s,   37.41/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1600/3456 ( 46%)]  Loss:  0.463718 (0.5098)  Time: 0.528s,   37.88/s  (0.534s,   37.42/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1650/3456 ( 48%)]  Loss:  0.396817 (0.5092)  Time: 0.528s,   37.91/s  (0.534s,   37.44/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1700/3456 ( 49%)]  Loss:  0.432572 (0.5089)  Time: 0.524s,   38.17/s  (0.534s,   37.46/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1750/3456 ( 51%)]  Loss:  0.572585 (0.5088)  Time: 0.527s,   37.98/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1800/3456 ( 52%)]  Loss:  0.511396 (0.5083)  Time: 0.530s,   37.73/s  (0.534s,   37.48/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1850/3456 ( 54%)]  Loss:  0.474854 (0.5074)  Time: 0.544s,   36.80/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.012 (0.011)\n",
      "Train: 28 [1900/3456 ( 55%)]  Loss:  0.544846 (0.5072)  Time: 0.527s,   37.92/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [1950/3456 ( 56%)]  Loss:  0.501400 (0.5066)  Time: 0.525s,   38.09/s  (0.534s,   37.48/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2000/3456 ( 58%)]  Loss:  0.421091 (0.5064)  Time: 0.532s,   37.61/s  (0.533s,   37.49/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2050/3456 ( 59%)]  Loss:  0.560098 (0.5062)  Time: 0.525s,   38.10/s  (0.533s,   37.49/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2100/3456 ( 61%)]  Loss:  0.453775 (0.5059)  Time: 0.532s,   37.62/s  (0.533s,   37.50/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2150/3456 ( 62%)]  Loss:  0.487000 (0.5053)  Time: 0.526s,   38.04/s  (0.533s,   37.51/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2200/3456 ( 64%)]  Loss:  0.390288 (0.5045)  Time: 0.529s,   37.84/s  (0.533s,   37.52/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2250/3456 ( 65%)]  Loss:  0.507257 (0.5041)  Time: 0.529s,   37.84/s  (0.533s,   37.53/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2300/3456 ( 67%)]  Loss:  0.559084 (0.5035)  Time: 0.547s,   36.56/s  (0.533s,   37.51/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2350/3456 ( 68%)]  Loss:  0.513095 (0.5032)  Time: 0.548s,   36.52/s  (0.533s,   37.49/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2400/3456 ( 69%)]  Loss:  0.513230 (0.5032)  Time: 0.548s,   36.52/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2450/3456 ( 71%)]  Loss:  0.437821 (0.5033)  Time: 0.547s,   36.59/s  (0.534s,   37.45/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2500/3456 ( 72%)]  Loss:  0.367945 (0.5031)  Time: 0.542s,   36.91/s  (0.534s,   37.44/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2550/3456 ( 74%)]  Loss:  0.504598 (0.5027)  Time: 0.542s,   36.91/s  (0.534s,   37.42/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2600/3456 ( 75%)]  Loss:  0.461937 (0.5023)  Time: 0.529s,   37.80/s  (0.534s,   37.43/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2650/3456 ( 77%)]  Loss:  0.541817 (0.5022)  Time: 0.528s,   37.90/s  (0.534s,   37.44/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2700/3456 ( 78%)]  Loss:  0.472267 (0.5021)  Time: 0.543s,   36.84/s  (0.534s,   37.43/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2750/3456 ( 80%)]  Loss:  0.481063 (0.5019)  Time: 0.526s,   38.03/s  (0.534s,   37.44/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2800/3456 ( 81%)]  Loss:  0.459216 (0.5018)  Time: 0.526s,   38.04/s  (0.534s,   37.45/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2850/3456 ( 82%)]  Loss:  0.565558 (0.5014)  Time: 0.525s,   38.11/s  (0.534s,   37.46/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2900/3456 ( 84%)]  Loss:  0.558256 (0.5014)  Time: 0.525s,   38.11/s  (0.534s,   37.47/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [2950/3456 ( 85%)]  Loss:  0.551459 (0.5011)  Time: 0.524s,   38.17/s  (0.534s,   37.48/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [3000/3456 ( 87%)]  Loss:  0.442092 (0.5012)  Time: 0.525s,   38.08/s  (0.534s,   37.49/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [3050/3456 ( 88%)]  Loss:  0.433706 (0.5007)  Time: 0.524s,   38.14/s  (0.533s,   37.50/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [3100/3456 ( 90%)]  Loss:  0.510460 (0.5006)  Time: 0.526s,   38.04/s  (0.533s,   37.50/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [3150/3456 ( 91%)]  Loss:  0.464768 (0.5003)  Time: 0.525s,   38.08/s  (0.533s,   37.51/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n",
      "Train: 28 [3200/3456 ( 93%)]  Loss:  0.449964 (0.4999)  Time: 0.526s,   38.02/s  (0.533s,   37.52/s)  LR: 3.915e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 28 [3250/3456 ( 94%)]  Loss:  1.344269 (0.5066)  Time: 0.932s,   21.46/s  (0.537s,   37.22/s)  LR: 3.915e-02  Data: 0.393 (0.015)\n",
      "Train: 28 [3300/3456 ( 96%)]  Loss:  0.738269 (0.5110)  Time: 0.894s,   22.37/s  (0.543s,   36.83/s)  LR: 3.915e-02  Data: 0.379 (0.021)\n",
      "Train: 28 [3350/3456 ( 97%)]  Loss:  1.040966 (0.5141)  Time: 0.897s,   22.29/s  (0.548s,   36.48/s)  LR: 3.915e-02  Data: 0.380 (0.026)\n",
      "Train: 28 [3400/3456 ( 98%)]  Loss:  0.634341 (0.5172)  Time: 0.913s,   21.90/s  (0.553s,   36.14/s)  LR: 3.915e-02  Data: 0.400 (0.032)\n",
      "Train: 28 [3450/3456 (100%)]  Loss:  0.703478 (0.5190)  Time: 0.919s,   21.76/s  (0.558s,   35.82/s)  LR: 3.915e-02  Data: 0.382 (0.037)\n",
      "Train: 28 [3455/3456 (100%)]  Loss:  1.239910 (0.5194)  Time: 0.466s,   23.58/s  (0.559s,   19.69/s)  LR: 3.915e-02  Data: 0.040 (0.037)\n",
      "Test: [   0/147]  Time: 0.881 (0.881)  Loss:  2.3905 (2.3905)  \n",
      "Test: [  50/147]  Time: 0.267 (0.284)  Loss:  2.8269 (2.6553)  \n",
      "Test: [ 100/147]  Time: 0.276 (0.277)  Loss:  2.7704 (2.6508)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.322)  Loss:  0.5714 (2.4604)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-24.pth.tar', 2.3681411847874925)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-25.pth.tar', 2.425751063872028)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-28.pth.tar', 2.4603959478035167)\n",
      "\n",
      "Train: 29 [   0/3456 (  0%)]  Loss:  2.361445 (2.3614)  Time: 1.440s,   13.89/s  (1.440s,   13.89/s)  LR: 3.909e-02  Data: 0.818 (0.818)\n",
      "Train: 29 [  50/3456 (  1%)]  Loss:  0.470230 (0.7417)  Time: 0.542s,   36.88/s  (0.551s,   36.27/s)  LR: 3.909e-02  Data: 0.012 (0.027)\n",
      "Train: 29 [ 100/3456 (  3%)]  Loss:  0.565844 (0.6460)  Time: 0.543s,   36.86/s  (0.547s,   36.54/s)  LR: 3.909e-02  Data: 0.012 (0.019)\n",
      "Train: 29 [ 150/3456 (  4%)]  Loss:  0.547166 (0.6008)  Time: 0.544s,   36.74/s  (0.546s,   36.64/s)  LR: 3.909e-02  Data: 0.011 (0.017)\n",
      "Train: 29 [ 200/3456 (  6%)]  Loss:  0.543389 (0.5770)  Time: 0.527s,   37.92/s  (0.544s,   36.75/s)  LR: 3.909e-02  Data: 0.011 (0.015)\n",
      "Train: 29 [ 250/3456 (  7%)]  Loss:  0.505227 (0.5631)  Time: 0.528s,   37.87/s  (0.540s,   37.01/s)  LR: 3.909e-02  Data: 0.011 (0.014)\n",
      "Train: 29 [ 300/3456 (  9%)]  Loss:  0.562478 (0.5542)  Time: 0.524s,   38.17/s  (0.539s,   37.13/s)  LR: 3.909e-02  Data: 0.011 (0.014)\n",
      "Train: 29 [ 350/3456 ( 10%)]  Loss:  0.498337 (0.5471)  Time: 0.542s,   36.91/s  (0.538s,   37.16/s)  LR: 3.909e-02  Data: 0.011 (0.013)\n",
      "Train: 29 [ 400/3456 ( 12%)]  Loss:  0.458973 (0.5413)  Time: 0.543s,   36.84/s  (0.539s,   37.10/s)  LR: 3.909e-02  Data: 0.011 (0.013)\n",
      "Train: 29 [ 450/3456 ( 13%)]  Loss:  0.506489 (0.5385)  Time: 0.543s,   36.82/s  (0.540s,   37.06/s)  LR: 3.909e-02  Data: 0.011 (0.013)\n",
      "Train: 29 [ 500/3456 ( 14%)]  Loss:  0.478571 (0.5341)  Time: 0.549s,   36.45/s  (0.540s,   37.03/s)  LR: 3.909e-02  Data: 0.011 (0.013)\n",
      "Train: 29 [ 550/3456 ( 16%)]  Loss:  0.475901 (0.5304)  Time: 0.550s,   36.39/s  (0.541s,   37.00/s)  LR: 3.909e-02  Data: 0.011 (0.013)\n",
      "Train: 29 [ 600/3456 ( 17%)]  Loss:  0.504855 (0.5282)  Time: 0.541s,   36.98/s  (0.541s,   36.99/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 650/3456 ( 19%)]  Loss:  0.598392 (0.5256)  Time: 0.540s,   37.05/s  (0.541s,   36.97/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 700/3456 ( 20%)]  Loss:  0.425091 (0.5234)  Time: 0.544s,   36.79/s  (0.541s,   36.95/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 750/3456 ( 22%)]  Loss:  0.531005 (0.5217)  Time: 0.544s,   36.79/s  (0.542s,   36.93/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 800/3456 ( 23%)]  Loss:  0.578228 (0.5207)  Time: 0.530s,   37.76/s  (0.541s,   36.97/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 850/3456 ( 25%)]  Loss:  0.481893 (0.5196)  Time: 0.543s,   36.84/s  (0.541s,   36.99/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 900/3456 ( 26%)]  Loss:  0.414879 (0.5186)  Time: 0.528s,   37.85/s  (0.540s,   37.00/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [ 950/3456 ( 27%)]  Loss:  0.560376 (0.5172)  Time: 0.529s,   37.80/s  (0.540s,   37.05/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1000/3456 ( 29%)]  Loss:  0.572792 (0.5163)  Time: 0.525s,   38.11/s  (0.539s,   37.10/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1050/3456 ( 30%)]  Loss:  0.443713 (0.5149)  Time: 0.525s,   38.13/s  (0.538s,   37.14/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1100/3456 ( 32%)]  Loss:  0.458891 (0.5145)  Time: 0.524s,   38.16/s  (0.538s,   37.18/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1150/3456 ( 33%)]  Loss:  0.460766 (0.5131)  Time: 0.524s,   38.18/s  (0.538s,   37.20/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1200/3456 ( 35%)]  Loss:  0.514127 (0.5124)  Time: 0.527s,   37.94/s  (0.537s,   37.24/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1250/3456 ( 36%)]  Loss:  0.484428 (0.5115)  Time: 0.524s,   38.15/s  (0.537s,   37.27/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1300/3456 ( 38%)]  Loss:  0.478128 (0.5105)  Time: 0.548s,   36.49/s  (0.537s,   37.28/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1350/3456 ( 39%)]  Loss:  0.511007 (0.5099)  Time: 0.550s,   36.37/s  (0.537s,   37.26/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1400/3456 ( 41%)]  Loss:  0.457532 (0.5088)  Time: 0.543s,   36.85/s  (0.537s,   37.24/s)  LR: 3.909e-02  Data: 0.011 (0.012)\n",
      "Train: 29 [1450/3456 ( 42%)]  Loss:  0.588039 (0.5084)  Time: 0.539s,   37.08/s  (0.537s,   37.22/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1500/3456 ( 43%)]  Loss:  0.475092 (0.5083)  Time: 0.541s,   36.94/s  (0.538s,   37.20/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1550/3456 ( 45%)]  Loss:  0.378552 (0.5080)  Time: 0.544s,   36.77/s  (0.538s,   37.18/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1600/3456 ( 46%)]  Loss:  0.477896 (0.5071)  Time: 0.543s,   36.86/s  (0.538s,   37.17/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1650/3456 ( 48%)]  Loss:  0.400149 (0.5071)  Time: 0.526s,   37.99/s  (0.538s,   37.16/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1700/3456 ( 49%)]  Loss:  0.363210 (0.5065)  Time: 0.521s,   38.35/s  (0.538s,   37.19/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1750/3456 ( 51%)]  Loss:  0.529129 (0.5063)  Time: 0.530s,   37.74/s  (0.537s,   37.21/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1800/3456 ( 52%)]  Loss:  0.473936 (0.5060)  Time: 0.528s,   37.87/s  (0.537s,   37.23/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1850/3456 ( 54%)]  Loss:  0.547243 (0.5054)  Time: 0.527s,   37.97/s  (0.537s,   37.25/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1900/3456 ( 55%)]  Loss:  0.543449 (0.5053)  Time: 0.525s,   38.08/s  (0.537s,   37.28/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [1950/3456 ( 56%)]  Loss:  0.481063 (0.5052)  Time: 0.523s,   38.26/s  (0.536s,   37.30/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2000/3456 ( 58%)]  Loss:  0.497873 (0.5053)  Time: 0.525s,   38.10/s  (0.536s,   37.32/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2050/3456 ( 59%)]  Loss:  0.511812 (0.5051)  Time: 0.525s,   38.09/s  (0.536s,   37.34/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2100/3456 ( 61%)]  Loss:  0.514776 (0.5050)  Time: 0.524s,   38.19/s  (0.535s,   37.35/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2150/3456 ( 62%)]  Loss:  0.495077 (0.5045)  Time: 0.530s,   37.74/s  (0.535s,   37.37/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2200/3456 ( 64%)]  Loss:  0.453609 (0.5037)  Time: 0.524s,   38.18/s  (0.535s,   37.38/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2250/3456 ( 65%)]  Loss:  0.554995 (0.5034)  Time: 0.529s,   37.84/s  (0.535s,   37.40/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2300/3456 ( 67%)]  Loss:  0.482539 (0.5028)  Time: 0.530s,   37.76/s  (0.535s,   37.41/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2350/3456 ( 68%)]  Loss:  0.450980 (0.5024)  Time: 0.525s,   38.10/s  (0.534s,   37.42/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2400/3456 ( 69%)]  Loss:  0.497621 (0.5023)  Time: 0.528s,   37.90/s  (0.534s,   37.44/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2450/3456 ( 71%)]  Loss:  0.561763 (0.5023)  Time: 0.524s,   38.19/s  (0.534s,   37.45/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2500/3456 ( 72%)]  Loss:  0.409681 (0.5019)  Time: 0.523s,   38.21/s  (0.534s,   37.46/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2550/3456 ( 74%)]  Loss:  0.468422 (0.5014)  Time: 0.524s,   38.14/s  (0.534s,   37.47/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2600/3456 ( 75%)]  Loss:  0.457340 (0.5011)  Time: 0.524s,   38.18/s  (0.534s,   37.48/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2650/3456 ( 77%)]  Loss:  0.508816 (0.5012)  Time: 0.524s,   38.17/s  (0.533s,   37.49/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2700/3456 ( 78%)]  Loss:  0.418690 (0.5009)  Time: 0.545s,   36.72/s  (0.533s,   37.50/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2750/3456 ( 80%)]  Loss:  0.541476 (0.5007)  Time: 0.525s,   38.10/s  (0.534s,   37.49/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2800/3456 ( 81%)]  Loss:  0.396498 (0.5005)  Time: 0.529s,   37.81/s  (0.533s,   37.49/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2850/3456 ( 82%)]  Loss:  0.477078 (0.5002)  Time: 0.524s,   38.17/s  (0.533s,   37.50/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2900/3456 ( 84%)]  Loss:  0.561997 (0.5002)  Time: 0.524s,   38.14/s  (0.533s,   37.51/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [2950/3456 ( 85%)]  Loss:  0.570185 (0.5000)  Time: 0.523s,   38.25/s  (0.533s,   37.52/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [3000/3456 ( 87%)]  Loss:  0.454205 (0.4999)  Time: 0.541s,   36.94/s  (0.533s,   37.52/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [3050/3456 ( 88%)]  Loss:  0.377976 (0.4996)  Time: 0.525s,   38.12/s  (0.533s,   37.53/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [3100/3456 ( 90%)]  Loss:  0.585501 (0.4993)  Time: 0.531s,   37.69/s  (0.533s,   37.53/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [3150/3456 ( 91%)]  Loss:  0.498448 (0.4992)  Time: 0.526s,   38.05/s  (0.533s,   37.54/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n",
      "Train: 29 [3200/3456 ( 93%)]  Loss:  0.489538 (0.4989)  Time: 0.546s,   36.65/s  (0.533s,   37.54/s)  LR: 3.909e-02  Data: 0.011 (0.011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 29 [3250/3456 ( 94%)]  Loss:  1.286037 (0.5056)  Time: 0.929s,   21.52/s  (0.537s,   37.23/s)  LR: 3.909e-02  Data: 0.396 (0.015)\n",
      "Train: 29 [3300/3456 ( 96%)]  Loss:  1.066470 (0.5101)  Time: 0.909s,   22.00/s  (0.543s,   36.83/s)  LR: 3.909e-02  Data: 0.392 (0.021)\n",
      "Train: 29 [3350/3456 ( 97%)]  Loss:  1.034995 (0.5129)  Time: 0.874s,   22.88/s  (0.548s,   36.47/s)  LR: 3.909e-02  Data: 0.356 (0.026)\n",
      "Train: 29 [3400/3456 ( 98%)]  Loss:  1.135929 (0.5160)  Time: 0.849s,   23.57/s  (0.554s,   36.13/s)  LR: 3.909e-02  Data: 0.334 (0.032)\n",
      "Train: 29 [3450/3456 (100%)]  Loss:  0.344065 (0.5180)  Time: 0.901s,   22.19/s  (0.558s,   35.82/s)  LR: 3.909e-02  Data: 0.387 (0.037)\n",
      "Train: 29 [3455/3456 (100%)]  Loss:  0.676077 (0.5182)  Time: 0.449s,   24.52/s  (0.559s,   19.69/s)  LR: 3.909e-02  Data: 0.040 (0.037)\n",
      "Test: [   0/147]  Time: 0.861 (0.861)  Loss:  2.2020 (2.2020)  \n",
      "Test: [  50/147]  Time: 0.263 (0.275)  Loss:  2.7000 (2.5574)  \n",
      "Test: [ 100/147]  Time: 0.270 (0.270)  Loss:  2.5894 (2.5422)  \n",
      "Test: [ 147/147]  Time: 0.284 (0.317)  Loss:  0.8575 (2.3929)  \n",
      "Current checkpoints:\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-0.pth.tar', 1.3770374389919076)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-14.pth.tar', 2.2393299304955714)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-17.pth.tar', 2.3276492161927997)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-24.pth.tar', 2.3681411847874925)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-11.pth.tar', 2.3857336813533627)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-29.pth.tar', 2.39293469609441)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-21.pth.tar', 2.41008029435132)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-22.pth.tar', 2.418250387584841)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-25.pth.tar', 2.425751063872028)\n",
      " ('./output/t1/train/20200610-110221-tf_efficientdet_d1/checkpoint-13.pth.tar', 2.4392088601315343)\n",
      "\n",
      "Train: 30 [   0/3456 (  0%)]  Loss:  2.327697 (2.3277)  Time: 1.429s,   14.00/s  (1.429s,   14.00/s)  LR: 3.902e-02  Data: 0.835 (0.835)\n",
      "Train: 30 [  50/3456 (  1%)]  Loss:  0.498564 (0.7263)  Time: 0.524s,   38.13/s  (0.545s,   36.72/s)  LR: 3.902e-02  Data: 0.011 (0.027)\n",
      "Train: 30 [ 100/3456 (  3%)]  Loss:  0.581605 (0.6330)  Time: 0.532s,   37.61/s  (0.536s,   37.30/s)  LR: 3.902e-02  Data: 0.011 (0.019)\n",
      "Train: 30 [ 150/3456 (  4%)]  Loss:  0.497167 (0.5901)  Time: 0.526s,   38.03/s  (0.534s,   37.47/s)  LR: 3.902e-02  Data: 0.011 (0.016)\n",
      "Train: 30 [ 200/3456 (  6%)]  Loss:  0.609112 (0.5718)  Time: 0.527s,   37.96/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.015)\n",
      "Train: 30 [ 250/3456 (  7%)]  Loss:  0.442429 (0.5621)  Time: 0.524s,   38.14/s  (0.532s,   37.62/s)  LR: 3.902e-02  Data: 0.011 (0.014)\n",
      "Train: 30 [ 300/3456 (  9%)]  Loss:  0.576861 (0.5545)  Time: 0.526s,   38.04/s  (0.531s,   37.69/s)  LR: 3.902e-02  Data: 0.011 (0.014)\n",
      "Train: 30 [ 350/3456 ( 10%)]  Loss:  0.558827 (0.5470)  Time: 0.523s,   38.21/s  (0.530s,   37.73/s)  LR: 3.902e-02  Data: 0.011 (0.013)\n",
      "Train: 30 [ 400/3456 ( 12%)]  Loss:  0.465839 (0.5403)  Time: 0.527s,   37.97/s  (0.531s,   37.66/s)  LR: 3.902e-02  Data: 0.011 (0.013)\n",
      "Train: 30 [ 450/3456 ( 13%)]  Loss:  0.517978 (0.5383)  Time: 0.526s,   38.01/s  (0.531s,   37.67/s)  LR: 3.902e-02  Data: 0.011 (0.013)\n",
      "Train: 30 [ 500/3456 ( 14%)]  Loss:  0.493125 (0.5348)  Time: 0.524s,   38.17/s  (0.530s,   37.71/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 550/3456 ( 16%)]  Loss:  0.493861 (0.5304)  Time: 0.526s,   38.03/s  (0.531s,   37.70/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 600/3456 ( 17%)]  Loss:  0.522218 (0.5286)  Time: 0.527s,   37.99/s  (0.530s,   37.72/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 650/3456 ( 19%)]  Loss:  0.617953 (0.5273)  Time: 0.544s,   36.80/s  (0.530s,   37.72/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 700/3456 ( 20%)]  Loss:  0.418521 (0.5243)  Time: 0.528s,   37.90/s  (0.531s,   37.70/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 750/3456 ( 22%)]  Loss:  0.631757 (0.5230)  Time: 0.541s,   36.97/s  (0.531s,   37.63/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 800/3456 ( 23%)]  Loss:  0.600583 (0.5219)  Time: 0.525s,   38.10/s  (0.532s,   37.62/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 850/3456 ( 25%)]  Loss:  0.480436 (0.5205)  Time: 0.526s,   37.99/s  (0.532s,   37.61/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 900/3456 ( 26%)]  Loss:  0.467862 (0.5193)  Time: 0.542s,   36.91/s  (0.532s,   37.59/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [ 950/3456 ( 27%)]  Loss:  0.468525 (0.5178)  Time: 0.544s,   36.79/s  (0.533s,   37.55/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1000/3456 ( 29%)]  Loss:  0.509594 (0.5162)  Time: 0.525s,   38.12/s  (0.533s,   37.56/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1050/3456 ( 30%)]  Loss:  0.447665 (0.5155)  Time: 0.523s,   38.25/s  (0.532s,   37.58/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1100/3456 ( 32%)]  Loss:  0.516958 (0.5156)  Time: 0.525s,   38.12/s  (0.532s,   37.60/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1150/3456 ( 33%)]  Loss:  0.390412 (0.5144)  Time: 0.526s,   38.04/s  (0.532s,   37.62/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1200/3456 ( 35%)]  Loss:  0.481437 (0.5135)  Time: 0.526s,   38.06/s  (0.531s,   37.64/s)  LR: 3.902e-02  Data: 0.011 (0.012)\n",
      "Train: 30 [1250/3456 ( 36%)]  Loss:  0.551473 (0.5125)  Time: 0.525s,   38.09/s  (0.531s,   37.66/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1300/3456 ( 38%)]  Loss:  0.525772 (0.5116)  Time: 0.526s,   38.02/s  (0.531s,   37.66/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1350/3456 ( 39%)]  Loss:  0.526415 (0.5112)  Time: 0.527s,   37.97/s  (0.531s,   37.68/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1400/3456 ( 41%)]  Loss:  0.482822 (0.5107)  Time: 0.545s,   36.67/s  (0.531s,   37.66/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1450/3456 ( 42%)]  Loss:  0.582165 (0.5103)  Time: 0.542s,   36.93/s  (0.531s,   37.63/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1500/3456 ( 43%)]  Loss:  0.522479 (0.5103)  Time: 0.544s,   36.77/s  (0.532s,   37.60/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1550/3456 ( 45%)]  Loss:  0.464031 (0.5100)  Time: 0.541s,   36.99/s  (0.532s,   37.57/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1600/3456 ( 46%)]  Loss:  0.461868 (0.5095)  Time: 0.543s,   36.82/s  (0.533s,   37.55/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1650/3456 ( 48%)]  Loss:  0.403994 (0.5090)  Time: 0.541s,   36.96/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1700/3456 ( 49%)]  Loss:  0.504666 (0.5086)  Time: 0.543s,   36.85/s  (0.533s,   37.50/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1750/3456 ( 51%)]  Loss:  0.521038 (0.5083)  Time: 0.544s,   36.75/s  (0.534s,   37.48/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1800/3456 ( 52%)]  Loss:  0.570240 (0.5080)  Time: 0.546s,   36.66/s  (0.534s,   37.46/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1850/3456 ( 54%)]  Loss:  0.495137 (0.5071)  Time: 0.545s,   36.70/s  (0.534s,   37.44/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1900/3456 ( 55%)]  Loss:  0.534002 (0.5070)  Time: 0.523s,   38.22/s  (0.534s,   37.43/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [1950/3456 ( 56%)]  Loss:  0.513926 (0.5066)  Time: 0.526s,   38.02/s  (0.534s,   37.44/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2000/3456 ( 58%)]  Loss:  0.463926 (0.5062)  Time: 0.526s,   38.03/s  (0.534s,   37.45/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2050/3456 ( 59%)]  Loss:  0.453000 (0.5058)  Time: 0.524s,   38.14/s  (0.534s,   37.47/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2100/3456 ( 61%)]  Loss:  0.504639 (0.5058)  Time: 0.525s,   38.08/s  (0.534s,   37.48/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2150/3456 ( 62%)]  Loss:  0.468100 (0.5055)  Time: 0.528s,   37.91/s  (0.533s,   37.50/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2200/3456 ( 64%)]  Loss:  0.456877 (0.5046)  Time: 0.528s,   37.86/s  (0.533s,   37.51/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2250/3456 ( 65%)]  Loss:  0.455268 (0.5040)  Time: 0.523s,   38.25/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2300/3456 ( 67%)]  Loss:  0.475466 (0.5034)  Time: 0.526s,   37.99/s  (0.533s,   37.53/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2350/3456 ( 68%)]  Loss:  0.459087 (0.5031)  Time: 0.524s,   38.18/s  (0.533s,   37.54/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2400/3456 ( 69%)]  Loss:  0.471958 (0.5031)  Time: 0.525s,   38.12/s  (0.533s,   37.55/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2450/3456 ( 71%)]  Loss:  0.425611 (0.5028)  Time: 0.528s,   37.88/s  (0.532s,   37.56/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2500/3456 ( 72%)]  Loss:  0.419613 (0.5027)  Time: 0.543s,   36.86/s  (0.532s,   37.57/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2550/3456 ( 74%)]  Loss:  0.429332 (0.5021)  Time: 0.528s,   37.91/s  (0.532s,   37.57/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2600/3456 ( 75%)]  Loss:  0.502188 (0.5019)  Time: 0.548s,   36.52/s  (0.532s,   37.56/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2650/3456 ( 77%)]  Loss:  0.589274 (0.5020)  Time: 0.546s,   36.66/s  (0.533s,   37.54/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2700/3456 ( 78%)]  Loss:  0.492936 (0.5018)  Time: 0.544s,   36.74/s  (0.533s,   37.53/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2750/3456 ( 80%)]  Loss:  0.480242 (0.5016)  Time: 0.528s,   37.90/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2800/3456 ( 81%)]  Loss:  0.404649 (0.5013)  Time: 0.526s,   38.00/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2850/3456 ( 82%)]  Loss:  0.590188 (0.5011)  Time: 0.524s,   38.14/s  (0.533s,   37.53/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2900/3456 ( 84%)]  Loss:  0.520954 (0.5011)  Time: 0.542s,   36.92/s  (0.533s,   37.53/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [2950/3456 ( 85%)]  Loss:  0.565328 (0.5010)  Time: 0.543s,   36.84/s  (0.533s,   37.52/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3000/3456 ( 87%)]  Loss:  0.408650 (0.5009)  Time: 0.543s,   36.83/s  (0.533s,   37.50/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3050/3456 ( 88%)]  Loss:  0.466558 (0.5006)  Time: 0.541s,   36.97/s  (0.533s,   37.49/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3100/3456 ( 90%)]  Loss:  0.449900 (0.5003)  Time: 0.541s,   36.96/s  (0.534s,   37.48/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3150/3456 ( 91%)]  Loss:  0.485142 (0.5001)  Time: 0.545s,   36.69/s  (0.534s,   37.46/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3200/3456 ( 93%)]  Loss:  0.479188 (0.5000)  Time: 0.548s,   36.47/s  (0.534s,   37.45/s)  LR: 3.902e-02  Data: 0.011 (0.011)\n",
      "Train: 30 [3250/3456 ( 94%)]  Loss:  1.229010 (0.5049)  Time: 0.954s,   20.96/s  (0.538s,   37.16/s)  LR: 3.902e-02  Data: 0.422 (0.015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 30 [3300/3456 ( 96%)]  Loss:  1.186914 (0.5098)  Time: 0.873s,   22.92/s  (0.544s,   36.78/s)  LR: 3.902e-02  Data: 0.341 (0.021)\n",
      "Train: 30 [3350/3456 ( 97%)]  Loss:  0.972958 (0.5128)  Time: 0.888s,   22.53/s  (0.549s,   36.42/s)  LR: 3.902e-02  Data: 0.353 (0.026)\n",
      "Train: 30 [3400/3456 ( 98%)]  Loss:  1.220486 (0.5174)  Time: 0.889s,   22.50/s  (0.554s,   36.10/s)  LR: 3.902e-02  Data: 0.358 (0.031)\n",
      "Train: 30 [3450/3456 (100%)]  Loss:  0.678808 (0.5191)  Time: 0.889s,   22.50/s  (0.559s,   35.79/s)  LR: 3.902e-02  Data: 0.358 (0.035)\n",
      "Train: 30 [3455/3456 (100%)]  Loss:  0.611609 (0.5193)  Time: 0.462s,   23.80/s  (0.559s,   19.68/s)  LR: 3.902e-02  Data: 0.040 (0.035)\n",
      "Test: [   0/147]  Time: 0.878 (0.878)  Loss:  2.4469 (2.4469)  \n",
      "Test: [  50/147]  Time: 0.282 (0.294)  Loss:  2.7918 (2.6836)  \n",
      "Test: [ 100/147]  Time: 0.280 (0.288)  Loss:  2.6717 (2.6757)  \n",
      "Test: [ 147/147]  Time: 0.286 (0.333)  Loss:  0.5419 (2.4875)  \n",
      "Train: 31 [   0/3456 (  0%)]  Loss:  2.362321 (2.3623)  Time: 1.426s,   14.03/s  (1.426s,   14.03/s)  LR: 3.896e-02  Data: 0.826 (0.826)\n",
      "Train: 31 [  50/3456 (  1%)]  Loss:  0.495768 (0.7249)  Time: 0.529s,   37.81/s  (0.545s,   36.68/s)  LR: 3.896e-02  Data: 0.011 (0.027)\n",
      "Train: 31 [ 100/3456 (  3%)]  Loss:  0.566916 (0.6339)  Time: 0.526s,   38.01/s  (0.537s,   37.25/s)  LR: 3.896e-02  Data: 0.011 (0.019)\n",
      "Train: 31 [ 150/3456 (  4%)]  Loss:  0.438653 (0.5911)  Time: 0.525s,   38.10/s  (0.535s,   37.41/s)  LR: 3.896e-02  Data: 0.011 (0.016)\n",
      "Train: 31 [ 200/3456 (  6%)]  Loss:  0.530655 (0.5720)  Time: 0.550s,   36.38/s  (0.537s,   37.23/s)  LR: 3.896e-02  Data: 0.011 (0.015)\n",
      "Train: 31 [ 250/3456 (  7%)]  Loss:  0.468937 (0.5617)  Time: 0.543s,   36.83/s  (0.539s,   37.10/s)  LR: 3.896e-02  Data: 0.011 (0.014)\n",
      "Train: 31 [ 300/3456 (  9%)]  Loss:  0.516123 (0.5534)  Time: 0.527s,   37.94/s  (0.539s,   37.09/s)  LR: 3.896e-02  Data: 0.011 (0.014)\n",
      "Train: 31 [ 350/3456 ( 10%)]  Loss:  0.476170 (0.5463)  Time: 0.549s,   36.43/s  (0.538s,   37.20/s)  LR: 3.896e-02  Data: 0.011 (0.013)\n",
      "Train: 31 [ 400/3456 ( 12%)]  Loss:  0.588711 (0.5408)  Time: 0.542s,   36.92/s  (0.539s,   37.13/s)  LR: 3.896e-02  Data: 0.011 (0.013)\n",
      "Train: 31 [ 450/3456 ( 13%)]  Loss:  0.453425 (0.5378)  Time: 0.543s,   36.86/s  (0.539s,   37.08/s)  LR: 3.896e-02  Data: 0.011 (0.013)\n",
      "Train: 31 [ 500/3456 ( 14%)]  Loss:  0.518605 (0.5341)  Time: 0.543s,   36.83/s  (0.540s,   37.04/s)  LR: 3.896e-02  Data: 0.011 (0.013)\n",
      "Train: 31 [ 550/3456 ( 16%)]  Loss:  0.473062 (0.5303)  Time: 0.543s,   36.85/s  (0.540s,   37.01/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [ 600/3456 ( 17%)]  Loss:  0.564238 (0.5289)  Time: 0.524s,   38.18/s  (0.540s,   37.01/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [ 650/3456 ( 19%)]  Loss:  0.533861 (0.5279)  Time: 0.523s,   38.27/s  (0.539s,   37.09/s)  LR: 3.896e-02  Data: 0.010 (0.012)\n",
      "Train: 31 [ 700/3456 ( 20%)]  Loss:  0.482027 (0.5252)  Time: 0.528s,   37.89/s  (0.539s,   37.14/s)  LR: 3.896e-02  Data: 0.010 (0.012)\n",
      "Train: 31 [ 750/3456 ( 22%)]  Loss:  0.544277 (0.5239)  Time: 0.527s,   37.95/s  (0.538s,   37.19/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [ 800/3456 ( 23%)]  Loss:  0.540064 (0.5220)  Time: 0.526s,   38.04/s  (0.537s,   37.24/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [ 850/3456 ( 25%)]  Loss:  0.506082 (0.5210)  Time: 0.527s,   37.99/s  (0.536s,   37.28/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [ 900/3456 ( 26%)]  Loss:  0.452440 (0.5210)  Time: 0.524s,   38.16/s  (0.536s,   37.32/s)  LR: 3.896e-02  Data: 0.010 (0.012)\n",
      "Train: 31 [ 950/3456 ( 27%)]  Loss:  0.600714 (0.5196)  Time: 0.531s,   37.64/s  (0.536s,   37.35/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [1000/3456 ( 29%)]  Loss:  0.545968 (0.5185)  Time: 0.525s,   38.08/s  (0.535s,   37.38/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [1050/3456 ( 30%)]  Loss:  0.425750 (0.5176)  Time: 0.543s,   36.83/s  (0.535s,   37.36/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [1100/3456 ( 32%)]  Loss:  0.575371 (0.5173)  Time: 0.541s,   36.95/s  (0.536s,   37.33/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [1150/3456 ( 33%)]  Loss:  0.410882 (0.5161)  Time: 0.544s,   36.79/s  (0.536s,   37.31/s)  LR: 3.896e-02  Data: 0.011 (0.012)\n",
      "Train: 31 [1200/3456 ( 35%)]  Loss:  0.508177 (0.5155)  Time: 0.543s,   36.83/s  (0.536s,   37.28/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1250/3456 ( 36%)]  Loss:  0.504476 (0.5148)  Time: 0.524s,   38.15/s  (0.536s,   37.29/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1300/3456 ( 38%)]  Loss:  0.527081 (0.5138)  Time: 0.527s,   37.93/s  (0.536s,   37.31/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1350/3456 ( 39%)]  Loss:  0.568434 (0.5132)  Time: 0.528s,   37.89/s  (0.536s,   37.34/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1400/3456 ( 41%)]  Loss:  0.484687 (0.5121)  Time: 0.526s,   38.04/s  (0.535s,   37.36/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1450/3456 ( 42%)]  Loss:  0.679867 (0.5117)  Time: 0.526s,   37.99/s  (0.535s,   37.38/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1500/3456 ( 43%)]  Loss:  0.449513 (0.5114)  Time: 0.528s,   37.87/s  (0.535s,   37.40/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1550/3456 ( 45%)]  Loss:  0.366053 (0.5111)  Time: 0.524s,   38.13/s  (0.535s,   37.42/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1600/3456 ( 46%)]  Loss:  0.469464 (0.5102)  Time: 0.526s,   38.04/s  (0.534s,   37.43/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1650/3456 ( 48%)]  Loss:  0.430494 (0.5095)  Time: 0.525s,   38.10/s  (0.534s,   37.45/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1700/3456 ( 49%)]  Loss:  0.436236 (0.5091)  Time: 0.525s,   38.08/s  (0.534s,   37.46/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1750/3456 ( 51%)]  Loss:  0.549038 (0.5090)  Time: 0.526s,   38.04/s  (0.534s,   37.48/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1800/3456 ( 52%)]  Loss:  0.496385 (0.5088)  Time: 0.525s,   38.10/s  (0.534s,   37.49/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1850/3456 ( 54%)]  Loss:  0.571821 (0.5079)  Time: 0.526s,   38.02/s  (0.533s,   37.50/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1900/3456 ( 55%)]  Loss:  0.595719 (0.5074)  Time: 0.523s,   38.21/s  (0.533s,   37.52/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [1950/3456 ( 56%)]  Loss:  0.641443 (0.5072)  Time: 0.526s,   38.01/s  (0.533s,   37.53/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2000/3456 ( 58%)]  Loss:  0.381700 (0.5070)  Time: 0.525s,   38.09/s  (0.533s,   37.55/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2050/3456 ( 59%)]  Loss:  0.568068 (0.5068)  Time: 0.525s,   38.08/s  (0.532s,   37.56/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2100/3456 ( 61%)]  Loss:  0.517479 (0.5071)  Time: 0.525s,   38.08/s  (0.532s,   37.57/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2150/3456 ( 62%)]  Loss:  0.572255 (0.5070)  Time: 0.541s,   36.95/s  (0.532s,   37.58/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2200/3456 ( 64%)]  Loss:  0.454499 (0.5063)  Time: 0.544s,   36.77/s  (0.533s,   37.55/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2250/3456 ( 65%)]  Loss:  0.537125 (0.5057)  Time: 0.549s,   36.41/s  (0.533s,   37.53/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2300/3456 ( 67%)]  Loss:  0.487760 (0.5053)  Time: 0.542s,   36.88/s  (0.533s,   37.52/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2350/3456 ( 68%)]  Loss:  0.547299 (0.5050)  Time: 0.549s,   36.45/s  (0.533s,   37.50/s)  LR: 3.896e-02  Data: 0.012 (0.011)\n",
      "Train: 31 [2400/3456 ( 69%)]  Loss:  0.563472 (0.5051)  Time: 0.544s,   36.74/s  (0.534s,   37.49/s)  LR: 3.896e-02  Data: 0.012 (0.011)\n",
      "Train: 31 [2450/3456 ( 71%)]  Loss:  0.515215 (0.5049)  Time: 0.548s,   36.52/s  (0.534s,   37.46/s)  LR: 3.896e-02  Data: 0.012 (0.011)\n",
      "Train: 31 [2500/3456 ( 72%)]  Loss:  0.441213 (0.5047)  Time: 0.542s,   36.88/s  (0.534s,   37.45/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2550/3456 ( 74%)]  Loss:  0.480898 (0.5045)  Time: 0.546s,   36.62/s  (0.534s,   37.44/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2600/3456 ( 75%)]  Loss:  0.497755 (0.5042)  Time: 0.525s,   38.12/s  (0.534s,   37.44/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2650/3456 ( 77%)]  Loss:  0.517401 (0.5041)  Time: 0.525s,   38.09/s  (0.534s,   37.45/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2700/3456 ( 78%)]  Loss:  0.443047 (0.5039)  Time: 0.524s,   38.14/s  (0.534s,   37.46/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2750/3456 ( 80%)]  Loss:  0.534615 (0.5037)  Time: 0.526s,   38.03/s  (0.534s,   37.47/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2800/3456 ( 81%)]  Loss:  0.422116 (0.5035)  Time: 0.526s,   38.03/s  (0.534s,   37.48/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2850/3456 ( 82%)]  Loss:  0.464093 (0.5033)  Time: 0.528s,   37.89/s  (0.533s,   37.49/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2900/3456 ( 84%)]  Loss:  0.556352 (0.5032)  Time: 0.525s,   38.06/s  (0.533s,   37.50/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [2950/3456 ( 85%)]  Loss:  0.548334 (0.5030)  Time: 0.525s,   38.07/s  (0.533s,   37.50/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3000/3456 ( 87%)]  Loss:  0.421335 (0.5028)  Time: 0.523s,   38.23/s  (0.533s,   37.51/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3050/3456 ( 88%)]  Loss:  0.405282 (0.5024)  Time: 0.531s,   37.65/s  (0.533s,   37.52/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3100/3456 ( 90%)]  Loss:  0.477467 (0.5020)  Time: 0.533s,   37.56/s  (0.533s,   37.53/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3150/3456 ( 91%)]  Loss:  0.574992 (0.5018)  Time: 0.527s,   37.98/s  (0.533s,   37.53/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3200/3456 ( 93%)]  Loss:  0.444939 (0.5017)  Time: 0.526s,   38.05/s  (0.533s,   37.53/s)  LR: 3.896e-02  Data: 0.011 (0.011)\n",
      "Train: 31 [3250/3456 ( 94%)]  Loss:  1.146857 (0.5056)  Time: 1.138s,   17.57/s  (0.537s,   37.23/s)  LR: 3.896e-02  Data: 0.431 (0.015)\n",
      "Train: 31 [3300/3456 ( 96%)]  Loss:  1.047299 (0.5105)  Time: 0.879s,   22.76/s  (0.543s,   36.84/s)  LR: 3.896e-02  Data: 0.344 (0.021)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 31 [3350/3456 ( 97%)]  Loss:  0.636268 (0.5160)  Time: 0.924s,   21.64/s  (0.548s,   36.49/s)  LR: 3.896e-02  Data: 0.392 (0.026)\n",
      "Train: 31 [3400/3456 ( 98%)]  Loss:  0.906297 (0.5190)  Time: 0.880s,   22.74/s  (0.553s,   36.16/s)  LR: 3.896e-02  Data: 0.348 (0.031)\n",
      "Train: 31 [3450/3456 (100%)]  Loss:  0.580237 (0.5212)  Time: 0.900s,   22.23/s  (0.558s,   35.84/s)  LR: 3.896e-02  Data: 0.384 (0.036)\n",
      "Train: 31 [3455/3456 (100%)]  Loss:  0.588537 (0.5215)  Time: 0.448s,   24.58/s  (0.558s,   19.71/s)  LR: 3.896e-02  Data: 0.040 (0.036)\n",
      "Test: [   0/147]  Time: 0.863 (0.863)  Loss:  2.4243 (2.4243)  \n",
      "Test: [  50/147]  Time: 0.265 (0.287)  Loss:  2.9158 (2.7069)  \n",
      "Test: [ 100/147]  Time: 0.270 (0.279)  Loss:  2.7038 (2.6971)  \n",
      "Test: [ 147/147]  Time: 0.291 (0.325)  Loss:  0.9734 (2.5642)  \n",
      "Train: 32 [   0/3456 (  0%)]  Loss:  2.438257 (2.4383)  Time: 1.453s,   13.76/s  (1.453s,   13.76/s)  LR: 3.889e-02  Data: 0.848 (0.848)\n",
      "Train: 32 [  50/3456 (  1%)]  Loss:  0.488885 (0.7593)  Time: 0.524s,   38.18/s  (0.544s,   36.73/s)  LR: 3.889e-02  Data: 0.011 (0.027)\n",
      "Train: 32 [ 100/3456 (  3%)]  Loss:  0.572829 (0.6546)  Time: 0.525s,   38.11/s  (0.535s,   37.36/s)  LR: 3.889e-02  Data: 0.011 (0.019)\n",
      "Train: 32 [ 150/3456 (  4%)]  Loss:  0.575458 (0.6063)  Time: 0.525s,   38.12/s  (0.533s,   37.53/s)  LR: 3.889e-02  Data: 0.011 (0.016)\n",
      "Train: 32 [ 200/3456 (  6%)]  Loss:  0.558842 (0.5823)  Time: 0.545s,   36.71/s  (0.535s,   37.39/s)  LR: 3.889e-02  Data: 0.011 (0.015)\n",
      "Train: 32 [ 250/3456 (  7%)]  Loss:  0.528850 (0.5700)  Time: 0.550s,   36.36/s  (0.535s,   37.38/s)  LR: 3.889e-02  Data: 0.011 (0.014)\n",
      "Train: 32 [ 300/3456 (  9%)]  Loss:  0.503767 (0.5596)  Time: 0.540s,   37.07/s  (0.537s,   37.25/s)  LR: 3.889e-02  Data: 0.011 (0.014)\n",
      "Train: 32 [ 350/3456 ( 10%)]  Loss:  0.507250 (0.5516)  Time: 0.550s,   36.36/s  (0.538s,   37.17/s)  LR: 3.889e-02  Data: 0.011 (0.013)\n",
      "Train: 32 [ 400/3456 ( 12%)]  Loss:  0.541655 (0.5455)  Time: 0.543s,   36.85/s  (0.539s,   37.10/s)  LR: 3.889e-02  Data: 0.011 (0.013)\n",
      "Train: 32 [ 450/3456 ( 13%)]  Loss:  0.489030 (0.5423)  Time: 0.543s,   36.84/s  (0.540s,   37.05/s)  LR: 3.889e-02  Data: 0.011 (0.013)\n",
      "Train: 32 [ 500/3456 ( 14%)]  Loss:  0.593764 (0.5394)  Time: 0.548s,   36.51/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.013)\n",
      "Train: 32 [ 550/3456 ( 16%)]  Loss:  0.487912 (0.5351)  Time: 0.540s,   37.02/s  (0.541s,   36.98/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 600/3456 ( 17%)]  Loss:  0.452078 (0.5327)  Time: 0.549s,   36.44/s  (0.541s,   36.94/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 650/3456 ( 19%)]  Loss:  0.571521 (0.5307)  Time: 0.541s,   37.00/s  (0.542s,   36.92/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 700/3456 ( 20%)]  Loss:  0.499888 (0.5284)  Time: 0.548s,   36.50/s  (0.542s,   36.90/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 750/3456 ( 22%)]  Loss:  0.604143 (0.5272)  Time: 0.542s,   36.88/s  (0.542s,   36.89/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 800/3456 ( 23%)]  Loss:  0.552915 (0.5249)  Time: 0.541s,   36.98/s  (0.542s,   36.88/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 850/3456 ( 25%)]  Loss:  0.535754 (0.5229)  Time: 0.548s,   36.53/s  (0.542s,   36.87/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 900/3456 ( 26%)]  Loss:  0.434027 (0.5224)  Time: 0.526s,   38.04/s  (0.542s,   36.92/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [ 950/3456 ( 27%)]  Loss:  0.525748 (0.5205)  Time: 0.527s,   37.92/s  (0.541s,   36.98/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1000/3456 ( 29%)]  Loss:  0.535499 (0.5192)  Time: 0.523s,   38.23/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1050/3456 ( 30%)]  Loss:  0.532763 (0.5179)  Time: 0.546s,   36.61/s  (0.539s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1100/3456 ( 32%)]  Loss:  0.495648 (0.5175)  Time: 0.543s,   36.85/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1150/3456 ( 33%)]  Loss:  0.559874 (0.5157)  Time: 0.527s,   37.93/s  (0.539s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1200/3456 ( 35%)]  Loss:  0.500850 (0.5148)  Time: 0.544s,   36.75/s  (0.539s,   37.10/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1250/3456 ( 36%)]  Loss:  0.619768 (0.5141)  Time: 0.546s,   36.60/s  (0.539s,   37.08/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1300/3456 ( 38%)]  Loss:  0.536147 (0.5130)  Time: 0.540s,   37.05/s  (0.539s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.012)\n",
      "Train: 32 [1350/3456 ( 39%)]  Loss:  0.466817 (0.5125)  Time: 0.542s,   36.90/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1400/3456 ( 41%)]  Loss:  0.458384 (0.5121)  Time: 0.541s,   36.95/s  (0.540s,   37.04/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1450/3456 ( 42%)]  Loss:  0.615052 (0.5116)  Time: 0.547s,   36.53/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1500/3456 ( 43%)]  Loss:  0.464077 (0.5111)  Time: 0.542s,   36.91/s  (0.540s,   37.02/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1550/3456 ( 45%)]  Loss:  0.468712 (0.5105)  Time: 0.548s,   36.51/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1600/3456 ( 46%)]  Loss:  0.514314 (0.5097)  Time: 0.545s,   36.68/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1650/3456 ( 48%)]  Loss:  0.436907 (0.5093)  Time: 0.526s,   38.03/s  (0.540s,   37.02/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1700/3456 ( 49%)]  Loss:  0.399636 (0.5089)  Time: 0.542s,   36.92/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1750/3456 ( 51%)]  Loss:  0.522149 (0.5085)  Time: 0.550s,   36.38/s  (0.540s,   37.02/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1800/3456 ( 52%)]  Loss:  0.519169 (0.5084)  Time: 0.546s,   36.64/s  (0.540s,   37.02/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1850/3456 ( 54%)]  Loss:  0.558054 (0.5074)  Time: 0.544s,   36.76/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1900/3456 ( 55%)]  Loss:  0.587742 (0.5070)  Time: 0.547s,   36.54/s  (0.541s,   37.00/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [1950/3456 ( 56%)]  Loss:  0.587834 (0.5068)  Time: 0.547s,   36.56/s  (0.541s,   36.99/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2000/3456 ( 58%)]  Loss:  0.451873 (0.5066)  Time: 0.544s,   36.73/s  (0.541s,   36.99/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2050/3456 ( 59%)]  Loss:  0.530419 (0.5065)  Time: 0.542s,   36.88/s  (0.541s,   36.98/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2100/3456 ( 61%)]  Loss:  0.446062 (0.5062)  Time: 0.526s,   38.03/s  (0.541s,   37.00/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2150/3456 ( 62%)]  Loss:  0.526465 (0.5061)  Time: 0.540s,   37.05/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2200/3456 ( 64%)]  Loss:  0.420007 (0.5053)  Time: 0.525s,   38.10/s  (0.540s,   37.01/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2250/3456 ( 65%)]  Loss:  0.494905 (0.5049)  Time: 0.526s,   38.02/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2300/3456 ( 67%)]  Loss:  0.506074 (0.5045)  Time: 0.525s,   38.10/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2350/3456 ( 68%)]  Loss:  0.515090 (0.5043)  Time: 0.528s,   37.88/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2400/3456 ( 69%)]  Loss:  0.459688 (0.5042)  Time: 0.548s,   36.50/s  (0.540s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2450/3456 ( 71%)]  Loss:  0.461489 (0.5040)  Time: 0.548s,   36.53/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2500/3456 ( 72%)]  Loss:  0.393782 (0.5038)  Time: 0.541s,   36.94/s  (0.540s,   37.05/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2550/3456 ( 74%)]  Loss:  0.425527 (0.5034)  Time: 0.544s,   36.76/s  (0.540s,   37.05/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2600/3456 ( 75%)]  Loss:  0.491852 (0.5033)  Time: 0.546s,   36.63/s  (0.540s,   37.04/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2650/3456 ( 77%)]  Loss:  0.584420 (0.5033)  Time: 0.547s,   36.59/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2700/3456 ( 78%)]  Loss:  0.455686 (0.5031)  Time: 0.542s,   36.87/s  (0.540s,   37.03/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2750/3456 ( 80%)]  Loss:  0.441638 (0.5030)  Time: 0.524s,   38.13/s  (0.540s,   37.04/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2800/3456 ( 81%)]  Loss:  0.480262 (0.5028)  Time: 0.527s,   37.98/s  (0.540s,   37.06/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2850/3456 ( 82%)]  Loss:  0.602439 (0.5026)  Time: 0.547s,   36.56/s  (0.539s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2900/3456 ( 84%)]  Loss:  0.540128 (0.5025)  Time: 0.524s,   38.18/s  (0.539s,   37.09/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [2950/3456 ( 85%)]  Loss:  0.517960 (0.5022)  Time: 0.548s,   36.50/s  (0.539s,   37.10/s)  LR: 3.889e-02  Data: 0.012 (0.011)\n",
      "Train: 32 [3000/3456 ( 87%)]  Loss:  0.400925 (0.5021)  Time: 0.541s,   36.99/s  (0.539s,   37.09/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [3050/3456 ( 88%)]  Loss:  0.429746 (0.5017)  Time: 0.547s,   36.54/s  (0.539s,   37.09/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [3100/3456 ( 90%)]  Loss:  0.471240 (0.5015)  Time: 0.542s,   36.90/s  (0.539s,   37.08/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [3150/3456 ( 91%)]  Loss:  0.396471 (0.5012)  Time: 0.539s,   37.10/s  (0.539s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [3200/3456 ( 93%)]  Loss:  0.502813 (0.5010)  Time: 0.543s,   36.84/s  (0.540s,   37.07/s)  LR: 3.889e-02  Data: 0.011 (0.011)\n",
      "Train: 32 [3250/3456 ( 94%)]  Loss:  1.871465 (0.5058)  Time: 0.958s,   20.87/s  (0.544s,   36.79/s)  LR: 3.889e-02  Data: 0.405 (0.015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 32 [3300/3456 ( 96%)]  Loss:  0.739372 (0.5124)  Time: 0.888s,   22.52/s  (0.549s,   36.42/s)  LR: 3.889e-02  Data: 0.358 (0.021)\n",
      "Train: 32 [3350/3456 ( 97%)]  Loss:  0.590645 (0.5149)  Time: 0.913s,   21.90/s  (0.554s,   36.09/s)  LR: 3.889e-02  Data: 0.375 (0.026)\n",
      "Train: 32 [3400/3456 ( 98%)]  Loss:  1.179923 (0.5180)  Time: 0.856s,   23.38/s  (0.559s,   35.77/s)  LR: 3.889e-02  Data: 0.342 (0.031)\n",
      "Train: 32 [3450/3456 (100%)]  Loss:  0.714888 (0.5197)  Time: 0.898s,   22.27/s  (0.564s,   35.45/s)  LR: 3.889e-02  Data: 0.385 (0.036)\n",
      "Train: 32 [3455/3456 (100%)]  Loss:  0.856402 (0.5200)  Time: 0.446s,   24.66/s  (0.564s,   19.49/s)  LR: 3.889e-02  Data: 0.040 (0.036)\n",
      "Test: [   0/147]  Time: 0.874 (0.874)  Loss:  2.6574 (2.6574)  \n",
      "Test: [  50/147]  Time: 0.265 (0.280)  Loss:  2.9942 (2.8046)  \n",
      "Test: [ 100/147]  Time: 0.279 (0.275)  Loss:  2.8679 (2.7926)  \n",
      "Test: [ 147/147]  Time: 0.285 (0.321)  Loss:  0.6668 (2.6081)  \n",
      "Train: 33 [   0/3456 (  0%)]  Loss:  2.855221 (2.8552)  Time: 1.390s,   14.39/s  (1.390s,   14.39/s)  LR: 3.882e-02  Data: 0.768 (0.768)\n",
      "Train: 33 [  50/3456 (  1%)]  Loss:  0.446895 (0.8937)  Time: 0.545s,   36.68/s  (0.594s,   33.64/s)  LR: 3.882e-02  Data: 0.011 (0.027)\n"
     ]
    }
   ],
   "source": [
    "best_metric = None\n",
    "best_epoch = None\n",
    "save_metric = None\n",
    "try:\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_metrics = train_epoch(epoch, model, loader_train, optimizer,  lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir, use_amp=use_amp)\n",
    "        \n",
    "        writer.add_scalar(\"train_loss\", train_metrics[\"loss\"], epoch)\n",
    "        \n",
    "        eval_metrics = validate(model, loader_eval) # evaluator)\n",
    "        \n",
    "        writer.add_scalar(\"eval_loss\", eval_metrics[\"loss\"], epoch)\n",
    "        #writer.add_scalar(\"eval\", eval_metrics[\"map\"], epoch)\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            # step LR for next epoch\n",
    "            lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "        if saver is not None:\n",
    "            update_summary(\n",
    "                epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "                write_header=best_metric is None\n",
    "            )\n",
    "\n",
    "            # save proper checkpoint with eval metric\n",
    "            class ArgPars():\n",
    "                def __init__(self):\n",
    "                    self.model = model_name\n",
    "            args = ArgPars()\n",
    "            save_metric = eval_metrics[eval_metric]\n",
    "            best_metric, best_epoch = saver.save_checkpoint(unwrap_bench(model), optimizer, args, epoch=epoch, metric=save_metric, use_amp=use_amp)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "if best_metric is not None:\n",
    "    logging.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('mlops-image-e1': venv)",
   "language": "python",
   "name": "python_defaultSpec_1600837787072"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}